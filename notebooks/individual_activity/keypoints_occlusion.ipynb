{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c187e3b9-f0fd-4a7e-97ac-508556fdaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append('../../src')\n",
    "from common import common, transform, json\n",
    "from common.json_io import IA_FORMAT, START_IDX\n",
    "from common.functions import gauss, cos_similarity, standardize\n",
    "from common.keypoint import body\n",
    "from common.default import PASSING_DEFAULT\n",
    "from display.display import display\n",
    "from individual_activity.individual_activity import IndividualActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa397fc-7255-4abd-ae06-22b591c5966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_idx = 0\n",
    "device = f'cuda:{device_idx}' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29ffed-bbf1-4a9d-ab65-0a5d0d0bb339",
   "metadata": {},
   "source": [
    "# Indivisual Activity をロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06e6100-fadf-4f16-b94d-f3212734c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_settings = [\n",
    "    {'room_num': '02', 'date': '20210903', 'option': 'passing'},\n",
    "    {'room_num': '08', 'date': '20210915', 'option': 'passing'},\n",
    "    {'room_num': '09', 'date': '20210706', 'option': 'passing'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9a3706-07b3-407b-9f52-3780345e18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homography\n",
    "def get_homo(room_num):\n",
    "    field_path = os.path.join(common.data_dir, '{}/field.png'.format(room_num))\n",
    "    field_raw = cv2.imread(field_path)\n",
    "    p_video = common.homo[room_num][0]\n",
    "    p_field = common.homo[room_num][1]\n",
    "    homo = transform.Homography(p_video, p_field, field_raw.shape)\n",
    "    return homo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90f2a5d-9573-44c3-ab7f-7a15eb621862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_individuals(json_data, prefix, homo):\n",
    "    individuals = {}\n",
    "    for data in json_data:\n",
    "        label = prefix + str(data[IA_FORMAT[0]])\n",
    "        frame_num = data[IA_FORMAT[1]]\n",
    "        tracking_point = data[IA_FORMAT[2]]\n",
    "        keypoints = data[IA_FORMAT[3]]\n",
    "\n",
    "        if label not in individuals:\n",
    "            ia = IndividualActivity(label, homo)\n",
    "            individuals[label] = ia\n",
    "        else:\n",
    "            ia = individuals[label]\n",
    "\n",
    "        ia.tracking_points[frame_num] = tracking_point\n",
    "        ia.keypoints[frame_num] = keypoints\n",
    "        for f in IA_FORMAT[START_IDX:]:\n",
    "            ia.indicator_dict[f][frame_num] = data[f]\n",
    "            \n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7b473d-c311-413b-bb10-b650e7d073a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'room_num': '02', 'date': '20210903', 'option': 'passing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:03<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'room_num': '08', 'date': '20210915', 'option': 'passing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:04<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'room_num': '09', 'date': '20210706', 'option': 'passing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 13.01it/s]\n"
     ]
    }
   ],
   "source": [
    "individuals = {}\n",
    "for setting in data_settings:\n",
    "    print(setting)\n",
    "    room_num = setting['room_num']\n",
    "    date = setting['date']\n",
    "    opt = setting['option']\n",
    "    \n",
    "    homo = get_homo(room_num)\n",
    "    \n",
    "    if opt is None:\n",
    "        dir_path = f'{common.data_dir}/{room_num}/{date}/*'\n",
    "    else:\n",
    "        dir_path = f'{common.data_dir}/{room_num}/{date}/{opt}/*'\n",
    "    dirs = glob.glob(dir_path)\n",
    "    dirs = sorted(dirs)[:-1]  # delete make_csv.csv\n",
    "    \n",
    "    for path in tqdm(dirs):\n",
    "        path = f'{path}/json/individual_activity.json'\n",
    "        json_data = json_io.load(path)\n",
    "        prefix = common.split_path(path)[-6] + '_' + common.split_path(path)[-3] + '_'  # room-num_date_\n",
    "        individuals.update(load_individuals(json_data, prefix, homo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2931a-56ba-4921-99ec-f13440a9f56c",
   "metadata": {},
   "source": [
    "# nanが存在しないkeypointsを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b00af8b-e7ea-4b86-bb00-48de9f009b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_lst = []\n",
    "for ind in individuals.values():\n",
    "    kps_dict = ind.keypoints  # {frame_num: keypoints}\n",
    "    for kps in kps_dict.values():\n",
    "        if True not in np.isnan(kps):\n",
    "            keypoints_lst.append(kps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0629923-a291-4405-911b-b75817670862",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2a27923f-e7cc-4fb9-9929-1ae20cc00186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f661c01f470>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# network config\n",
    "net_config = {\n",
    "    'n_features': 4 * 2,\n",
    "    'n_layers': 5,\n",
    "    'hidden_dims': [128, 128, 128, 64, 16],\n",
    "    'dropouts': [0.4, 0.2, 0.2, 0.2, 0.1],\n",
    "    'output_dim': 4 * 2,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "# leraning rate\n",
    "LR = 0.0001\n",
    "\n",
    "# training and data settings\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "# setting random seed\n",
    "SEED = 64\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24356fc-1707-43aa-88b9-790d592693ea",
   "metadata": {},
   "source": [
    "## データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a6ee96-be65-4c16-a24a-5560e57e2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, keypoints_lst, indices, **config):\n",
    "        self.x, self.y = [], []\n",
    "        for idx in tqdm(indices):\n",
    "            kps = keypoints_lst[idx]\n",
    "            mean = np.mean(kps)\n",
    "            std = np.std(kps)\n",
    "            kps = (kps - mean) / std\n",
    "\n",
    "            self.x.append([\n",
    "                kps[body['LShoulder']],\n",
    "                kps[body['RShoulder']],\n",
    "                kps[body['LHip']],\n",
    "                kps[body['RHip']],\n",
    "            ])\n",
    "            self.y.append([\n",
    "                kps[body['LKnee']],\n",
    "                kps[body['RKnee']],\n",
    "                kps[body['LAnkle']],\n",
    "                kps[body['RAnkle']],\n",
    "            ])\n",
    "        self.x = np.array(self.x).reshape(-1, config['n_features'])\n",
    "        self.y = np.array(self.x).reshape(-1, config['n_features'])\n",
    "            \n",
    "        self.device = config['device']\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            tensor(self.x[index]).float().to(self.device),\n",
    "            tensor(self.y[index]).float().to(self.device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54b3bb0c-0fa1-4d08-918b-f37a54935d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:00<00:00, 10157.03it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 10581.50it/s]\n",
      "100%|██████████| 65/65 [00:00<00:00, 11404.72it/s]\n"
     ]
    }
   ],
   "source": [
    "random_idxs = np.random.choice(\n",
    "    range(len(keypoints_lst)),\n",
    "    size=len(keypoints_lst),\n",
    "    replace=False\n",
    ")\n",
    "train_len = int(len(keypoints_lst) * TRAIN_RATIO)\n",
    "val_len = int(len(keypoints_lst) * VAL_RATIO)\n",
    "train_idxs = random_idxs[:train_len]\n",
    "val_idxs = random_idxs[train_len:train_len + val_len]\n",
    "test_idxs = random_idxs[train_len + val_len:]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MyDataset(keypoints_lst, train_idxs, **net_config), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(\n",
    "    MyDataset(keypoints_lst, val_idxs, **net_config), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    MyDataset(keypoints_lst, test_idxs, **net_config), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310beaa-6bb3-4084-a5ac-e3a02276c683",
   "metadata": {},
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f55019fa-688d-4aab-8488-9bb81a233370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super(Network, self).__init__()\n",
    "        self.n_features = config['n_features']\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.output_dim = config['output_dim']\n",
    "        self.dropouts = config['dropouts']\n",
    "        \n",
    "        assert\\\n",
    "            self.n_layers == len(self.hidden_dims),\\\n",
    "            f'n_layers:{self.n_layers}, n_hidden_dims:{len(self.hidden_dims)}'\n",
    "        \n",
    "        assert\\\n",
    "            self.n_layers == len(self.dropouts),\\\n",
    "            f'n_layers:{self.n_layers}, n_dropouts:{len(self.dropouts)}'\n",
    "        \n",
    "        \n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                in_dim = self.n_features\n",
    "            else:\n",
    "                in_dim = self.hidden_dims[i - 1]\n",
    "            out_dim = self.hidden_dims[i]\n",
    "            self.net.add_module(\n",
    "                f'fc{i + 1}',\n",
    "                Layer(in_dim, out_dim, self.dropouts[i])\n",
    "            )\n",
    "        self.output_layer = nn.Linear(self.hidden_dims[-1], self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x.reshape(-1, self.output_dim)\n",
    "\n",
    "        \n",
    "class Layer(nn.Sequential):\n",
    "    def __init__(self, in_dims, out_dims, dropout_rate):\n",
    "        super(Layer, self).__init__(\n",
    "            nn.Linear(in_dims, out_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ac9ce1a1-8b26-4462-94f0-157dfd9471be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (net): Sequential(\n",
       "    (fc1): Layer(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (fc2): Layer(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc3): Layer(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc4): Layer(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc5): Layer(\n",
       "      (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=16, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network(**net_config)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9dec2016-c660-498d-a81e-89fbc8556b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2ba80-4b9a-4ed1-83a9-424f47f7eb9d",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "163bdac8-7ee4-4828-b8a8-2bdb29dde112",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1000] train loss: 0.86265, val loss: 0.94581, time: 0.02\n",
      "Epoch[2/1000] train loss: 0.93330, val loss: 0.94359, time: 0.01\n",
      "Epoch[3/1000] train loss: 0.91882, val loss: 0.94133, time: 0.01\n",
      "Epoch[4/1000] train loss: 0.93831, val loss: 0.93907, time: 0.01\n",
      "Epoch[5/1000] train loss: 0.91716, val loss: 0.93681, time: 0.01\n",
      "Epoch[6/1000] train loss: 0.85611, val loss: 0.93456, time: 0.01\n",
      "Epoch[7/1000] train loss: 0.91884, val loss: 0.93226, time: 0.01\n",
      "Epoch[8/1000] train loss: 0.85908, val loss: 0.92989, time: 0.01\n",
      "Epoch[9/1000] train loss: 0.76455, val loss: 0.92753, time: 0.01\n",
      "Epoch[10/1000] train loss: 0.90634, val loss: 0.92519, time: 0.01\n",
      "Epoch[11/1000] train loss: 0.88559, val loss: 0.92280, time: 0.01\n",
      "Epoch[12/1000] train loss: 0.90766, val loss: 0.92038, time: 0.01\n",
      "Epoch[13/1000] train loss: 0.79709, val loss: 0.91813, time: 0.01\n",
      "Epoch[14/1000] train loss: 0.91652, val loss: 0.91585, time: 0.01\n",
      "Epoch[15/1000] train loss: 0.88820, val loss: 0.91333, time: 0.01\n",
      "Epoch[16/1000] train loss: 0.69491, val loss: 0.91058, time: 0.01\n",
      "Epoch[17/1000] train loss: 0.88248, val loss: 0.90783, time: 0.01\n",
      "Epoch[18/1000] train loss: 0.86723, val loss: 0.90501, time: 0.01\n",
      "Epoch[19/1000] train loss: 0.70224, val loss: 0.90196, time: 0.01\n",
      "Epoch[20/1000] train loss: 0.89165, val loss: 0.89878, time: 0.01\n",
      "Epoch[21/1000] train loss: 0.69594, val loss: 0.89541, time: 0.01\n",
      "Epoch[22/1000] train loss: 0.87455, val loss: 0.89193, time: 0.01\n",
      "Epoch[23/1000] train loss: 0.85762, val loss: 0.88796, time: 0.01\n",
      "Epoch[24/1000] train loss: 0.84691, val loss: 0.88353, time: 0.01\n",
      "Epoch[25/1000] train loss: 0.84318, val loss: 0.87871, time: 0.01\n",
      "Epoch[26/1000] train loss: 0.85260, val loss: 0.87356, time: 0.01\n",
      "Epoch[27/1000] train loss: 0.67240, val loss: 0.86825, time: 0.01\n",
      "Epoch[28/1000] train loss: 0.83981, val loss: 0.86266, time: 0.01\n",
      "Epoch[29/1000] train loss: 0.81163, val loss: 0.85648, time: 0.01\n",
      "Epoch[30/1000] train loss: 0.75934, val loss: 0.84990, time: 0.01\n",
      "Epoch[31/1000] train loss: 0.80012, val loss: 0.84315, time: 0.01\n",
      "Epoch[32/1000] train loss: 0.70719, val loss: 0.83578, time: 0.01\n",
      "Epoch[33/1000] train loss: 0.69639, val loss: 0.82786, time: 0.01\n",
      "Epoch[34/1000] train loss: 0.84241, val loss: 0.81953, time: 0.01\n",
      "Epoch[35/1000] train loss: 0.66487, val loss: 0.81061, time: 0.01\n",
      "Epoch[36/1000] train loss: 0.74622, val loss: 0.80043, time: 0.01\n",
      "Epoch[37/1000] train loss: 0.72028, val loss: 0.78924, time: 0.01\n",
      "Epoch[38/1000] train loss: 0.79944, val loss: 0.77767, time: 0.01\n",
      "Epoch[39/1000] train loss: 0.76695, val loss: 0.76507, time: 0.01\n",
      "Epoch[40/1000] train loss: 0.66207, val loss: 0.75143, time: 0.01\n",
      "Epoch[41/1000] train loss: 0.73648, val loss: 0.73659, time: 0.01\n",
      "Epoch[42/1000] train loss: 0.67779, val loss: 0.72035, time: 0.01\n",
      "Epoch[43/1000] train loss: 0.78395, val loss: 0.70310, time: 0.01\n",
      "Epoch[44/1000] train loss: 0.60191, val loss: 0.68528, time: 0.01\n",
      "Epoch[45/1000] train loss: 0.51793, val loss: 0.66581, time: 0.01\n",
      "Epoch[46/1000] train loss: 0.67069, val loss: 0.64588, time: 0.01\n",
      "Epoch[47/1000] train loss: 0.75647, val loss: 0.62647, time: 0.01\n",
      "Epoch[48/1000] train loss: 0.49818, val loss: 0.60678, time: 0.01\n",
      "Epoch[49/1000] train loss: 0.52466, val loss: 0.58472, time: 0.01\n",
      "Epoch[50/1000] train loss: 0.46277, val loss: 0.56192, time: 0.01\n",
      "Epoch[51/1000] train loss: 0.67246, val loss: 0.54059, time: 0.01\n",
      "Epoch[52/1000] train loss: 0.41477, val loss: 0.52054, time: 0.01\n",
      "Epoch[53/1000] train loss: 0.42890, val loss: 0.50191, time: 0.01\n",
      "Epoch[54/1000] train loss: 0.66597, val loss: 0.48493, time: 0.01\n",
      "Epoch[55/1000] train loss: 0.62510, val loss: 0.46915, time: 0.01\n",
      "Epoch[56/1000] train loss: 0.57176, val loss: 0.45465, time: 0.01\n",
      "Epoch[57/1000] train loss: 0.33870, val loss: 0.44110, time: 0.01\n",
      "Epoch[58/1000] train loss: 0.38614, val loss: 0.42741, time: 0.01\n",
      "Epoch[59/1000] train loss: 0.41705, val loss: 0.41335, time: 0.01\n",
      "Epoch[60/1000] train loss: 0.46034, val loss: 0.40063, time: 0.01\n",
      "Epoch[61/1000] train loss: 0.57277, val loss: 0.38997, time: 0.01\n",
      "Epoch[62/1000] train loss: 0.41910, val loss: 0.38148, time: 0.01\n",
      "Epoch[63/1000] train loss: 0.35372, val loss: 0.37514, time: 0.01\n",
      "Epoch[64/1000] train loss: 0.29760, val loss: 0.36931, time: 0.01\n",
      "Epoch[65/1000] train loss: 0.35565, val loss: 0.36350, time: 0.01\n",
      "Epoch[66/1000] train loss: 0.28958, val loss: 0.35855, time: 0.01\n",
      "Epoch[67/1000] train loss: 0.28378, val loss: 0.35470, time: 0.01\n",
      "Epoch[68/1000] train loss: 0.31523, val loss: 0.35170, time: 0.01\n",
      "Epoch[69/1000] train loss: 0.54380, val loss: 0.34950, time: 0.01\n",
      "Epoch[70/1000] train loss: 0.28434, val loss: 0.34706, time: 0.01\n",
      "Epoch[71/1000] train loss: 0.32528, val loss: 0.34483, time: 0.01\n",
      "Epoch[72/1000] train loss: 0.54676, val loss: 0.34278, time: 0.01\n",
      "Epoch[73/1000] train loss: 0.34285, val loss: 0.34045, time: 0.01\n",
      "Epoch[74/1000] train loss: 0.27276, val loss: 0.33840, time: 0.01\n",
      "Epoch[75/1000] train loss: 0.49939, val loss: 0.33629, time: 0.01\n",
      "Epoch[76/1000] train loss: 0.26280, val loss: 0.33435, time: 0.01\n",
      "Epoch[77/1000] train loss: 0.32740, val loss: 0.33233, time: 0.01\n",
      "Epoch[78/1000] train loss: 0.34965, val loss: 0.32839, time: 0.01\n",
      "Epoch[79/1000] train loss: 0.35233, val loss: 0.32377, time: 0.01\n",
      "Epoch[80/1000] train loss: 0.41734, val loss: 0.32022, time: 0.01\n",
      "Epoch[81/1000] train loss: 0.25606, val loss: 0.31759, time: 0.01\n",
      "Epoch[82/1000] train loss: 0.31485, val loss: 0.31624, time: 0.01\n",
      "Epoch[83/1000] train loss: 0.36285, val loss: 0.31556, time: 0.01\n",
      "Epoch[84/1000] train loss: 0.26987, val loss: 0.31428, time: 0.01\n",
      "Epoch[85/1000] train loss: 0.35609, val loss: 0.31340, time: 0.01\n",
      "Epoch[86/1000] train loss: 0.32864, val loss: 0.31171, time: 0.01\n",
      "Epoch[87/1000] train loss: 0.24968, val loss: 0.30979, time: 0.01\n",
      "Epoch[88/1000] train loss: 0.43292, val loss: 0.30716, time: 0.01\n",
      "Epoch[89/1000] train loss: 0.25284, val loss: 0.30508, time: 0.01\n",
      "Epoch[90/1000] train loss: 0.32663, val loss: 0.30345, time: 0.01\n",
      "Epoch[91/1000] train loss: 0.24608, val loss: 0.30178, time: 0.01\n",
      "Epoch[92/1000] train loss: 0.23220, val loss: 0.30044, time: 0.01\n",
      "Epoch[93/1000] train loss: 0.39817, val loss: 0.29926, time: 0.01\n",
      "Epoch[94/1000] train loss: 0.24700, val loss: 0.29764, time: 0.01\n",
      "Epoch[95/1000] train loss: 0.39029, val loss: 0.29579, time: 0.01\n",
      "Epoch[96/1000] train loss: 0.24106, val loss: 0.29415, time: 0.01\n",
      "Epoch[97/1000] train loss: 0.33389, val loss: 0.29348, time: 0.01\n",
      "Epoch[98/1000] train loss: 0.46665, val loss: 0.29292, time: 0.01\n",
      "Epoch[99/1000] train loss: 0.21886, val loss: 0.29211, time: 0.01\n",
      "Epoch[100/1000] train loss: 0.28834, val loss: 0.29239, time: 0.01\n",
      "Epoch[101/1000] train loss: 0.29600, val loss: 0.29315, time: 0.01\n",
      "Epoch[102/1000] train loss: 0.31863, val loss: 0.29304, time: 0.01\n",
      "Epoch[103/1000] train loss: 0.23941, val loss: 0.29280, time: 0.01\n",
      "Epoch[104/1000] train loss: 0.41562, val loss: 0.29272, time: 0.01\n",
      "Epoch[105/1000] train loss: 0.28895, val loss: 0.29175, time: 0.01\n",
      "Epoch[106/1000] train loss: 0.32667, val loss: 0.28958, time: 0.01\n",
      "Epoch[107/1000] train loss: 0.26528, val loss: 0.28566, time: 0.01\n",
      "Epoch[108/1000] train loss: 0.26815, val loss: 0.28165, time: 0.01\n",
      "Epoch[109/1000] train loss: 0.22946, val loss: 0.27828, time: 0.01\n",
      "Epoch[110/1000] train loss: 0.23343, val loss: 0.27453, time: 0.01\n",
      "Epoch[111/1000] train loss: 0.26171, val loss: 0.27144, time: 0.01\n",
      "Epoch[112/1000] train loss: 0.29473, val loss: 0.26994, time: 0.01\n",
      "Epoch[113/1000] train loss: 0.45327, val loss: 0.26811, time: 0.01\n",
      "Epoch[114/1000] train loss: 0.22127, val loss: 0.26588, time: 0.01\n",
      "Epoch[115/1000] train loss: 0.28834, val loss: 0.26334, time: 0.01\n",
      "Epoch[116/1000] train loss: 0.19974, val loss: 0.26123, time: 0.01\n",
      "Epoch[117/1000] train loss: 0.31969, val loss: 0.26053, time: 0.01\n",
      "Epoch[118/1000] train loss: 0.30481, val loss: 0.26117, time: 0.01\n",
      "Epoch[119/1000] train loss: 0.31142, val loss: 0.26073, time: 0.01\n",
      "Epoch[120/1000] train loss: 0.38498, val loss: 0.25967, time: 0.01\n",
      "Epoch[121/1000] train loss: 0.28454, val loss: 0.25771, time: 0.01\n",
      "Epoch[122/1000] train loss: 0.25531, val loss: 0.25583, time: 0.01\n",
      "Epoch[123/1000] train loss: 0.33440, val loss: 0.25376, time: 0.01\n",
      "Epoch[124/1000] train loss: 0.21935, val loss: 0.25137, time: 0.01\n",
      "Epoch[125/1000] train loss: 0.24325, val loss: 0.24888, time: 0.01\n",
      "Epoch[126/1000] train loss: 0.27953, val loss: 0.24545, time: 0.01\n",
      "Epoch[127/1000] train loss: 0.28920, val loss: 0.24182, time: 0.01\n",
      "Epoch[128/1000] train loss: 0.27268, val loss: 0.23854, time: 0.01\n",
      "Epoch[129/1000] train loss: 0.20907, val loss: 0.23540, time: 0.01\n",
      "Epoch[130/1000] train loss: 0.18577, val loss: 0.23226, time: 0.01\n",
      "Epoch[131/1000] train loss: 0.40239, val loss: 0.22946, time: 0.01\n",
      "Epoch[132/1000] train loss: 0.26901, val loss: 0.22676, time: 0.01\n",
      "Epoch[133/1000] train loss: 0.29823, val loss: 0.22392, time: 0.01\n",
      "Epoch[134/1000] train loss: 0.22435, val loss: 0.22114, time: 0.01\n",
      "Epoch[135/1000] train loss: 0.21664, val loss: 0.21872, time: 0.01\n",
      "Epoch[136/1000] train loss: 0.22159, val loss: 0.21675, time: 0.01\n",
      "Epoch[137/1000] train loss: 0.26945, val loss: 0.21596, time: 0.01\n",
      "Epoch[138/1000] train loss: 0.39450, val loss: 0.21477, time: 0.01\n",
      "Epoch[139/1000] train loss: 0.35154, val loss: 0.21278, time: 0.01\n",
      "Epoch[140/1000] train loss: 0.18076, val loss: 0.21035, time: 0.01\n",
      "Epoch[141/1000] train loss: 0.29999, val loss: 0.20743, time: 0.01\n",
      "Epoch[142/1000] train loss: 0.27781, val loss: 0.20344, time: 0.01\n",
      "Epoch[143/1000] train loss: 0.19399, val loss: 0.19972, time: 0.01\n",
      "Epoch[144/1000] train loss: 0.21542, val loss: 0.19615, time: 0.01\n",
      "Epoch[145/1000] train loss: 0.24506, val loss: 0.19311, time: 0.01\n",
      "Epoch[146/1000] train loss: 0.18254, val loss: 0.19045, time: 0.01\n",
      "Epoch[147/1000] train loss: 0.17128, val loss: 0.18824, time: 0.01\n",
      "Epoch[148/1000] train loss: 0.20482, val loss: 0.18652, time: 0.01\n",
      "Epoch[149/1000] train loss: 0.20194, val loss: 0.18497, time: 0.01\n",
      "Epoch[150/1000] train loss: 0.16577, val loss: 0.18374, time: 0.01\n",
      "Epoch[151/1000] train loss: 0.19913, val loss: 0.18247, time: 0.01\n",
      "Epoch[152/1000] train loss: 0.18447, val loss: 0.18038, time: 0.01\n",
      "Epoch[153/1000] train loss: 0.32650, val loss: 0.17804, time: 0.01\n",
      "Epoch[154/1000] train loss: 0.19079, val loss: 0.17616, time: 0.01\n",
      "Epoch[155/1000] train loss: 0.18187, val loss: 0.17509, time: 0.01\n",
      "Epoch[156/1000] train loss: 0.17971, val loss: 0.17410, time: 0.01\n",
      "Epoch[157/1000] train loss: 0.35232, val loss: 0.17283, time: 0.01\n",
      "Epoch[158/1000] train loss: 0.39715, val loss: 0.17052, time: 0.01\n",
      "Epoch[159/1000] train loss: 0.24204, val loss: 0.16912, time: 0.01\n",
      "Epoch[160/1000] train loss: 0.24391, val loss: 0.16956, time: 0.01\n",
      "Epoch[161/1000] train loss: 0.21765, val loss: 0.16822, time: 0.01\n",
      "Epoch[162/1000] train loss: 0.22767, val loss: 0.16647, time: 0.01\n",
      "Epoch[163/1000] train loss: 0.16497, val loss: 0.16454, time: 0.01\n",
      "Epoch[164/1000] train loss: 0.27420, val loss: 0.16228, time: 0.01\n",
      "Epoch[165/1000] train loss: 0.20848, val loss: 0.16145, time: 0.01\n",
      "Epoch[166/1000] train loss: 0.16342, val loss: 0.16208, time: 0.01\n",
      "Epoch[167/1000] train loss: 0.16228, val loss: 0.16131, time: 0.01\n",
      "Epoch[168/1000] train loss: 0.31974, val loss: 0.15942, time: 0.01\n",
      "Epoch[169/1000] train loss: 0.22065, val loss: 0.15723, time: 0.01\n",
      "Epoch[170/1000] train loss: 0.22764, val loss: 0.15653, time: 0.01\n",
      "Epoch[171/1000] train loss: 0.24588, val loss: 0.15828, time: 0.01\n",
      "Epoch[172/1000] train loss: 0.23135, val loss: 0.15855, time: 0.01\n",
      "Epoch[173/1000] train loss: 0.20836, val loss: 0.15753, time: 0.01\n",
      "Epoch[174/1000] train loss: 0.24841, val loss: 0.15635, time: 0.01\n",
      "Epoch[175/1000] train loss: 0.24738, val loss: 0.15561, time: 0.01\n",
      "Epoch[176/1000] train loss: 0.21745, val loss: 0.15422, time: 0.01\n",
      "Epoch[177/1000] train loss: 0.25262, val loss: 0.15190, time: 0.01\n",
      "Epoch[178/1000] train loss: 0.17202, val loss: 0.14911, time: 0.01\n",
      "Epoch[179/1000] train loss: 0.18994, val loss: 0.14558, time: 0.01\n",
      "Epoch[180/1000] train loss: 0.17601, val loss: 0.14193, time: 0.01\n",
      "Epoch[181/1000] train loss: 0.27676, val loss: 0.13863, time: 0.01\n",
      "Epoch[182/1000] train loss: 0.27453, val loss: 0.13597, time: 0.01\n",
      "Epoch[183/1000] train loss: 0.14705, val loss: 0.13342, time: 0.01\n",
      "Epoch[184/1000] train loss: 0.24523, val loss: 0.13123, time: 0.01\n",
      "Epoch[185/1000] train loss: 0.22905, val loss: 0.12898, time: 0.01\n",
      "Epoch[186/1000] train loss: 0.25148, val loss: 0.12694, time: 0.01\n",
      "Epoch[187/1000] train loss: 0.25840, val loss: 0.12518, time: 0.01\n",
      "Epoch[188/1000] train loss: 0.27246, val loss: 0.12335, time: 0.01\n",
      "Epoch[189/1000] train loss: 0.25963, val loss: 0.12148, time: 0.01\n",
      "Epoch[190/1000] train loss: 0.29889, val loss: 0.12038, time: 0.01\n",
      "Epoch[191/1000] train loss: 0.21117, val loss: 0.12091, time: 0.01\n",
      "Epoch[192/1000] train loss: 0.22024, val loss: 0.12165, time: 0.01\n",
      "Epoch[193/1000] train loss: 0.22290, val loss: 0.12197, time: 0.01\n",
      "Epoch[194/1000] train loss: 0.29701, val loss: 0.12147, time: 0.01\n",
      "Epoch[195/1000] train loss: 0.19564, val loss: 0.12046, time: 0.01\n",
      "Epoch[196/1000] train loss: 0.19767, val loss: 0.11939, time: 0.01\n",
      "Epoch[197/1000] train loss: 0.16482, val loss: 0.11762, time: 0.01\n",
      "Epoch[198/1000] train loss: 0.13983, val loss: 0.11595, time: 0.01\n",
      "Epoch[199/1000] train loss: 0.17709, val loss: 0.11449, time: 0.01\n",
      "Epoch[200/1000] train loss: 0.18347, val loss: 0.11337, time: 0.01\n",
      "Epoch[201/1000] train loss: 0.16883, val loss: 0.11246, time: 0.01\n",
      "Epoch[202/1000] train loss: 0.15067, val loss: 0.11173, time: 0.01\n",
      "Epoch[203/1000] train loss: 0.18720, val loss: 0.11090, time: 0.01\n",
      "Epoch[204/1000] train loss: 0.21826, val loss: 0.11023, time: 0.01\n",
      "Epoch[205/1000] train loss: 0.14515, val loss: 0.10972, time: 0.01\n",
      "Epoch[206/1000] train loss: 0.23467, val loss: 0.11044, time: 0.01\n",
      "Epoch[207/1000] train loss: 0.14092, val loss: 0.11314, time: 0.01\n",
      "Epoch[208/1000] train loss: 0.16580, val loss: 0.11536, time: 0.01\n",
      "Epoch[209/1000] train loss: 0.21917, val loss: 0.11678, time: 0.01\n",
      "Epoch[210/1000] train loss: 0.15921, val loss: 0.11719, time: 0.01\n",
      "Epoch[211/1000] train loss: 0.20268, val loss: 0.11685, time: 0.01\n",
      "Epoch[212/1000] train loss: 0.14950, val loss: 0.11578, time: 0.01\n",
      "Epoch[213/1000] train loss: 0.18053, val loss: 0.11443, time: 0.01\n",
      "Epoch[214/1000] train loss: 0.17004, val loss: 0.11261, time: 0.01\n",
      "Epoch[215/1000] train loss: 0.15622, val loss: 0.11081, time: 0.01\n",
      "Epoch[216/1000] train loss: 0.15864, val loss: 0.10953, time: 0.01\n",
      "Epoch[217/1000] train loss: 0.14554, val loss: 0.10845, time: 0.01\n",
      "Epoch[218/1000] train loss: 0.18410, val loss: 0.10824, time: 0.01\n",
      "Epoch[219/1000] train loss: 0.28370, val loss: 0.10892, time: 0.01\n",
      "Epoch[220/1000] train loss: 0.15685, val loss: 0.10954, time: 0.01\n",
      "Epoch[221/1000] train loss: 0.17601, val loss: 0.10925, time: 0.01\n",
      "Epoch[222/1000] train loss: 0.20688, val loss: 0.10883, time: 0.01\n",
      "Epoch[223/1000] train loss: 0.25399, val loss: 0.10765, time: 0.01\n",
      "Epoch[224/1000] train loss: 0.17230, val loss: 0.10560, time: 0.01\n",
      "Epoch[225/1000] train loss: 0.12877, val loss: 0.10316, time: 0.01\n",
      "Epoch[226/1000] train loss: 0.16271, val loss: 0.10096, time: 0.01\n",
      "Epoch[227/1000] train loss: 0.12826, val loss: 0.09897, time: 0.01\n",
      "Epoch[228/1000] train loss: 0.17796, val loss: 0.09754, time: 0.01\n",
      "Epoch[229/1000] train loss: 0.23403, val loss: 0.09617, time: 0.01\n",
      "Epoch[230/1000] train loss: 0.16645, val loss: 0.09476, time: 0.01\n",
      "Epoch[231/1000] train loss: 0.15316, val loss: 0.09357, time: 0.01\n",
      "Epoch[232/1000] train loss: 0.33574, val loss: 0.09272, time: 0.01\n",
      "Epoch[233/1000] train loss: 0.12908, val loss: 0.09174, time: 0.01\n",
      "Epoch[234/1000] train loss: 0.17454, val loss: 0.09118, time: 0.01\n",
      "Epoch[235/1000] train loss: 0.29242, val loss: 0.09064, time: 0.01\n",
      "Epoch[236/1000] train loss: 0.14613, val loss: 0.09011, time: 0.01\n",
      "Epoch[237/1000] train loss: 0.12846, val loss: 0.09041, time: 0.01\n",
      "Epoch[238/1000] train loss: 0.14123, val loss: 0.09060, time: 0.01\n",
      "Epoch[239/1000] train loss: 0.15681, val loss: 0.09075, time: 0.01\n",
      "Epoch[240/1000] train loss: 0.21300, val loss: 0.09042, time: 0.01\n",
      "Epoch[241/1000] train loss: 0.14417, val loss: 0.09039, time: 0.01\n",
      "Epoch[242/1000] train loss: 0.14009, val loss: 0.09031, time: 0.01\n",
      "Epoch[243/1000] train loss: 0.13535, val loss: 0.08928, time: 0.01\n",
      "Epoch[244/1000] train loss: 0.25583, val loss: 0.08766, time: 0.01\n",
      "Epoch[245/1000] train loss: 0.22105, val loss: 0.08616, time: 0.01\n",
      "Epoch[246/1000] train loss: 0.18967, val loss: 0.08537, time: 0.01\n",
      "Epoch[247/1000] train loss: 0.19292, val loss: 0.08478, time: 0.01\n",
      "Epoch[248/1000] train loss: 0.20854, val loss: 0.08425, time: 0.01\n",
      "Epoch[249/1000] train loss: 0.20050, val loss: 0.08348, time: 0.01\n",
      "Epoch[250/1000] train loss: 0.14558, val loss: 0.08263, time: 0.01\n",
      "Epoch[251/1000] train loss: 0.12781, val loss: 0.08156, time: 0.01\n",
      "Epoch[252/1000] train loss: 0.16417, val loss: 0.08050, time: 0.01\n",
      "Epoch[253/1000] train loss: 0.16128, val loss: 0.07956, time: 0.01\n",
      "Epoch[254/1000] train loss: 0.13185, val loss: 0.07901, time: 0.01\n",
      "Epoch[255/1000] train loss: 0.14307, val loss: 0.07859, time: 0.01\n",
      "Epoch[256/1000] train loss: 0.18557, val loss: 0.07814, time: 0.01\n",
      "Epoch[257/1000] train loss: 0.23266, val loss: 0.07787, time: 0.01\n",
      "Epoch[258/1000] train loss: 0.12800, val loss: 0.07818, time: 0.01\n",
      "Epoch[259/1000] train loss: 0.14588, val loss: 0.07925, time: 0.01\n",
      "Epoch[260/1000] train loss: 0.13021, val loss: 0.08105, time: 0.01\n",
      "Epoch[261/1000] train loss: 0.12005, val loss: 0.08228, time: 0.01\n",
      "Epoch[262/1000] train loss: 0.12150, val loss: 0.08312, time: 0.01\n",
      "Epoch[263/1000] train loss: 0.11875, val loss: 0.08378, time: 0.01\n",
      "Epoch[264/1000] train loss: 0.13933, val loss: 0.08500, time: 0.01\n",
      "Epoch[265/1000] train loss: 0.19693, val loss: 0.08635, time: 0.01\n",
      "Epoch[266/1000] train loss: 0.21976, val loss: 0.08649, time: 0.01\n",
      "Epoch[267/1000] train loss: 0.13523, val loss: 0.08534, time: 0.01\n",
      "Epoch[268/1000] train loss: 0.13305, val loss: 0.08330, time: 0.01\n",
      "Epoch[269/1000] train loss: 0.16867, val loss: 0.08126, time: 0.01\n",
      "Epoch[270/1000] train loss: 0.34994, val loss: 0.07952, time: 0.01\n",
      "Epoch[271/1000] train loss: 0.14335, val loss: 0.07811, time: 0.01\n",
      "Epoch[272/1000] train loss: 0.13766, val loss: 0.07617, time: 0.01\n",
      "Epoch[273/1000] train loss: 0.21491, val loss: 0.07412, time: 0.01\n",
      "Epoch[274/1000] train loss: 0.17442, val loss: 0.07246, time: 0.01\n",
      "Epoch[275/1000] train loss: 0.13229, val loss: 0.07162, time: 0.01\n",
      "Epoch[276/1000] train loss: 0.22976, val loss: 0.07202, time: 0.01\n",
      "Epoch[277/1000] train loss: 0.18300, val loss: 0.07201, time: 0.01\n",
      "Epoch[278/1000] train loss: 0.23107, val loss: 0.07166, time: 0.01\n",
      "Epoch[279/1000] train loss: 0.11298, val loss: 0.07161, time: 0.01\n",
      "Epoch[280/1000] train loss: 0.20385, val loss: 0.07234, time: 0.01\n",
      "Epoch[281/1000] train loss: 0.12479, val loss: 0.07413, time: 0.01\n",
      "Epoch[282/1000] train loss: 0.15201, val loss: 0.07633, time: 0.01\n",
      "Epoch[283/1000] train loss: 0.14206, val loss: 0.07705, time: 0.01\n",
      "Epoch[284/1000] train loss: 0.13317, val loss: 0.07703, time: 0.01\n",
      "Epoch[285/1000] train loss: 0.10862, val loss: 0.07782, time: 0.01\n",
      "Epoch[286/1000] train loss: 0.13815, val loss: 0.07838, time: 0.01\n",
      "Epoch[287/1000] train loss: 0.13724, val loss: 0.07873, time: 0.01\n",
      "Epoch[288/1000] train loss: 0.17034, val loss: 0.07879, time: 0.01\n",
      "Epoch[289/1000] train loss: 0.18358, val loss: 0.07752, time: 0.01\n",
      "Epoch[290/1000] train loss: 0.17486, val loss: 0.07577, time: 0.01\n",
      "Epoch[291/1000] train loss: 0.17111, val loss: 0.07380, time: 0.01\n",
      "Epoch[292/1000] train loss: 0.14531, val loss: 0.07182, time: 0.01\n",
      "Epoch[293/1000] train loss: 0.10719, val loss: 0.07002, time: 0.01\n",
      "Epoch[294/1000] train loss: 0.14725, val loss: 0.06855, time: 0.01\n",
      "Epoch[295/1000] train loss: 0.12002, val loss: 0.06760, time: 0.01\n",
      "Epoch[296/1000] train loss: 0.23608, val loss: 0.06643, time: 0.01\n",
      "Epoch[297/1000] train loss: 0.17193, val loss: 0.06466, time: 0.01\n",
      "Epoch[298/1000] train loss: 0.13313, val loss: 0.06294, time: 0.01\n",
      "Epoch[299/1000] train loss: 0.14023, val loss: 0.06198, time: 0.01\n",
      "Epoch[300/1000] train loss: 0.16272, val loss: 0.06226, time: 0.01\n",
      "Epoch[301/1000] train loss: 0.11136, val loss: 0.06223, time: 0.01\n",
      "Epoch[302/1000] train loss: 0.20388, val loss: 0.06265, time: 0.01\n",
      "Epoch[303/1000] train loss: 0.26081, val loss: 0.06407, time: 0.01\n",
      "Epoch[304/1000] train loss: 0.14411, val loss: 0.06507, time: 0.01\n",
      "Epoch[305/1000] train loss: 0.17582, val loss: 0.06562, time: 0.01\n",
      "Epoch[306/1000] train loss: 0.11219, val loss: 0.06624, time: 0.01\n",
      "Epoch[307/1000] train loss: 0.12822, val loss: 0.06632, time: 0.01\n",
      "Epoch[308/1000] train loss: 0.12326, val loss: 0.06498, time: 0.01\n",
      "Epoch[309/1000] train loss: 0.13205, val loss: 0.06318, time: 0.01\n",
      "Epoch[310/1000] train loss: 0.10722, val loss: 0.06189, time: 0.01\n",
      "Epoch[311/1000] train loss: 0.15781, val loss: 0.06202, time: 0.01\n",
      "Epoch[312/1000] train loss: 0.34924, val loss: 0.06448, time: 0.01\n",
      "Epoch[313/1000] train loss: 0.13119, val loss: 0.06716, time: 0.01\n",
      "Epoch[314/1000] train loss: 0.26044, val loss: 0.06881, time: 0.01\n",
      "Epoch[315/1000] train loss: 0.16575, val loss: 0.06972, time: 0.01\n",
      "Epoch[316/1000] train loss: 0.15564, val loss: 0.07014, time: 0.01\n",
      "Epoch[317/1000] train loss: 0.10917, val loss: 0.06945, time: 0.01\n",
      "Epoch[318/1000] train loss: 0.10822, val loss: 0.06753, time: 0.01\n",
      "Epoch[319/1000] train loss: 0.11203, val loss: 0.06595, time: 0.01\n",
      "Epoch[320/1000] train loss: 0.12757, val loss: 0.06523, time: 0.01\n",
      "Epoch[321/1000] train loss: 0.12198, val loss: 0.06437, time: 0.01\n",
      "Epoch[322/1000] train loss: 0.13784, val loss: 0.06303, time: 0.01\n",
      "Epoch[323/1000] train loss: 0.13679, val loss: 0.06161, time: 0.01\n",
      "Epoch[324/1000] train loss: 0.14596, val loss: 0.06044, time: 0.01\n",
      "Epoch[325/1000] train loss: 0.12529, val loss: 0.06012, time: 0.01\n",
      "Epoch[326/1000] train loss: 0.12943, val loss: 0.06051, time: 0.01\n",
      "Epoch[327/1000] train loss: 0.09473, val loss: 0.06022, time: 0.01\n",
      "Epoch[328/1000] train loss: 0.12497, val loss: 0.05973, time: 0.01\n",
      "Epoch[329/1000] train loss: 0.15368, val loss: 0.05872, time: 0.01\n",
      "Epoch[330/1000] train loss: 0.16142, val loss: 0.05788, time: 0.01\n",
      "Epoch[331/1000] train loss: 0.17625, val loss: 0.05792, time: 0.01\n",
      "Epoch[332/1000] train loss: 0.14684, val loss: 0.05843, time: 0.01\n",
      "Epoch[333/1000] train loss: 0.12237, val loss: 0.05842, time: 0.01\n",
      "Epoch[334/1000] train loss: 0.10740, val loss: 0.05839, time: 0.01\n",
      "Epoch[335/1000] train loss: 0.11332, val loss: 0.05840, time: 0.01\n",
      "Epoch[336/1000] train loss: 0.17310, val loss: 0.05875, time: 0.01\n",
      "Epoch[337/1000] train loss: 0.12639, val loss: 0.05906, time: 0.01\n",
      "Epoch[338/1000] train loss: 0.13209, val loss: 0.05947, time: 0.01\n",
      "Epoch[339/1000] train loss: 0.12223, val loss: 0.05895, time: 0.01\n",
      "Epoch[340/1000] train loss: 0.13022, val loss: 0.05822, time: 0.01\n",
      "Epoch[341/1000] train loss: 0.14433, val loss: 0.05766, time: 0.01\n",
      "Epoch[342/1000] train loss: 0.11474, val loss: 0.05724, time: 0.01\n",
      "Epoch[343/1000] train loss: 0.09986, val loss: 0.05725, time: 0.01\n",
      "Epoch[344/1000] train loss: 0.20012, val loss: 0.05770, time: 0.01\n",
      "Epoch[345/1000] train loss: 0.12736, val loss: 0.05820, time: 0.01\n",
      "Epoch[346/1000] train loss: 0.16135, val loss: 0.05793, time: 0.01\n",
      "Epoch[347/1000] train loss: 0.14768, val loss: 0.05744, time: 0.01\n",
      "Epoch[348/1000] train loss: 0.16763, val loss: 0.05667, time: 0.01\n",
      "Epoch[349/1000] train loss: 0.13171, val loss: 0.05536, time: 0.01\n",
      "Epoch[350/1000] train loss: 0.16786, val loss: 0.05410, time: 0.01\n",
      "Epoch[351/1000] train loss: 0.10362, val loss: 0.05326, time: 0.01\n",
      "Epoch[352/1000] train loss: 0.10833, val loss: 0.05311, time: 0.01\n",
      "Epoch[353/1000] train loss: 0.11022, val loss: 0.05285, time: 0.01\n",
      "Epoch[354/1000] train loss: 0.16511, val loss: 0.05248, time: 0.01\n",
      "Epoch[355/1000] train loss: 0.24780, val loss: 0.05224, time: 0.01\n",
      "Epoch[356/1000] train loss: 0.12417, val loss: 0.05217, time: 0.01\n",
      "Epoch[357/1000] train loss: 0.13427, val loss: 0.05193, time: 0.01\n",
      "Epoch[358/1000] train loss: 0.13954, val loss: 0.05134, time: 0.01\n",
      "Epoch[359/1000] train loss: 0.14047, val loss: 0.05083, time: 0.01\n",
      "Epoch[360/1000] train loss: 0.12321, val loss: 0.05134, time: 0.01\n",
      "Epoch[361/1000] train loss: 0.28297, val loss: 0.05147, time: 0.01\n",
      "Epoch[362/1000] train loss: 0.17241, val loss: 0.05143, time: 0.01\n",
      "Epoch[363/1000] train loss: 0.12772, val loss: 0.05096, time: 0.01\n",
      "Epoch[364/1000] train loss: 0.10650, val loss: 0.04983, time: 0.01\n",
      "Epoch[365/1000] train loss: 0.10641, val loss: 0.04881, time: 0.01\n",
      "Epoch[366/1000] train loss: 0.13924, val loss: 0.04774, time: 0.01\n",
      "Epoch[367/1000] train loss: 0.13676, val loss: 0.04698, time: 0.01\n",
      "Epoch[368/1000] train loss: 0.15201, val loss: 0.04728, time: 0.01\n",
      "Epoch[369/1000] train loss: 0.15472, val loss: 0.04868, time: 0.01\n",
      "Epoch[370/1000] train loss: 0.11165, val loss: 0.04897, time: 0.01\n",
      "Epoch[371/1000] train loss: 0.18018, val loss: 0.04893, time: 0.01\n",
      "Epoch[372/1000] train loss: 0.14447, val loss: 0.04836, time: 0.01\n",
      "Epoch[373/1000] train loss: 0.15083, val loss: 0.04790, time: 0.01\n",
      "Epoch[374/1000] train loss: 0.12770, val loss: 0.04715, time: 0.01\n",
      "Epoch[375/1000] train loss: 0.16332, val loss: 0.04648, time: 0.01\n",
      "Epoch[376/1000] train loss: 0.09334, val loss: 0.04618, time: 0.01\n",
      "Epoch[377/1000] train loss: 0.12893, val loss: 0.04596, time: 0.01\n",
      "Epoch[378/1000] train loss: 0.11068, val loss: 0.04533, time: 0.01\n",
      "Epoch[379/1000] train loss: 0.12695, val loss: 0.04530, time: 0.01\n",
      "Epoch[380/1000] train loss: 0.12336, val loss: 0.04620, time: 0.01\n",
      "Epoch[381/1000] train loss: 0.09800, val loss: 0.04734, time: 0.01\n",
      "Epoch[382/1000] train loss: 0.17840, val loss: 0.04839, time: 0.01\n",
      "Epoch[383/1000] train loss: 0.13060, val loss: 0.04849, time: 0.01\n",
      "Epoch[384/1000] train loss: 0.11863, val loss: 0.04862, time: 0.01\n",
      "Epoch[385/1000] train loss: 0.10163, val loss: 0.04895, time: 0.01\n",
      "Epoch[386/1000] train loss: 0.14461, val loss: 0.04935, time: 0.01\n",
      "Epoch[387/1000] train loss: 0.13819, val loss: 0.04951, time: 0.01\n",
      "Epoch[388/1000] train loss: 0.20257, val loss: 0.04938, time: 0.01\n",
      "Epoch[389/1000] train loss: 0.11564, val loss: 0.05004, time: 0.01\n",
      "Epoch[390/1000] train loss: 0.13272, val loss: 0.05007, time: 0.01\n",
      "Epoch[391/1000] train loss: 0.11147, val loss: 0.04862, time: 0.01\n",
      "Epoch[392/1000] train loss: 0.12340, val loss: 0.04636, time: 0.01\n",
      "Epoch[393/1000] train loss: 0.14286, val loss: 0.04470, time: 0.01\n",
      "Epoch[394/1000] train loss: 0.11470, val loss: 0.04410, time: 0.01\n",
      "Epoch[395/1000] train loss: 0.11394, val loss: 0.04358, time: 0.01\n",
      "Epoch[396/1000] train loss: 0.11571, val loss: 0.04301, time: 0.01\n",
      "Epoch[397/1000] train loss: 0.10299, val loss: 0.04242, time: 0.01\n",
      "Epoch[398/1000] train loss: 0.11028, val loss: 0.04213, time: 0.01\n",
      "Epoch[399/1000] train loss: 0.11497, val loss: 0.04179, time: 0.01\n",
      "Epoch[400/1000] train loss: 0.18450, val loss: 0.04165, time: 0.01\n",
      "Epoch[401/1000] train loss: 0.10379, val loss: 0.04212, time: 0.01\n",
      "Epoch[402/1000] train loss: 0.15809, val loss: 0.04302, time: 0.01\n",
      "Epoch[403/1000] train loss: 0.21781, val loss: 0.04309, time: 0.01\n",
      "Epoch[404/1000] train loss: 0.17728, val loss: 0.04276, time: 0.01\n",
      "Epoch[405/1000] train loss: 0.20583, val loss: 0.04251, time: 0.01\n",
      "Epoch[406/1000] train loss: 0.10138, val loss: 0.04256, time: 0.01\n",
      "Epoch[407/1000] train loss: 0.10837, val loss: 0.04314, time: 0.01\n",
      "Epoch[408/1000] train loss: 0.10012, val loss: 0.04382, time: 0.01\n",
      "Epoch[409/1000] train loss: 0.16136, val loss: 0.04459, time: 0.01\n",
      "Epoch[410/1000] train loss: 0.10284, val loss: 0.04611, time: 0.01\n",
      "Epoch[411/1000] train loss: 0.17286, val loss: 0.04752, time: 0.01\n",
      "Epoch[412/1000] train loss: 0.16091, val loss: 0.04805, time: 0.01\n",
      "Epoch[413/1000] train loss: 0.11964, val loss: 0.04771, time: 0.01\n",
      "Epoch[414/1000] train loss: 0.16019, val loss: 0.04738, time: 0.01\n",
      "Epoch[415/1000] train loss: 0.10315, val loss: 0.04678, time: 0.01\n",
      "Epoch[416/1000] train loss: 0.17603, val loss: 0.04618, time: 0.01\n",
      "Epoch[417/1000] train loss: 0.12607, val loss: 0.04550, time: 0.01\n",
      "Epoch[418/1000] train loss: 0.12362, val loss: 0.04511, time: 0.01\n",
      "Epoch[419/1000] train loss: 0.18471, val loss: 0.04483, time: 0.01\n",
      "Epoch[420/1000] train loss: 0.20047, val loss: 0.04434, time: 0.01\n",
      "Epoch[421/1000] train loss: 0.17107, val loss: 0.04370, time: 0.01\n",
      "Epoch[422/1000] train loss: 0.08873, val loss: 0.04342, time: 0.01\n",
      "Epoch[423/1000] train loss: 0.17684, val loss: 0.04364, time: 0.01\n",
      "Epoch[424/1000] train loss: 0.11953, val loss: 0.04378, time: 0.01\n",
      "Epoch[425/1000] train loss: 0.12669, val loss: 0.04395, time: 0.01\n",
      "Epoch[426/1000] train loss: 0.09820, val loss: 0.04497, time: 0.01\n",
      "Epoch[427/1000] train loss: 0.19148, val loss: 0.04557, time: 0.01\n",
      "Epoch[428/1000] train loss: 0.10609, val loss: 0.04581, time: 0.01\n",
      "Epoch[429/1000] train loss: 0.11891, val loss: 0.04540, time: 0.01\n",
      "Epoch[430/1000] train loss: 0.13430, val loss: 0.04381, time: 0.01\n",
      "Epoch[431/1000] train loss: 0.10898, val loss: 0.04195, time: 0.01\n",
      "Epoch[432/1000] train loss: 0.20742, val loss: 0.04049, time: 0.01\n",
      "Epoch[433/1000] train loss: 0.11436, val loss: 0.03946, time: 0.01\n",
      "Epoch[434/1000] train loss: 0.09225, val loss: 0.03852, time: 0.01\n",
      "Epoch[435/1000] train loss: 0.08397, val loss: 0.03795, time: 0.01\n",
      "Epoch[436/1000] train loss: 0.13329, val loss: 0.03766, time: 0.01\n",
      "Epoch[437/1000] train loss: 0.18387, val loss: 0.03761, time: 0.01\n",
      "Epoch[438/1000] train loss: 0.09936, val loss: 0.03804, time: 0.01\n",
      "Epoch[439/1000] train loss: 0.12495, val loss: 0.03945, time: 0.01\n",
      "Epoch[440/1000] train loss: 0.13211, val loss: 0.04204, time: 0.01\n",
      "Epoch[441/1000] train loss: 0.09015, val loss: 0.04584, time: 0.01\n",
      "Epoch[442/1000] train loss: 0.13115, val loss: 0.04845, time: 0.01\n",
      "Epoch[443/1000] train loss: 0.11220, val loss: 0.05026, time: 0.01\n",
      "Epoch[444/1000] train loss: 0.08535, val loss: 0.05156, time: 0.01\n",
      "Epoch[445/1000] train loss: 0.09339, val loss: 0.05224, time: 0.01\n",
      "Epoch[446/1000] train loss: 0.12316, val loss: 0.05192, time: 0.01\n",
      "Epoch[447/1000] train loss: 0.14028, val loss: 0.05013, time: 0.01\n",
      "Epoch[448/1000] train loss: 0.11959, val loss: 0.04855, time: 0.01\n",
      "Epoch[449/1000] train loss: 0.17578, val loss: 0.04757, time: 0.01\n",
      "Epoch[450/1000] train loss: 0.08827, val loss: 0.04636, time: 0.01\n",
      "Epoch[451/1000] train loss: 0.14171, val loss: 0.04489, time: 0.01\n",
      "Epoch[452/1000] train loss: 0.11177, val loss: 0.04343, time: 0.01\n",
      "Epoch[453/1000] train loss: 0.10841, val loss: 0.04174, time: 0.01\n",
      "Epoch[454/1000] train loss: 0.12485, val loss: 0.04063, time: 0.01\n",
      "Epoch[455/1000] train loss: 0.12215, val loss: 0.03997, time: 0.01\n",
      "Epoch[456/1000] train loss: 0.12231, val loss: 0.03908, time: 0.01\n",
      "Epoch[457/1000] train loss: 0.16694, val loss: 0.03791, time: 0.01\n",
      "Epoch[458/1000] train loss: 0.14211, val loss: 0.03746, time: 0.01\n",
      "Epoch[459/1000] train loss: 0.10835, val loss: 0.03732, time: 0.01\n",
      "Epoch[460/1000] train loss: 0.12623, val loss: 0.03755, time: 0.01\n",
      "Epoch[461/1000] train loss: 0.21541, val loss: 0.03855, time: 0.01\n",
      "Epoch[462/1000] train loss: 0.13728, val loss: 0.03869, time: 0.01\n",
      "Epoch[463/1000] train loss: 0.08411, val loss: 0.03877, time: 0.01\n",
      "Epoch[464/1000] train loss: 0.10406, val loss: 0.03915, time: 0.01\n",
      "Epoch[465/1000] train loss: 0.10906, val loss: 0.03954, time: 0.01\n",
      "Epoch[466/1000] train loss: 0.08962, val loss: 0.03949, time: 0.01\n",
      "Epoch[467/1000] train loss: 0.10347, val loss: 0.03956, time: 0.01\n",
      "Epoch[468/1000] train loss: 0.12233, val loss: 0.03946, time: 0.01\n",
      "Epoch[469/1000] train loss: 0.12103, val loss: 0.03901, time: 0.01\n",
      "Epoch[470/1000] train loss: 0.12468, val loss: 0.03823, time: 0.01\n",
      "Epoch[471/1000] train loss: 0.10683, val loss: 0.03788, time: 0.01\n",
      "Epoch[472/1000] train loss: 0.08407, val loss: 0.03746, time: 0.01\n",
      "Epoch[473/1000] train loss: 0.16201, val loss: 0.03724, time: 0.01\n",
      "Epoch[474/1000] train loss: 0.21807, val loss: 0.03706, time: 0.01\n",
      "Epoch[475/1000] train loss: 0.10062, val loss: 0.03686, time: 0.01\n",
      "Epoch[476/1000] train loss: 0.13087, val loss: 0.03740, time: 0.01\n",
      "Epoch[477/1000] train loss: 0.13697, val loss: 0.03890, time: 0.01\n",
      "Epoch[478/1000] train loss: 0.11337, val loss: 0.04039, time: 0.01\n",
      "Epoch[479/1000] train loss: 0.14121, val loss: 0.04227, time: 0.01\n",
      "Epoch[480/1000] train loss: 0.16063, val loss: 0.04404, time: 0.01\n",
      "Epoch[481/1000] train loss: 0.12607, val loss: 0.04489, time: 0.01\n",
      "Epoch[482/1000] train loss: 0.16232, val loss: 0.04516, time: 0.01\n",
      "Epoch[483/1000] train loss: 0.08808, val loss: 0.04466, time: 0.01\n",
      "Epoch[484/1000] train loss: 0.12092, val loss: 0.04327, time: 0.01\n",
      "Epoch[485/1000] train loss: 0.19046, val loss: 0.04329, time: 0.01\n",
      "Epoch[486/1000] train loss: 0.12432, val loss: 0.04670, time: 0.01\n",
      "Epoch[487/1000] train loss: 0.13235, val loss: 0.04822, time: 0.01\n",
      "Epoch[488/1000] train loss: 0.12010, val loss: 0.04857, time: 0.01\n",
      "Epoch[489/1000] train loss: 0.09683, val loss: 0.04920, time: 0.01\n",
      "Epoch[490/1000] train loss: 0.09933, val loss: 0.05018, time: 0.01\n",
      "Epoch[491/1000] train loss: 0.16245, val loss: 0.05084, time: 0.01\n",
      "Epoch[492/1000] train loss: 0.09013, val loss: 0.05088, time: 0.01\n",
      "Epoch[493/1000] train loss: 0.13257, val loss: 0.05074, time: 0.01\n",
      "Epoch[494/1000] train loss: 0.11109, val loss: 0.05030, time: 0.01\n",
      "Epoch[495/1000] train loss: 0.14836, val loss: 0.04922, time: 0.01\n",
      "Epoch[496/1000] train loss: 0.19241, val loss: 0.04842, time: 0.01\n",
      "Epoch[497/1000] train loss: 0.10658, val loss: 0.04873, time: 0.01\n",
      "Epoch[498/1000] train loss: 0.09645, val loss: 0.04941, time: 0.01\n",
      "Epoch[499/1000] train loss: 0.09100, val loss: 0.05074, time: 0.01\n",
      "Epoch[500/1000] train loss: 0.08985, val loss: 0.05275, time: 0.01\n",
      "Epoch[501/1000] train loss: 0.12680, val loss: 0.05338, time: 0.01\n",
      "Epoch[502/1000] train loss: 0.08690, val loss: 0.05296, time: 0.01\n",
      "Epoch[503/1000] train loss: 0.14544, val loss: 0.05177, time: 0.01\n",
      "Epoch[504/1000] train loss: 0.19729, val loss: 0.04959, time: 0.01\n",
      "Epoch[505/1000] train loss: 0.09223, val loss: 0.04815, time: 0.01\n",
      "Epoch[506/1000] train loss: 0.09187, val loss: 0.04712, time: 0.01\n",
      "Epoch[507/1000] train loss: 0.16814, val loss: 0.04613, time: 0.01\n",
      "Epoch[508/1000] train loss: 0.12873, val loss: 0.04542, time: 0.01\n",
      "Epoch[509/1000] train loss: 0.16052, val loss: 0.04386, time: 0.01\n",
      "Epoch[510/1000] train loss: 0.08880, val loss: 0.04218, time: 0.01\n",
      "Epoch[511/1000] train loss: 0.11650, val loss: 0.04067, time: 0.01\n",
      "Epoch[512/1000] train loss: 0.08714, val loss: 0.03947, time: 0.01\n",
      "Epoch[513/1000] train loss: 0.11485, val loss: 0.03866, time: 0.01\n",
      "Epoch[514/1000] train loss: 0.11971, val loss: 0.03820, time: 0.01\n",
      "Epoch[515/1000] train loss: 0.10531, val loss: 0.03805, time: 0.01\n",
      "Epoch[516/1000] train loss: 0.11021, val loss: 0.03883, time: 0.01\n",
      "Epoch[517/1000] train loss: 0.11236, val loss: 0.03959, time: 0.01\n",
      "Epoch[518/1000] train loss: 0.16171, val loss: 0.04125, time: 0.01\n",
      "Epoch[519/1000] train loss: 0.11967, val loss: 0.04221, time: 0.01\n",
      "Epoch[520/1000] train loss: 0.10792, val loss: 0.04218, time: 0.01\n",
      "Epoch[521/1000] train loss: 0.12346, val loss: 0.04196, time: 0.01\n",
      "Epoch[522/1000] train loss: 0.09062, val loss: 0.04146, time: 0.01\n",
      "Epoch[523/1000] train loss: 0.12848, val loss: 0.04038, time: 0.01\n",
      "Epoch[524/1000] train loss: 0.15187, val loss: 0.03858, time: 0.01\n",
      "Epoch[525/1000] train loss: 0.09263, val loss: 0.03753, time: 0.01\n",
      "Epoch[526/1000] train loss: 0.11738, val loss: 0.03747, time: 0.01\n",
      "Epoch[527/1000] train loss: 0.11328, val loss: 0.03748, time: 0.01\n",
      "Epoch[528/1000] train loss: 0.09943, val loss: 0.03773, time: 0.01\n",
      "Epoch[529/1000] train loss: 0.09077, val loss: 0.03801, time: 0.01\n",
      "Epoch[530/1000] train loss: 0.10859, val loss: 0.03766, time: 0.01\n",
      "Epoch[531/1000] train loss: 0.08856, val loss: 0.03740, time: 0.01\n",
      "Epoch[532/1000] train loss: 0.11420, val loss: 0.03851, time: 0.01\n",
      "Epoch[533/1000] train loss: 0.11380, val loss: 0.03984, time: 0.01\n",
      "Epoch[534/1000] train loss: 0.09374, val loss: 0.04033, time: 0.01\n",
      "Epoch[535/1000] train loss: 0.08543, val loss: 0.04007, time: 0.01\n",
      "Epoch[536/1000] train loss: 0.09011, val loss: 0.03908, time: 0.01\n",
      "Epoch[537/1000] train loss: 0.14258, val loss: 0.03830, time: 0.01\n",
      "Epoch[538/1000] train loss: 0.08637, val loss: 0.03851, time: 0.01\n",
      "Epoch[539/1000] train loss: 0.12823, val loss: 0.04014, time: 0.01\n",
      "Epoch[540/1000] train loss: 0.17330, val loss: 0.04300, time: 0.01\n",
      "Epoch[541/1000] train loss: 0.11705, val loss: 0.04426, time: 0.01\n",
      "Epoch[542/1000] train loss: 0.09824, val loss: 0.04447, time: 0.01\n",
      "Epoch[543/1000] train loss: 0.16667, val loss: 0.04325, time: 0.01\n",
      "Epoch[544/1000] train loss: 0.10944, val loss: 0.04171, time: 0.01\n",
      "Epoch[545/1000] train loss: 0.08931, val loss: 0.03997, time: 0.01\n",
      "Epoch[546/1000] train loss: 0.10538, val loss: 0.03786, time: 0.01\n",
      "Epoch[547/1000] train loss: 0.11067, val loss: 0.03689, time: 0.01\n",
      "Epoch[548/1000] train loss: 0.12287, val loss: 0.03830, time: 0.01\n",
      "Epoch[549/1000] train loss: 0.09348, val loss: 0.04001, time: 0.01\n",
      "Epoch[550/1000] train loss: 0.08650, val loss: 0.04027, time: 0.01\n",
      "Epoch[551/1000] train loss: 0.10288, val loss: 0.03996, time: 0.01\n",
      "Epoch[552/1000] train loss: 0.07763, val loss: 0.03875, time: 0.01\n",
      "Epoch[553/1000] train loss: 0.09580, val loss: 0.03824, time: 0.01\n",
      "Epoch[554/1000] train loss: 0.09508, val loss: 0.03926, time: 0.01\n",
      "Epoch[555/1000] train loss: 0.11659, val loss: 0.04014, time: 0.01\n",
      "Epoch[556/1000] train loss: 0.09973, val loss: 0.03946, time: 0.01\n",
      "Epoch[557/1000] train loss: 0.10549, val loss: 0.03930, time: 0.01\n",
      "Epoch[558/1000] train loss: 0.10331, val loss: 0.04016, time: 0.01\n",
      "Epoch[559/1000] train loss: 0.08769, val loss: 0.04023, time: 0.01\n",
      "Epoch[560/1000] train loss: 0.13678, val loss: 0.03942, time: 0.01\n",
      "Epoch[561/1000] train loss: 0.15104, val loss: 0.03886, time: 0.01\n",
      "Epoch[562/1000] train loss: 0.08423, val loss: 0.03886, time: 0.01\n",
      "Epoch[563/1000] train loss: 0.11010, val loss: 0.03896, time: 0.01\n",
      "Epoch[564/1000] train loss: 0.18742, val loss: 0.03852, time: 0.01\n",
      "Epoch[565/1000] train loss: 0.14690, val loss: 0.03843, time: 0.01\n",
      "Epoch[566/1000] train loss: 0.08966, val loss: 0.03848, time: 0.01\n",
      "Epoch[567/1000] train loss: 0.16374, val loss: 0.03910, time: 0.01\n",
      "Epoch[568/1000] train loss: 0.11129, val loss: 0.04071, time: 0.01\n",
      "Epoch[569/1000] train loss: 0.09863, val loss: 0.04185, time: 0.01\n",
      "Epoch[570/1000] train loss: 0.20813, val loss: 0.04204, time: 0.01\n",
      "Epoch[571/1000] train loss: 0.11205, val loss: 0.04092, time: 0.01\n",
      "Epoch[572/1000] train loss: 0.09288, val loss: 0.03994, time: 0.01\n",
      "Epoch[573/1000] train loss: 0.09918, val loss: 0.03901, time: 0.01\n",
      "Epoch[574/1000] train loss: 0.11067, val loss: 0.03960, time: 0.01\n",
      "Epoch[575/1000] train loss: 0.21350, val loss: 0.04132, time: 0.01\n",
      "Epoch[576/1000] train loss: 0.11400, val loss: 0.04183, time: 0.01\n",
      "Epoch[577/1000] train loss: 0.11770, val loss: 0.04127, time: 0.01\n",
      "Epoch[578/1000] train loss: 0.08999, val loss: 0.04043, time: 0.01\n",
      "Epoch[579/1000] train loss: 0.14290, val loss: 0.03942, time: 0.01\n",
      "Epoch[580/1000] train loss: 0.11441, val loss: 0.03759, time: 0.01\n",
      "Epoch[581/1000] train loss: 0.23071, val loss: 0.03554, time: 0.01\n",
      "Epoch[582/1000] train loss: 0.12165, val loss: 0.03373, time: 0.01\n",
      "Epoch[583/1000] train loss: 0.08876, val loss: 0.03305, time: 0.01\n",
      "Epoch[584/1000] train loss: 0.14000, val loss: 0.03246, time: 0.01\n",
      "Epoch[585/1000] train loss: 0.13449, val loss: 0.03210, time: 0.01\n",
      "Epoch[586/1000] train loss: 0.10679, val loss: 0.03236, time: 0.01\n",
      "Epoch[587/1000] train loss: 0.10724, val loss: 0.03245, time: 0.01\n",
      "Epoch[588/1000] train loss: 0.13983, val loss: 0.03244, time: 0.01\n",
      "Epoch[589/1000] train loss: 0.10726, val loss: 0.03268, time: 0.01\n",
      "Epoch[590/1000] train loss: 0.08756, val loss: 0.03399, time: 0.01\n",
      "Epoch[591/1000] train loss: 0.09046, val loss: 0.03549, time: 0.01\n",
      "Epoch[592/1000] train loss: 0.09291, val loss: 0.03638, time: 0.01\n",
      "Epoch[593/1000] train loss: 0.17094, val loss: 0.03657, time: 0.01\n",
      "Epoch[594/1000] train loss: 0.17118, val loss: 0.03585, time: 0.01\n",
      "Epoch[595/1000] train loss: 0.09043, val loss: 0.03537, time: 0.01\n",
      "Epoch[596/1000] train loss: 0.11122, val loss: 0.03509, time: 0.01\n",
      "Epoch[597/1000] train loss: 0.09624, val loss: 0.03603, time: 0.01\n",
      "Epoch[598/1000] train loss: 0.08448, val loss: 0.03774, time: 0.01\n",
      "Epoch[599/1000] train loss: 0.08808, val loss: 0.03919, time: 0.01\n",
      "Epoch[600/1000] train loss: 0.11647, val loss: 0.04046, time: 0.01\n",
      "Epoch[601/1000] train loss: 0.11461, val loss: 0.04093, time: 0.01\n",
      "Epoch[602/1000] train loss: 0.11882, val loss: 0.04166, time: 0.01\n",
      "Epoch[603/1000] train loss: 0.13967, val loss: 0.04249, time: 0.01\n",
      "Epoch[604/1000] train loss: 0.09219, val loss: 0.04175, time: 0.01\n",
      "Epoch[605/1000] train loss: 0.13566, val loss: 0.04069, time: 0.01\n",
      "Epoch[606/1000] train loss: 0.12103, val loss: 0.03923, time: 0.01\n",
      "Epoch[607/1000] train loss: 0.20715, val loss: 0.03764, time: 0.01\n",
      "Epoch[608/1000] train loss: 0.08163, val loss: 0.03646, time: 0.01\n",
      "Epoch[609/1000] train loss: 0.11212, val loss: 0.03548, time: 0.01\n",
      "Epoch[610/1000] train loss: 0.10820, val loss: 0.03487, time: 0.01\n",
      "Epoch[611/1000] train loss: 0.09842, val loss: 0.03476, time: 0.01\n",
      "Epoch[612/1000] train loss: 0.13940, val loss: 0.03560, time: 0.01\n",
      "Epoch[613/1000] train loss: 0.08627, val loss: 0.03710, time: 0.01\n",
      "Epoch[614/1000] train loss: 0.20727, val loss: 0.03790, time: 0.01\n",
      "Epoch[615/1000] train loss: 0.11047, val loss: 0.03846, time: 0.01\n",
      "Epoch[616/1000] train loss: 0.11473, val loss: 0.03887, time: 0.01\n",
      "Epoch[617/1000] train loss: 0.08979, val loss: 0.03907, time: 0.01\n",
      "Epoch[618/1000] train loss: 0.11286, val loss: 0.03948, time: 0.01\n",
      "Epoch[619/1000] train loss: 0.10029, val loss: 0.03945, time: 0.01\n",
      "Epoch[620/1000] train loss: 0.13026, val loss: 0.03834, time: 0.01\n",
      "Epoch[621/1000] train loss: 0.07868, val loss: 0.03656, time: 0.01\n",
      "Epoch[622/1000] train loss: 0.17613, val loss: 0.03525, time: 0.01\n",
      "Epoch[623/1000] train loss: 0.10742, val loss: 0.03410, time: 0.01\n",
      "Epoch[624/1000] train loss: 0.07366, val loss: 0.03293, time: 0.01\n",
      "Epoch[625/1000] train loss: 0.12247, val loss: 0.03183, time: 0.01\n",
      "Epoch[626/1000] train loss: 0.10341, val loss: 0.03123, time: 0.01\n",
      "Epoch[627/1000] train loss: 0.09167, val loss: 0.03088, time: 0.01\n",
      "Epoch[628/1000] train loss: 0.12602, val loss: 0.03054, time: 0.01\n",
      "Epoch[629/1000] train loss: 0.11067, val loss: 0.03021, time: 0.01\n",
      "Epoch[630/1000] train loss: 0.08354, val loss: 0.03014, time: 0.01\n",
      "Epoch[631/1000] train loss: 0.08610, val loss: 0.03029, time: 0.01\n",
      "Epoch[632/1000] train loss: 0.09805, val loss: 0.03051, time: 0.01\n",
      "Epoch[633/1000] train loss: 0.07203, val loss: 0.03064, time: 0.01\n",
      "Epoch[634/1000] train loss: 0.14584, val loss: 0.03046, time: 0.01\n",
      "Epoch[635/1000] train loss: 0.14213, val loss: 0.03035, time: 0.01\n",
      "Epoch[636/1000] train loss: 0.08534, val loss: 0.03104, time: 0.01\n",
      "Epoch[637/1000] train loss: 0.13073, val loss: 0.03211, time: 0.01\n",
      "Epoch[638/1000] train loss: 0.11204, val loss: 0.03261, time: 0.01\n",
      "Epoch[639/1000] train loss: 0.12742, val loss: 0.03250, time: 0.01\n",
      "Epoch[640/1000] train loss: 0.14815, val loss: 0.03208, time: 0.01\n",
      "Epoch[641/1000] train loss: 0.12899, val loss: 0.03156, time: 0.01\n",
      "Epoch[642/1000] train loss: 0.10415, val loss: 0.03083, time: 0.01\n",
      "Epoch[643/1000] train loss: 0.08478, val loss: 0.03081, time: 0.01\n",
      "Epoch[644/1000] train loss: 0.12284, val loss: 0.03086, time: 0.01\n",
      "Epoch[645/1000] train loss: 0.12754, val loss: 0.03082, time: 0.01\n",
      "Epoch[646/1000] train loss: 0.11659, val loss: 0.03013, time: 0.01\n",
      "Epoch[647/1000] train loss: 0.09765, val loss: 0.02949, time: 0.01\n",
      "Epoch[648/1000] train loss: 0.11341, val loss: 0.02886, time: 0.01\n",
      "Epoch[649/1000] train loss: 0.09911, val loss: 0.02938, time: 0.01\n",
      "Epoch[650/1000] train loss: 0.08676, val loss: 0.03073, time: 0.01\n",
      "Epoch[651/1000] train loss: 0.13542, val loss: 0.03178, time: 0.01\n",
      "Epoch[652/1000] train loss: 0.10563, val loss: 0.03282, time: 0.01\n",
      "Epoch[653/1000] train loss: 0.08852, val loss: 0.03434, time: 0.01\n",
      "Epoch[654/1000] train loss: 0.08279, val loss: 0.03680, time: 0.01\n",
      "Epoch[655/1000] train loss: 0.13756, val loss: 0.03845, time: 0.01\n",
      "Epoch[656/1000] train loss: 0.08927, val loss: 0.03909, time: 0.01\n",
      "Epoch[657/1000] train loss: 0.13679, val loss: 0.04052, time: 0.01\n",
      "Epoch[658/1000] train loss: 0.07516, val loss: 0.04169, time: 0.01\n",
      "Epoch[659/1000] train loss: 0.09883, val loss: 0.04144, time: 0.01\n",
      "Epoch[660/1000] train loss: 0.08054, val loss: 0.04092, time: 0.01\n",
      "Epoch[661/1000] train loss: 0.12150, val loss: 0.04020, time: 0.01\n",
      "Epoch[662/1000] train loss: 0.17765, val loss: 0.03865, time: 0.01\n",
      "Epoch[663/1000] train loss: 0.18722, val loss: 0.03701, time: 0.01\n",
      "Epoch[664/1000] train loss: 0.10068, val loss: 0.03714, time: 0.01\n",
      "Epoch[665/1000] train loss: 0.09754, val loss: 0.03757, time: 0.01\n",
      "Epoch[666/1000] train loss: 0.11847, val loss: 0.03825, time: 0.01\n",
      "Epoch[667/1000] train loss: 0.08817, val loss: 0.03817, time: 0.01\n",
      "Epoch[668/1000] train loss: 0.09221, val loss: 0.03776, time: 0.01\n",
      "Epoch[669/1000] train loss: 0.07910, val loss: 0.03705, time: 0.01\n",
      "Epoch[670/1000] train loss: 0.10645, val loss: 0.03690, time: 0.01\n",
      "Epoch[671/1000] train loss: 0.09422, val loss: 0.03755, time: 0.01\n",
      "Epoch[672/1000] train loss: 0.09843, val loss: 0.03847, time: 0.01\n",
      "Epoch[673/1000] train loss: 0.10254, val loss: 0.03901, time: 0.01\n",
      "Epoch[674/1000] train loss: 0.07710, val loss: 0.03946, time: 0.01\n",
      "Epoch[675/1000] train loss: 0.09679, val loss: 0.03991, time: 0.01\n",
      "Epoch[676/1000] train loss: 0.10549, val loss: 0.04024, time: 0.01\n",
      "Epoch[677/1000] train loss: 0.09631, val loss: 0.04010, time: 0.01\n",
      "Epoch[678/1000] train loss: 0.11208, val loss: 0.04039, time: 0.01\n",
      "Epoch[679/1000] train loss: 0.08034, val loss: 0.04207, time: 0.01\n",
      "Epoch[680/1000] train loss: 0.11838, val loss: 0.04288, time: 0.01\n",
      "Epoch[681/1000] train loss: 0.08404, val loss: 0.04288, time: 0.01\n",
      "Epoch[682/1000] train loss: 0.14831, val loss: 0.04199, time: 0.01\n",
      "Epoch[683/1000] train loss: 0.10170, val loss: 0.04070, time: 0.01\n",
      "Epoch[684/1000] train loss: 0.10940, val loss: 0.03930, time: 0.01\n",
      "Epoch[685/1000] train loss: 0.09555, val loss: 0.03798, time: 0.01\n",
      "Epoch[686/1000] train loss: 0.14183, val loss: 0.03745, time: 0.01\n",
      "Epoch[687/1000] train loss: 0.08852, val loss: 0.03742, time: 0.01\n",
      "Epoch[688/1000] train loss: 0.12052, val loss: 0.03786, time: 0.01\n",
      "Epoch[689/1000] train loss: 0.08954, val loss: 0.03840, time: 0.01\n",
      "Epoch[690/1000] train loss: 0.07693, val loss: 0.03896, time: 0.01\n",
      "Epoch[691/1000] train loss: 0.06980, val loss: 0.03970, time: 0.01\n",
      "Epoch[692/1000] train loss: 0.09181, val loss: 0.03973, time: 0.01\n",
      "Epoch[693/1000] train loss: 0.10906, val loss: 0.03893, time: 0.01\n",
      "Epoch[694/1000] train loss: 0.07613, val loss: 0.03803, time: 0.01\n",
      "Epoch[695/1000] train loss: 0.09042, val loss: 0.03780, time: 0.01\n",
      "Epoch[696/1000] train loss: 0.14061, val loss: 0.03779, time: 0.01\n",
      "Epoch[697/1000] train loss: 0.11244, val loss: 0.03792, time: 0.01\n",
      "Epoch[698/1000] train loss: 0.08220, val loss: 0.03720, time: 0.01\n",
      "Epoch[699/1000] train loss: 0.12793, val loss: 0.03612, time: 0.01\n",
      "Epoch[700/1000] train loss: 0.10631, val loss: 0.03622, time: 0.01\n",
      "Epoch[701/1000] train loss: 0.17586, val loss: 0.03752, time: 0.01\n",
      "Epoch[702/1000] train loss: 0.07298, val loss: 0.03843, time: 0.01\n",
      "Epoch[703/1000] train loss: 0.09668, val loss: 0.03908, time: 0.01\n",
      "Epoch[704/1000] train loss: 0.08402, val loss: 0.03952, time: 0.01\n",
      "Epoch[705/1000] train loss: 0.08096, val loss: 0.03973, time: 0.01\n",
      "Epoch[706/1000] train loss: 0.09675, val loss: 0.03891, time: 0.01\n",
      "Epoch[707/1000] train loss: 0.08867, val loss: 0.03767, time: 0.01\n",
      "Epoch[708/1000] train loss: 0.13078, val loss: 0.03637, time: 0.01\n",
      "Epoch[709/1000] train loss: 0.10912, val loss: 0.03431, time: 0.01\n",
      "Epoch[710/1000] train loss: 0.10880, val loss: 0.03251, time: 0.01\n",
      "Epoch[711/1000] train loss: 0.09030, val loss: 0.03201, time: 0.01\n",
      "Epoch[712/1000] train loss: 0.11614, val loss: 0.03137, time: 0.01\n",
      "Epoch[713/1000] train loss: 0.09642, val loss: 0.03122, time: 0.01\n",
      "Epoch[714/1000] train loss: 0.09626, val loss: 0.03184, time: 0.01\n",
      "Epoch[715/1000] train loss: 0.10793, val loss: 0.03196, time: 0.01\n",
      "Epoch[716/1000] train loss: 0.14834, val loss: 0.03210, time: 0.01\n",
      "Epoch[717/1000] train loss: 0.08104, val loss: 0.03282, time: 0.01\n",
      "Epoch[718/1000] train loss: 0.09058, val loss: 0.03467, time: 0.01\n",
      "Epoch[719/1000] train loss: 0.13250, val loss: 0.03691, time: 0.01\n",
      "Epoch[720/1000] train loss: 0.08733, val loss: 0.03767, time: 0.01\n",
      "Epoch[721/1000] train loss: 0.09531, val loss: 0.03753, time: 0.01\n",
      "Epoch[722/1000] train loss: 0.12031, val loss: 0.03733, time: 0.01\n",
      "Epoch[723/1000] train loss: 0.09244, val loss: 0.03697, time: 0.01\n",
      "Epoch[724/1000] train loss: 0.10321, val loss: 0.03664, time: 0.01\n",
      "Epoch[725/1000] train loss: 0.09207, val loss: 0.03587, time: 0.01\n",
      "Epoch[726/1000] train loss: 0.10235, val loss: 0.03466, time: 0.01\n",
      "Epoch[727/1000] train loss: 0.10747, val loss: 0.03303, time: 0.01\n",
      "Epoch[728/1000] train loss: 0.08500, val loss: 0.03148, time: 0.01\n",
      "Epoch[729/1000] train loss: 0.08539, val loss: 0.03037, time: 0.01\n",
      "Epoch[730/1000] train loss: 0.09741, val loss: 0.02985, time: 0.01\n",
      "Epoch[731/1000] train loss: 0.08847, val loss: 0.02991, time: 0.01\n",
      "Epoch[732/1000] train loss: 0.06847, val loss: 0.03055, time: 0.01\n",
      "Epoch[733/1000] train loss: 0.09964, val loss: 0.03203, time: 0.01\n",
      "Epoch[734/1000] train loss: 0.09571, val loss: 0.03341, time: 0.01\n",
      "Epoch[735/1000] train loss: 0.07076, val loss: 0.03415, time: 0.01\n",
      "Epoch[736/1000] train loss: 0.08346, val loss: 0.03415, time: 0.01\n",
      "Epoch[737/1000] train loss: 0.10491, val loss: 0.03332, time: 0.01\n",
      "Epoch[738/1000] train loss: 0.08878, val loss: 0.03252, time: 0.01\n",
      "Epoch[739/1000] train loss: 0.10593, val loss: 0.03174, time: 0.01\n",
      "Epoch[740/1000] train loss: 0.21876, val loss: 0.03127, time: 0.01\n",
      "Epoch[741/1000] train loss: 0.22257, val loss: 0.03102, time: 0.01\n",
      "Epoch[742/1000] train loss: 0.07054, val loss: 0.03226, time: 0.01\n",
      "Epoch[743/1000] train loss: 0.07699, val loss: 0.03412, time: 0.01\n",
      "Epoch[744/1000] train loss: 0.07933, val loss: 0.03521, time: 0.01\n",
      "Epoch[745/1000] train loss: 0.12132, val loss: 0.03563, time: 0.01\n",
      "Epoch[746/1000] train loss: 0.12090, val loss: 0.03546, time: 0.01\n",
      "Epoch[747/1000] train loss: 0.09692, val loss: 0.03474, time: 0.01\n",
      "Epoch[748/1000] train loss: 0.09737, val loss: 0.03454, time: 0.01\n",
      "Epoch[749/1000] train loss: 0.14202, val loss: 0.03448, time: 0.01\n",
      "Epoch[750/1000] train loss: 0.10156, val loss: 0.03393, time: 0.01\n",
      "Epoch[751/1000] train loss: 0.14361, val loss: 0.03453, time: 0.01\n",
      "Epoch[752/1000] train loss: 0.08727, val loss: 0.03611, time: 0.01\n",
      "Epoch[753/1000] train loss: 0.14844, val loss: 0.03678, time: 0.01\n",
      "Epoch[754/1000] train loss: 0.08481, val loss: 0.03590, time: 0.01\n",
      "Epoch[755/1000] train loss: 0.08923, val loss: 0.03468, time: 0.01\n",
      "Epoch[756/1000] train loss: 0.07776, val loss: 0.03380, time: 0.01\n",
      "Epoch[757/1000] train loss: 0.12601, val loss: 0.03306, time: 0.01\n",
      "Epoch[758/1000] train loss: 0.13602, val loss: 0.03214, time: 0.01\n",
      "Epoch[759/1000] train loss: 0.16807, val loss: 0.03119, time: 0.01\n",
      "Epoch[760/1000] train loss: 0.08757, val loss: 0.03027, time: 0.01\n",
      "Epoch[761/1000] train loss: 0.12660, val loss: 0.02939, time: 0.01\n",
      "Epoch[762/1000] train loss: 0.13790, val loss: 0.02824, time: 0.01\n",
      "Epoch[763/1000] train loss: 0.07191, val loss: 0.02723, time: 0.01\n",
      "Epoch[764/1000] train loss: 0.15572, val loss: 0.02660, time: 0.01\n",
      "Epoch[765/1000] train loss: 0.11585, val loss: 0.02589, time: 0.01\n",
      "Epoch[766/1000] train loss: 0.08384, val loss: 0.02533, time: 0.01\n",
      "Epoch[767/1000] train loss: 0.14711, val loss: 0.02522, time: 0.01\n",
      "Epoch[768/1000] train loss: 0.12373, val loss: 0.02549, time: 0.01\n",
      "Epoch[769/1000] train loss: 0.06749, val loss: 0.02621, time: 0.01\n",
      "Epoch[770/1000] train loss: 0.08891, val loss: 0.02687, time: 0.01\n",
      "Epoch[771/1000] train loss: 0.11428, val loss: 0.02756, time: 0.01\n",
      "Epoch[772/1000] train loss: 0.09008, val loss: 0.02822, time: 0.01\n",
      "Epoch[773/1000] train loss: 0.09162, val loss: 0.02875, time: 0.01\n",
      "Epoch[774/1000] train loss: 0.09009, val loss: 0.02955, time: 0.01\n",
      "Epoch[775/1000] train loss: 0.10975, val loss: 0.03114, time: 0.01\n",
      "Epoch[776/1000] train loss: 0.08670, val loss: 0.03415, time: 0.01\n",
      "Epoch[777/1000] train loss: 0.10685, val loss: 0.03686, time: 0.01\n",
      "Epoch[778/1000] train loss: 0.25864, val loss: 0.03848, time: 0.01\n",
      "Epoch[779/1000] train loss: 0.09572, val loss: 0.03902, time: 0.01\n",
      "Epoch[780/1000] train loss: 0.11515, val loss: 0.03823, time: 0.01\n",
      "Epoch[781/1000] train loss: 0.14007, val loss: 0.03625, time: 0.01\n",
      "Epoch[782/1000] train loss: 0.11755, val loss: 0.03337, time: 0.01\n",
      "Epoch[783/1000] train loss: 0.08872, val loss: 0.03088, time: 0.01\n",
      "Epoch[784/1000] train loss: 0.06862, val loss: 0.03027, time: 0.01\n",
      "Epoch[785/1000] train loss: 0.07447, val loss: 0.02985, time: 0.01\n",
      "Epoch[786/1000] train loss: 0.08517, val loss: 0.02940, time: 0.01\n",
      "Epoch[787/1000] train loss: 0.07291, val loss: 0.02917, time: 0.01\n",
      "Epoch[788/1000] train loss: 0.08070, val loss: 0.02949, time: 0.01\n",
      "Epoch[789/1000] train loss: 0.10432, val loss: 0.03095, time: 0.01\n",
      "Epoch[790/1000] train loss: 0.06702, val loss: 0.03243, time: 0.01\n",
      "Epoch[791/1000] train loss: 0.09323, val loss: 0.03402, time: 0.01\n",
      "Epoch[792/1000] train loss: 0.08032, val loss: 0.03494, time: 0.01\n",
      "Epoch[793/1000] train loss: 0.07505, val loss: 0.03476, time: 0.01\n",
      "Epoch[794/1000] train loss: 0.08811, val loss: 0.03450, time: 0.01\n",
      "Epoch[795/1000] train loss: 0.11730, val loss: 0.03490, time: 0.01\n",
      "Epoch[796/1000] train loss: 0.09355, val loss: 0.03470, time: 0.01\n",
      "Epoch[797/1000] train loss: 0.12051, val loss: 0.03334, time: 0.01\n",
      "Epoch[798/1000] train loss: 0.07106, val loss: 0.03180, time: 0.01\n",
      "Epoch[799/1000] train loss: 0.14803, val loss: 0.03056, time: 0.01\n",
      "Epoch[800/1000] train loss: 0.08732, val loss: 0.03015, time: 0.01\n",
      "Epoch[801/1000] train loss: 0.07451, val loss: 0.03007, time: 0.01\n",
      "Epoch[802/1000] train loss: 0.12136, val loss: 0.03035, time: 0.01\n",
      "Epoch[803/1000] train loss: 0.06993, val loss: 0.03067, time: 0.01\n",
      "Epoch[804/1000] train loss: 0.12430, val loss: 0.03167, time: 0.01\n",
      "Epoch[805/1000] train loss: 0.07755, val loss: 0.03434, time: 0.01\n",
      "Epoch[806/1000] train loss: 0.10762, val loss: 0.03752, time: 0.01\n",
      "Epoch[807/1000] train loss: 0.07220, val loss: 0.04164, time: 0.01\n",
      "Epoch[808/1000] train loss: 0.07757, val loss: 0.04417, time: 0.01\n",
      "Epoch[809/1000] train loss: 0.07212, val loss: 0.04619, time: 0.01\n",
      "Epoch[810/1000] train loss: 0.08843, val loss: 0.04720, time: 0.01\n",
      "Epoch[811/1000] train loss: 0.07708, val loss: 0.04629, time: 0.01\n",
      "Epoch[812/1000] train loss: 0.07341, val loss: 0.04441, time: 0.01\n",
      "Epoch[813/1000] train loss: 0.12300, val loss: 0.04154, time: 0.01\n",
      "Epoch[814/1000] train loss: 0.11844, val loss: 0.03804, time: 0.01\n",
      "Epoch[815/1000] train loss: 0.11513, val loss: 0.03554, time: 0.01\n",
      "Epoch[816/1000] train loss: 0.09574, val loss: 0.03361, time: 0.01\n",
      "Epoch[817/1000] train loss: 0.11591, val loss: 0.03270, time: 0.01\n",
      "Epoch[818/1000] train loss: 0.09235, val loss: 0.03335, time: 0.01\n",
      "Epoch[819/1000] train loss: 0.07802, val loss: 0.03371, time: 0.01\n",
      "Epoch[820/1000] train loss: 0.08296, val loss: 0.03549, time: 0.01\n",
      "Epoch[821/1000] train loss: 0.07974, val loss: 0.03761, time: 0.01\n",
      "Epoch[822/1000] train loss: 0.10251, val loss: 0.03911, time: 0.01\n",
      "Epoch[823/1000] train loss: 0.10952, val loss: 0.03979, time: 0.01\n",
      "Epoch[824/1000] train loss: 0.17313, val loss: 0.03991, time: 0.01\n",
      "Epoch[825/1000] train loss: 0.07099, val loss: 0.03947, time: 0.01\n",
      "Epoch[826/1000] train loss: 0.10798, val loss: 0.03831, time: 0.01\n",
      "Epoch[827/1000] train loss: 0.09513, val loss: 0.03736, time: 0.01\n",
      "Epoch[828/1000] train loss: 0.07023, val loss: 0.03643, time: 0.01\n",
      "Epoch[829/1000] train loss: 0.11782, val loss: 0.03509, time: 0.01\n",
      "Epoch[830/1000] train loss: 0.11607, val loss: 0.03394, time: 0.01\n",
      "Epoch[831/1000] train loss: 0.10384, val loss: 0.03285, time: 0.01\n",
      "Epoch[832/1000] train loss: 0.13651, val loss: 0.03211, time: 0.01\n",
      "Epoch[833/1000] train loss: 0.15276, val loss: 0.03502, time: 0.01\n",
      "Epoch[834/1000] train loss: 0.10325, val loss: 0.04026, time: 0.01\n",
      "Epoch[835/1000] train loss: 0.07059, val loss: 0.04288, time: 0.01\n",
      "Epoch[836/1000] train loss: 0.09484, val loss: 0.04410, time: 0.01\n",
      "Epoch[837/1000] train loss: 0.08995, val loss: 0.04601, time: 0.01\n",
      "Epoch[838/1000] train loss: 0.08975, val loss: 0.04923, time: 0.01\n",
      "Epoch[839/1000] train loss: 0.09855, val loss: 0.05008, time: 0.01\n",
      "Epoch[840/1000] train loss: 0.10747, val loss: 0.04877, time: 0.01\n",
      "Epoch[841/1000] train loss: 0.07592, val loss: 0.04625, time: 0.01\n",
      "Epoch[842/1000] train loss: 0.08602, val loss: 0.04265, time: 0.01\n",
      "Epoch[843/1000] train loss: 0.14990, val loss: 0.03808, time: 0.01\n",
      "Epoch[844/1000] train loss: 0.11713, val loss: 0.03410, time: 0.01\n",
      "Epoch[845/1000] train loss: 0.08250, val loss: 0.03150, time: 0.01\n",
      "Epoch[846/1000] train loss: 0.07827, val loss: 0.03018, time: 0.01\n",
      "Epoch[847/1000] train loss: 0.07115, val loss: 0.02955, time: 0.01\n",
      "Epoch[848/1000] train loss: 0.08268, val loss: 0.02910, time: 0.01\n",
      "Epoch[849/1000] train loss: 0.12086, val loss: 0.02914, time: 0.01\n",
      "Epoch[850/1000] train loss: 0.10297, val loss: 0.02955, time: 0.01\n",
      "Epoch[851/1000] train loss: 0.11700, val loss: 0.02999, time: 0.01\n",
      "Epoch[852/1000] train loss: 0.08442, val loss: 0.02980, time: 0.01\n",
      "Epoch[853/1000] train loss: 0.06532, val loss: 0.02924, time: 0.01\n",
      "Epoch[854/1000] train loss: 0.14128, val loss: 0.02847, time: 0.01\n",
      "Epoch[855/1000] train loss: 0.16236, val loss: 0.02740, time: 0.01\n",
      "Epoch[856/1000] train loss: 0.08811, val loss: 0.02672, time: 0.01\n",
      "Epoch[857/1000] train loss: 0.10268, val loss: 0.02698, time: 0.01\n",
      "Epoch[858/1000] train loss: 0.08175, val loss: 0.02874, time: 0.01\n",
      "Epoch[859/1000] train loss: 0.08786, val loss: 0.02992, time: 0.01\n",
      "Epoch[860/1000] train loss: 0.08037, val loss: 0.03075, time: 0.01\n",
      "Epoch[861/1000] train loss: 0.09860, val loss: 0.03105, time: 0.01\n",
      "Epoch[862/1000] train loss: 0.08344, val loss: 0.03127, time: 0.01\n",
      "Epoch[863/1000] train loss: 0.06949, val loss: 0.03128, time: 0.01\n",
      "Epoch[864/1000] train loss: 0.09182, val loss: 0.03136, time: 0.01\n",
      "Epoch[865/1000] train loss: 0.07880, val loss: 0.03090, time: 0.01\n",
      "Epoch[866/1000] train loss: 0.07126, val loss: 0.03093, time: 0.01\n",
      "Epoch[867/1000] train loss: 0.07255, val loss: 0.03168, time: 0.01\n",
      "Epoch[868/1000] train loss: 0.13721, val loss: 0.03179, time: 0.01\n",
      "Epoch[869/1000] train loss: 0.10500, val loss: 0.03190, time: 0.01\n",
      "Epoch[870/1000] train loss: 0.11628, val loss: 0.03289, time: 0.01\n",
      "Epoch[871/1000] train loss: 0.08182, val loss: 0.03569, time: 0.01\n",
      "Epoch[872/1000] train loss: 0.09098, val loss: 0.03783, time: 0.01\n",
      "Epoch[873/1000] train loss: 0.10488, val loss: 0.03973, time: 0.01\n",
      "Epoch[874/1000] train loss: 0.09885, val loss: 0.04222, time: 0.01\n",
      "Epoch[875/1000] train loss: 0.14672, val loss: 0.04238, time: 0.01\n",
      "Epoch[876/1000] train loss: 0.08145, val loss: 0.04174, time: 0.01\n",
      "Epoch[877/1000] train loss: 0.07641, val loss: 0.04064, time: 0.01\n",
      "Epoch[878/1000] train loss: 0.08237, val loss: 0.03902, time: 0.01\n",
      "Epoch[879/1000] train loss: 0.09555, val loss: 0.03681, time: 0.01\n",
      "Epoch[880/1000] train loss: 0.14933, val loss: 0.03516, time: 0.01\n",
      "Epoch[881/1000] train loss: 0.07541, val loss: 0.03321, time: 0.01\n",
      "Epoch[882/1000] train loss: 0.06854, val loss: 0.03252, time: 0.01\n",
      "Epoch[883/1000] train loss: 0.06831, val loss: 0.03228, time: 0.01\n",
      "Epoch[884/1000] train loss: 0.08999, val loss: 0.03146, time: 0.01\n",
      "Epoch[885/1000] train loss: 0.13725, val loss: 0.03049, time: 0.01\n",
      "Epoch[886/1000] train loss: 0.08711, val loss: 0.03015, time: 0.01\n",
      "Epoch[887/1000] train loss: 0.08094, val loss: 0.02991, time: 0.01\n",
      "Epoch[888/1000] train loss: 0.08013, val loss: 0.03027, time: 0.01\n",
      "Epoch[889/1000] train loss: 0.11561, val loss: 0.03079, time: 0.01\n",
      "Epoch[890/1000] train loss: 0.10578, val loss: 0.03167, time: 0.01\n",
      "Epoch[891/1000] train loss: 0.08581, val loss: 0.03358, time: 0.01\n",
      "Epoch[892/1000] train loss: 0.12432, val loss: 0.03427, time: 0.01\n",
      "Epoch[893/1000] train loss: 0.07846, val loss: 0.03456, time: 0.01\n",
      "Epoch[894/1000] train loss: 0.11843, val loss: 0.03419, time: 0.01\n",
      "Epoch[895/1000] train loss: 0.09534, val loss: 0.03321, time: 0.01\n",
      "Epoch[896/1000] train loss: 0.11935, val loss: 0.03375, time: 0.01\n",
      "Epoch[897/1000] train loss: 0.07837, val loss: 0.03656, time: 0.01\n",
      "Epoch[898/1000] train loss: 0.09307, val loss: 0.03847, time: 0.01\n",
      "Epoch[899/1000] train loss: 0.10878, val loss: 0.03964, time: 0.01\n",
      "Epoch[900/1000] train loss: 0.08841, val loss: 0.03933, time: 0.01\n",
      "Epoch[901/1000] train loss: 0.16650, val loss: 0.03711, time: 0.01\n",
      "Epoch[902/1000] train loss: 0.09652, val loss: 0.03544, time: 0.01\n",
      "Epoch[903/1000] train loss: 0.10920, val loss: 0.03505, time: 0.01\n",
      "Epoch[904/1000] train loss: 0.07791, val loss: 0.03358, time: 0.01\n",
      "Epoch[905/1000] train loss: 0.10971, val loss: 0.03176, time: 0.01\n",
      "Epoch[906/1000] train loss: 0.11117, val loss: 0.03078, time: 0.01\n",
      "Epoch[907/1000] train loss: 0.08341, val loss: 0.03178, time: 0.01\n",
      "Epoch[908/1000] train loss: 0.07754, val loss: 0.03254, time: 0.01\n",
      "Epoch[909/1000] train loss: 0.13137, val loss: 0.03285, time: 0.01\n",
      "Epoch[910/1000] train loss: 0.16776, val loss: 0.03274, time: 0.01\n",
      "Epoch[911/1000] train loss: 0.10097, val loss: 0.03262, time: 0.01\n",
      "Epoch[912/1000] train loss: 0.08878, val loss: 0.03236, time: 0.01\n",
      "Epoch[913/1000] train loss: 0.06342, val loss: 0.03215, time: 0.01\n",
      "Epoch[914/1000] train loss: 0.10126, val loss: 0.03194, time: 0.01\n",
      "Epoch[915/1000] train loss: 0.08908, val loss: 0.03095, time: 0.01\n",
      "Epoch[916/1000] train loss: 0.07535, val loss: 0.03029, time: 0.01\n",
      "Epoch[917/1000] train loss: 0.09691, val loss: 0.03010, time: 0.01\n",
      "Epoch[918/1000] train loss: 0.07794, val loss: 0.02985, time: 0.01\n",
      "Epoch[919/1000] train loss: 0.07550, val loss: 0.02968, time: 0.01\n",
      "Epoch[920/1000] train loss: 0.06625, val loss: 0.03061, time: 0.01\n",
      "Epoch[921/1000] train loss: 0.06782, val loss: 0.03190, time: 0.01\n",
      "Epoch[922/1000] train loss: 0.06210, val loss: 0.03277, time: 0.01\n",
      "Epoch[923/1000] train loss: 0.07113, val loss: 0.03343, time: 0.01\n",
      "Epoch[924/1000] train loss: 0.07828, val loss: 0.03377, time: 0.01\n",
      "Epoch[925/1000] train loss: 0.08098, val loss: 0.03349, time: 0.01\n",
      "Epoch[926/1000] train loss: 0.09622, val loss: 0.03298, time: 0.01\n",
      "Epoch[927/1000] train loss: 0.08114, val loss: 0.03267, time: 0.01\n",
      "Epoch[928/1000] train loss: 0.08956, val loss: 0.03405, time: 0.01\n",
      "Epoch[929/1000] train loss: 0.12528, val loss: 0.03746, time: 0.01\n",
      "Epoch[930/1000] train loss: 0.08467, val loss: 0.04038, time: 0.01\n",
      "Epoch[931/1000] train loss: 0.10244, val loss: 0.04106, time: 0.01\n",
      "Epoch[932/1000] train loss: 0.08831, val loss: 0.03976, time: 0.01\n",
      "Epoch[933/1000] train loss: 0.10335, val loss: 0.03859, time: 0.01\n",
      "Epoch[934/1000] train loss: 0.06771, val loss: 0.03760, time: 0.01\n",
      "Epoch[935/1000] train loss: 0.08475, val loss: 0.03635, time: 0.01\n",
      "Epoch[936/1000] train loss: 0.07080, val loss: 0.03504, time: 0.01\n",
      "Epoch[937/1000] train loss: 0.08745, val loss: 0.03363, time: 0.01\n",
      "Epoch[938/1000] train loss: 0.06833, val loss: 0.03196, time: 0.01\n",
      "Epoch[939/1000] train loss: 0.09856, val loss: 0.03028, time: 0.01\n",
      "Epoch[940/1000] train loss: 0.09823, val loss: 0.02873, time: 0.01\n",
      "Epoch[941/1000] train loss: 0.14029, val loss: 0.02831, time: 0.01\n",
      "Epoch[942/1000] train loss: 0.08198, val loss: 0.03019, time: 0.01\n",
      "Epoch[943/1000] train loss: 0.09741, val loss: 0.03223, time: 0.01\n",
      "Epoch[944/1000] train loss: 0.09194, val loss: 0.03390, time: 0.01\n",
      "Epoch[945/1000] train loss: 0.08857, val loss: 0.03524, time: 0.01\n",
      "Epoch[946/1000] train loss: 0.07873, val loss: 0.03622, time: 0.01\n",
      "Epoch[947/1000] train loss: 0.08161, val loss: 0.03668, time: 0.01\n",
      "Epoch[948/1000] train loss: 0.08591, val loss: 0.03721, time: 0.01\n",
      "Epoch[949/1000] train loss: 0.08777, val loss: 0.03798, time: 0.01\n",
      "Epoch[950/1000] train loss: 0.07680, val loss: 0.03833, time: 0.01\n",
      "Epoch[951/1000] train loss: 0.08121, val loss: 0.03920, time: 0.01\n",
      "Epoch[952/1000] train loss: 0.08953, val loss: 0.04024, time: 0.01\n",
      "Epoch[953/1000] train loss: 0.08573, val loss: 0.04057, time: 0.01\n",
      "Epoch[954/1000] train loss: 0.11373, val loss: 0.04011, time: 0.01\n",
      "Epoch[955/1000] train loss: 0.10854, val loss: 0.03857, time: 0.01\n",
      "Epoch[956/1000] train loss: 0.08640, val loss: 0.03675, time: 0.01\n",
      "Epoch[957/1000] train loss: 0.07568, val loss: 0.03468, time: 0.01\n",
      "Epoch[958/1000] train loss: 0.07683, val loss: 0.03300, time: 0.01\n",
      "Epoch[959/1000] train loss: 0.07883, val loss: 0.03214, time: 0.01\n",
      "Epoch[960/1000] train loss: 0.13379, val loss: 0.03148, time: 0.01\n",
      "Epoch[961/1000] train loss: 0.09567, val loss: 0.03074, time: 0.01\n",
      "Epoch[962/1000] train loss: 0.07297, val loss: 0.03034, time: 0.01\n",
      "Epoch[963/1000] train loss: 0.14427, val loss: 0.03041, time: 0.01\n",
      "Epoch[964/1000] train loss: 0.13244, val loss: 0.03040, time: 0.01\n",
      "Epoch[965/1000] train loss: 0.09566, val loss: 0.03099, time: 0.01\n",
      "Epoch[966/1000] train loss: 0.10859, val loss: 0.03178, time: 0.01\n",
      "Epoch[967/1000] train loss: 0.07951, val loss: 0.03158, time: 0.01\n",
      "Epoch[968/1000] train loss: 0.10222, val loss: 0.03127, time: 0.01\n",
      "Epoch[969/1000] train loss: 0.07542, val loss: 0.03132, time: 0.01\n",
      "Epoch[970/1000] train loss: 0.09934, val loss: 0.03229, time: 0.01\n",
      "Epoch[971/1000] train loss: 0.12774, val loss: 0.03325, time: 0.01\n",
      "Epoch[972/1000] train loss: 0.11232, val loss: 0.03474, time: 0.01\n",
      "Epoch[973/1000] train loss: 0.09209, val loss: 0.03554, time: 0.01\n",
      "Epoch[974/1000] train loss: 0.15452, val loss: 0.03516, time: 0.01\n",
      "Epoch[975/1000] train loss: 0.11219, val loss: 0.03519, time: 0.01\n",
      "Epoch[976/1000] train loss: 0.06935, val loss: 0.03673, time: 0.01\n",
      "Epoch[977/1000] train loss: 0.10703, val loss: 0.03825, time: 0.01\n",
      "Epoch[978/1000] train loss: 0.07267, val loss: 0.03944, time: 0.01\n",
      "Epoch[979/1000] train loss: 0.09462, val loss: 0.04094, time: 0.01\n",
      "Epoch[980/1000] train loss: 0.09407, val loss: 0.04324, time: 0.01\n",
      "Epoch[981/1000] train loss: 0.07492, val loss: 0.04417, time: 0.01\n",
      "Epoch[982/1000] train loss: 0.08515, val loss: 0.04410, time: 0.01\n",
      "Epoch[983/1000] train loss: 0.11612, val loss: 0.04391, time: 0.01\n",
      "Epoch[984/1000] train loss: 0.10901, val loss: 0.04243, time: 0.01\n",
      "Epoch[985/1000] train loss: 0.08373, val loss: 0.04049, time: 0.01\n",
      "Epoch[986/1000] train loss: 0.07791, val loss: 0.03944, time: 0.01\n",
      "Epoch[987/1000] train loss: 0.08651, val loss: 0.03809, time: 0.01\n",
      "Epoch[988/1000] train loss: 0.11884, val loss: 0.03713, time: 0.01\n",
      "Epoch[989/1000] train loss: 0.07960, val loss: 0.03605, time: 0.01\n",
      "Epoch[990/1000] train loss: 0.06422, val loss: 0.03503, time: 0.01\n",
      "Epoch[991/1000] train loss: 0.08020, val loss: 0.03434, time: 0.01\n",
      "Epoch[992/1000] train loss: 0.11121, val loss: 0.03388, time: 0.01\n",
      "Epoch[993/1000] train loss: 0.07580, val loss: 0.03385, time: 0.01\n",
      "Epoch[994/1000] train loss: 0.13083, val loss: 0.03566, time: 0.01\n",
      "Epoch[995/1000] train loss: 0.08249, val loss: 0.03994, time: 0.01\n",
      "Epoch[996/1000] train loss: 0.15939, val loss: 0.04117, time: 0.01\n",
      "Epoch[997/1000] train loss: 0.19752, val loss: 0.04017, time: 0.01\n",
      "Epoch[998/1000] train loss: 0.08536, val loss: 0.03850, time: 0.01\n",
      "Epoch[999/1000] train loss: 0.09391, val loss: 0.03602, time: 0.01\n",
      "Epoch[1000/1000] train loss: 0.07898, val loss: 0.03446, time: 0.01\n"
     ]
    }
   ],
   "source": [
    "history = dict(train=[], val=[])\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    ts = time.time()\n",
    "\n",
    "    # train\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = model(x)\n",
    "\n",
    "        loss = criterion(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # validate\n",
    "    model = model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = model(x)\n",
    "            loss = criterion(x, y)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    te = time.time()\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    print(f\"Epoch[{epoch}/{(EPOCH)}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}, time: {te - ts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e092367a-80ba-4360-becb-92a7977a128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train'])\n",
    "plt.plot(history['val'])\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Loss over training epochs')\n",
    "plt.legend(['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8520c690-6352-4b65-a58a-ab0ea5654388",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{common.root}/model/keypoints_occlusion.pth'\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9451843-fd1c-46e4-b693-4e8616995717",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bbb1912-26f5-49e5-90ab-4d0ea3c7d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{common.root}/model/keypoints_occlusion.pth'\n",
    "model = torch.load(path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "270cbf17-585d-4468-b76f-bdbaa3b5cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    for x, y in test_loader:\n",
    "        x = model(x)\n",
    "        loss = criterion(x, y)\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5b112dda-4f86-4d76-be56-eb9a34263006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048484938219189644\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.mean(losses))\n",
    "plt.hist(losses)\n",
    "plt.xlabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fc4ad2d4-57c7-4fa5-9c9b-8c943fedcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = [\n",
    "    # ========== 4 ============ 9 =========== 14 =====\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Nose\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # LEye\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # REye\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # LEar\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # REar\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # LShoulder\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],  # RShoulder\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # LElbow\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # RElbow\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # LWrist\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # RWrist\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  # LHip\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  # RHip\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],  # LKnee\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],  # RKnee\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # LAnkle\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # RAnkle\n",
    "]\n",
    "\n",
    "def plot_keypoints(img, keypoints, color=(255, 0, 0)):\n",
    "    if np.any(np.isnan(keypoints)):\n",
    "        return img\n",
    "    keypoints = np.array(keypoints)\n",
    "    for i in range(len(keypoints) - 1):\n",
    "        for j in range(i + 1, len(keypoints)):\n",
    "            p1 = tuple(keypoints[i].astype(int))\n",
    "            p2 = tuple(keypoints[j].astype(int))\n",
    "            if graph[i][j] == 1:\n",
    "                img = cv2.line(img, p1, p2, color, 3)\n",
    "    return img\n",
    "\n",
    "def plot_prediction(img, y, color=(0, 255, 0)):\n",
    "    for pt in y:\n",
    "        pt = tuple(pt.astype(int))\n",
    "        img = cv2.circle(img, pt, 10, color, -1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c2fd7569-7fad-4ba4-9bd9-6c62f14aecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10195533 -1.17263717]\n",
      " [ 1.11745992 -1.18038929]\n",
      " [ 1.10195533 -1.18814149]\n",
      " [ 1.05544227 -1.22690243]\n",
      " [ 1.07094686 -1.21915023]\n",
      " [ 1.05544227 -1.13387623]\n",
      " [ 0.95466404 -1.15713276]\n",
      " [ 1.03993804 -1.03309783]\n",
      " [ 0.90815099 -1.0175935 ]\n",
      " [ 1.1097078  -0.97883257]\n",
      " [ 1.01668133 -0.97108036]\n",
      " [ 0.9779204  -0.89355857]\n",
      " [ 0.90815099 -0.90131069]\n",
      " [ 0.92365522 -0.7307627 ]\n",
      " [ 0.83838123 -0.76177143]\n",
      " [ 0.86163793 -0.58347124]\n",
      " [ 0.72985071 -0.62223218]]\n",
      "[[ 0.87545925 -1.0487424 ]\n",
      " [ 0.89443946 -1.0185748 ]\n",
      " [ 0.83731264 -0.70751107]\n",
      " [ 0.8168233  -0.7886143 ]]\n",
      "[[1146.1258544921875, 363.5169372558594], [1151.46044921875, 360.8497009277344], [1146.1258544921875, 358.18243408203125], [1130.122314453125, 344.84613037109375], [1135.4569091796875, 347.5133972167969], [1130.122314453125, 376.8532409667969], [1095.447998046875, 368.8514709472656], [1124.787841796875, 411.5276184082031], [1079.4444580078125, 416.86212158203125], [1148.793212890625, 430.19842529296875], [1116.7860107421875, 432.8656921386719], [1103.44970703125, 459.53826904296875], [1079.4444580078125, 456.87103271484375], [1084.7789306640625, 515.5507202148438], [1055.4390869140625, 504.8816833496094], [1063.44091796875, 566.2286376953125], [1018.0974731445312, 552.892333984375]]\n",
      "[[1068.1964   406.14487]\n",
      " [1074.7268   416.5245 ]\n",
      " [1055.0714   523.55084]\n",
      " [1048.0217   495.64603]]\n",
      "[[128.02838135  18.67080688]\n",
      " [133.36297607  16.00357056]\n",
      " [128.02838135  13.33630371]\n",
      " [112.02484131   0.        ]\n",
      " [117.35943604   2.66726685]\n",
      " [112.02484131  32.0071106 ]\n",
      " [ 77.3505249   24.00534058]\n",
      " [106.69036865  66.68148804]\n",
      " [ 61.34698486  72.01599121]\n",
      " [130.69573975  85.35229492]\n",
      " [ 98.6885376   88.01956177]\n",
      " [ 85.35223389 114.69213867]\n",
      " [ 61.34698486 112.02490234]\n",
      " [ 66.68145752 170.70458984]\n",
      " [ 37.34161377 160.03555298]\n",
      " [ 45.34344482 221.38250732]\n",
      " [  0.         208.04620361]]\n",
      "[[ 50.09893799  61.29873657]\n",
      " [ 56.6293335   71.67837524]\n",
      " [ 36.97393799 178.70471191]\n",
      " [ 29.92425537 150.79989624]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_len = 15\n",
    "row_len = len(test_idxs) // col_len + 1\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    fig = plt.figure(figsize=(col_len * 2, row_len * 3))\n",
    "    for i, idx in enumerate(test_idxs):\n",
    "        kps = keypoints_lst[idx]\n",
    "\n",
    "        mean = np.mean(kps)\n",
    "        std = np.std(kps)\n",
    "        \n",
    "        kps_transformed = ((kps - mean) / std)\n",
    "        x = [\n",
    "            kps_transformed[body['LShoulder']],\n",
    "            kps_transformed[body['RShoulder']],\n",
    "            kps_transformed[body['LHip']],\n",
    "            kps_transformed[body['RHip']],\n",
    "        ]\n",
    "        x = np.array(x).flatten()\n",
    "        x = tensor(x).float().to(device)\n",
    "        y = model(x)\n",
    "        y = y.cpu().numpy().reshape(-1, 2)\n",
    "        y_transformed = y * std + mean\n",
    "        \n",
    "        \n",
    "        mins = np.min(kps, axis=0)\n",
    "        size = (np.max(kps, axis=0) - mins).astype(int)\n",
    "        img = np.ones((size[1], size[0], 3), np.uint8) * 255\n",
    "        print(kps_transformed)\n",
    "        print(y)\n",
    "        print(kps)\n",
    "        print(y_transformed)\n",
    "        print(kps - mins)\n",
    "        print(y_transformed - mins)\n",
    "        \n",
    "        img = plot_keypoints(img, kps - mins)\n",
    "        img = plot_prediction(img, y_transformed - mins)\n",
    "        \n",
    "        ax = fig.add_subplot(col_len, row_len, i + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf44b04-f0db-4d2c-b086-a1b468d91c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
