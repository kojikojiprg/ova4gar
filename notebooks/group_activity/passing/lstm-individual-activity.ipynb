{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/k2111/research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k2111/research/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# back to project root\n",
    "%cd ~/research\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch import nn, optim\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from group.passing.dataset import make_data_loaders, make_all_data\n",
    "from group.passing.lstm_model import LSTMModel\n",
    "from utility.activity_loader import load_individuals\n",
    "from utility.logger import logger\n",
    "from tools.train_passing import init_model, init_loss, init_optim, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc298add-7d1e-4c1a-8a98-767bde6e9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams['xtick.direction'] = 'in'  # x axis in\n",
    "plt.rcParams['ytick.direction'] = 'in'  # y axis in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28083f0-6a29-4348-b2f0-6099f5289d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f8937b-15e0-489c-adfe-fc605429bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"config/passing/pass_train.yaml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    train_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"individual\"], \"r\") as f:\n",
    "    ind_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"group\"], \"r\") as f:\n",
    "    grp_cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe7d65b-260f-4091-b271-5cf80ff3c575",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:31:41,201 => loading individuals from {'02_001': ['data/02/001/passing/01', 'data/02/001/passing/02', 'data/02/001/passing/03', 'data/02/001/passing/04', 'data/02/001/passing/05', 'data/02/001/passing/06', 'data/02/001/passing/07', 'data/02/001/passing/08', 'data/02/001/passing/09', 'data/02/001/passing/10', 'data/02/001/passing/11', 'data/02/001/passing/12', 'data/02/001/passing/13', 'data/02/001/passing/14', 'data/02/001/passing/15', 'data/02/001/passing/16', 'data/02/001/passing/17', 'data/02/001/passing/18', 'data/02/001/passing/19', 'data/02/001/passing/20', 'data/02/001/passing/21', 'data/02/001/passing/22', 'data/02/001/passing/23'], '08_001': ['data/08/001/passing/01', 'data/08/001/passing/02', 'data/08/001/passing/03', 'data/08/001/passing/04', 'data/08/001/passing/05', 'data/08/001/passing/06', 'data/08/001/passing/07', 'data/08/001/passing/08', 'data/08/001/passing/09', 'data/08/001/passing/10', 'data/08/001/passing/11', 'data/08/001/passing/12', 'data/08/001/passing/13', 'data/08/001/passing/14', 'data/08/001/passing/15', 'data/08/001/passing/16', 'data/08/001/passing/17', 'data/08/001/passing/18', 'data/08/001/passing/19', 'data/08/001/passing/20', 'data/08/001/passing/21', 'data/08/001/passing/22', 'data/08/001/passing/23', 'data/08/001/passing/24', 'data/08/001/passing/25', 'data/08/001/passing/26', 'data/08/001/passing/27', 'data/08/001/passing/28', 'data/08/001/passing/29', 'data/08/001/passing/30', 'data/08/001/passing/31', 'data/08/001/passing/32', 'data/08/001/passing/33', 'data/08/001/passing/34', 'data/08/001/passing/35', 'data/08/001/passing/36', 'data/08/001/passing/37', 'data/08/001/passing/38', 'data/08/001/passing/39', 'data/08/001/passing/40', 'data/08/001/passing/41'], '09_001': ['data/09/001/passing/01', 'data/09/001/passing/02', 'data/09/001/passing/03', 'data/09/001/passing/04', 'data/09/001/passing/05', 'data/09/001/passing/06', 'data/09/001/passing/07', 'data/09/001/passing/08', 'data/09/001/passing/09']}\n"
     ]
    }
   ],
   "source": [
    "data_dirs_all = {}\n",
    "for room_num, surgery_items in train_cfg[\"dataset\"][\"setting\"].items():\n",
    "    for surgery_num in surgery_items.keys():\n",
    "        dirs = sorted(glob(os.path.join(\"data\", room_num, surgery_num, \"passing\", \"*\")))\n",
    "        data_dirs_all[f\"{room_num}_{surgery_num}\"] = dirs\n",
    "\n",
    "logger.info(f\"=> loading individuals from {data_dirs_all}\")\n",
    "inds = {}\n",
    "for key_prefix, dirs in data_dirs_all.items():\n",
    "    for model_path in dirs:\n",
    "        num = model_path.split(\"/\")[-1]\n",
    "        json_path = os.path.join(model_path, \".json\", \"individual.json\")\n",
    "        tmp_inds, _ = load_individuals(json_path, ind_cfg)\n",
    "        for pid, ind in tmp_inds.items():\n",
    "            inds[f\"{key_prefix}_{num}_{pid}\"] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b443de49-2306-476a-b2bb-295506a68dc5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:31:48,196 => createing time series 02_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  7.98it/s]\n",
      "2022-06-08 17:31:51,080 => createing time series 08_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 14.80it/s]\n",
      "2022-06-08 17:31:53,852 => createing time series 09_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  7.76it/s]\n",
      "2022-06-08 17:31:55,014 => extracting feature 02_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:10<00:00,  2.22it/s]\n",
      "2022-06-08 17:32:05,380 => extracting feature 08_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:09<00:00,  4.15it/s]\n",
      "2022-06-08 17:32:15,266 => extracting feature 09_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.45it/s]\n",
      "2022-06-08 17:32:21,486 => create train loader\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4044/4044 [00:00<00:00, 8213.88it/s]\n",
      "2022-06-08 17:32:21,984 => skip creating val loader\n",
      "2022-06-08 17:32:21,984 => create test loader\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1011/1011 [00:00<00:00, 32912.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data loader\n",
    "dataset_cfg = train_cfg[\"dataset\"]\n",
    "passing_defs = grp_cfg[\"passing\"][\"default\"]\n",
    "train_loader, val_loader, test_loader = make_data_loaders(\n",
    "    inds, dataset_cfg, passing_defs, logger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7b174-dd28-4aa8-8d26-9bc4427a0163",
   "metadata": {},
   "source": [
    "# グリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60011bb5-8a2a-497d-8f5e-a1fe6b56b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "mdl_cfg = {\n",
    "    \"dropouts\": [0.1, 0],\n",
    "    \"hidden_dims\": [128, 8],\n",
    "    \"n_classes\": 2,\n",
    "    \"n_linears\": 2,\n",
    "    \"rnn_dropout\": 0.1,\n",
    "    \"size\": 4,\n",
    "}\n",
    "\n",
    "# grid search parameters\n",
    "params = {\n",
    "    'n_rnns': [1, 2],\n",
    "    'rnn_hidden_dim': [128, 256, 512],\n",
    "    'pos_weight': [4, 8, 16]\n",
    "}\n",
    "\n",
    "# epoch\n",
    "epoch_len = train_cfg[\"optim\"][\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d9a76f-fd7b-4bde-8e77-33dba46fd26a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:32:25,447 => start training\n",
      "2022-06-08 17:32:28,249 Epoch[1/120] train loss: 0.77627, val loss: nan, lr: 0.0010000, time: 2.80\n",
      "2022-06-08 17:32:30,966 Epoch[2/120] train loss: 0.63205, val loss: nan, lr: 0.0010000, time: 2.72\n",
      "2022-06-08 17:32:33,568 Epoch[3/120] train loss: 0.53177, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:32:36,191 Epoch[4/120] train loss: 0.47213, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:32:38,748 Epoch[5/120] train loss: 0.44233, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:32:41,316 Epoch[6/120] train loss: 0.43068, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:32:43,953 Epoch[7/120] train loss: 0.41511, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:32:46,526 Epoch[8/120] train loss: 0.40525, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:32:49,143 Epoch[9/120] train loss: 0.39902, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:32:51,744 Epoch[10/120] train loss: 0.39428, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:32:54,263 Epoch[11/120] train loss: 0.39081, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:32:56,843 Epoch[12/120] train loss: 0.38706, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:32:59,481 Epoch[13/120] train loss: 0.38432, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:33:02,071 Epoch[14/120] train loss: 0.38308, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:33:04,602 Epoch[15/120] train loss: 0.38136, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:33:07,215 Epoch[16/120] train loss: 0.38041, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:33:09,777 Epoch[17/120] train loss: 0.37901, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:33:12,308 Epoch[18/120] train loss: 0.38039, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:33:14,889 Epoch[19/120] train loss: 0.37736, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:33:17,497 Epoch[20/120] train loss: 0.37655, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:33:20,012 Epoch[21/120] train loss: 0.37612, val loss: nan, lr: 0.0010000, time: 2.51\n",
      "2022-06-08 17:33:22,669 Epoch[22/120] train loss: 0.37558, val loss: nan, lr: 0.0010000, time: 2.66\n",
      "2022-06-08 17:33:25,235 Epoch[23/120] train loss: 0.37469, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:33:27,775 Epoch[24/120] train loss: 0.37521, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:33:30,323 Epoch[25/120] train loss: 0.37447, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:33:32,913 Epoch[26/120] train loss: 0.37461, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:33:35,508 Epoch[27/120] train loss: 0.37293, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:33:38,035 Epoch[28/120] train loss: 0.37282, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:33:40,603 Epoch[29/120] train loss: 0.37265, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:33:43,185 Epoch[30/120] train loss: 0.37257, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:33:45,793 Epoch[31/120] train loss: 0.37162, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:33:48,366 Epoch[32/120] train loss: 0.37076, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:33:50,936 Epoch[33/120] train loss: 0.37121, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:33:53,635 Epoch[34/120] train loss: 0.37502, val loss: nan, lr: 0.0010000, time: 2.70\n",
      "2022-06-08 17:33:56,174 Epoch[35/120] train loss: 0.37274, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:33:58,739 Epoch[36/120] train loss: 0.37050, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:34:01,263 Epoch[37/120] train loss: 0.36997, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:34:03,863 Epoch[38/120] train loss: 0.36959, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:34:06,476 Epoch[39/120] train loss: 0.36906, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:34:09,012 Epoch[40/120] train loss: 0.36966, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:34:11,566 Epoch[41/120] train loss: 0.36839, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:34:14,147 Epoch[42/120] train loss: 0.36849, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:34:16,782 Epoch[43/120] train loss: 0.36767, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:34:19,449 Epoch[44/120] train loss: 0.36757, val loss: nan, lr: 0.0010000, time: 2.67\n",
      "2022-06-08 17:34:22,055 Epoch[45/120] train loss: 0.36843, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:34:24,634 Epoch[46/120] train loss: 0.36904, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:34:27,259 Epoch[47/120] train loss: 0.36743, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:34:29,893 Epoch[48/120] train loss: 0.36792, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:34:32,499 Epoch[49/120] train loss: 0.36740, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:34:35,085 Epoch[50/120] train loss: 0.36830, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:34:37,642 Epoch[51/120] train loss: 0.36697, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:34:40,222 Epoch[52/120] train loss: 0.36891, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:34:42,781 Epoch[53/120] train loss: 0.36667, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:34:45,362 Epoch[54/120] train loss: 0.36634, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:34:47,985 Epoch[55/120] train loss: 0.36679, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:34:50,593 Epoch[56/120] train loss: 0.36578, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:34:53,154 Epoch[57/120] train loss: 0.36601, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:34:55,759 Epoch[58/120] train loss: 0.36588, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:34:58,358 Epoch[59/120] train loss: 0.36614, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:35:00,921 Epoch[60/120] train loss: 0.36598, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:35:03,528 Epoch[61/120] train loss: 0.36799, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:35:06,122 Epoch[62/120] train loss: 0.37087, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:35:08,703 Epoch[63/120] train loss: 0.36666, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:11,333 Epoch[64/120] train loss: 0.36579, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:35:13,948 Epoch[65/120] train loss: 0.36495, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:35:16,505 Epoch[66/120] train loss: 0.36623, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:35:19,084 Epoch[67/120] train loss: 0.36466, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:21,702 Epoch[68/120] train loss: 0.36388, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:35:24,281 Epoch[69/120] train loss: 0.36527, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:26,861 Epoch[70/120] train loss: 0.36596, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:29,444 Epoch[71/120] train loss: 0.36313, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:32,031 Epoch[72/120] train loss: 0.36306, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:35:34,571 Epoch[73/120] train loss: 0.36309, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:35:37,148 Epoch[74/120] train loss: 0.36564, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:39,755 Epoch[75/120] train loss: 0.36286, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:35:42,334 Epoch[76/120] train loss: 0.36254, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:44,925 Epoch[77/120] train loss: 0.36267, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:35:47,508 Epoch[78/120] train loss: 0.36177, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:35:50,117 Epoch[79/120] train loss: 0.36410, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:35:52,750 Epoch[80/120] train loss: 0.36241, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:35:55,337 Epoch[81/120] train loss: 0.36182, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:35:57,910 Epoch[82/120] train loss: 0.36183, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:36:00,511 Epoch[83/120] train loss: 0.36164, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:03,114 Epoch[84/120] train loss: 0.36140, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:05,726 Epoch[85/120] train loss: 0.36021, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:36:08,341 Epoch[86/120] train loss: 0.36128, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:36:10,911 Epoch[87/120] train loss: 0.36062, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:36:13,540 Epoch[88/120] train loss: 0.36139, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:36:16,146 Epoch[89/120] train loss: 0.36107, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:36:18,746 Epoch[90/120] train loss: 0.35968, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:21,348 Epoch[91/120] train loss: 0.35971, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:23,968 Epoch[92/120] train loss: 0.36079, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:36:26,616 Epoch[93/120] train loss: 0.36051, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:36:29,242 Epoch[94/120] train loss: 0.35973, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:36:31,807 Epoch[95/120] train loss: 0.36113, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:36:35,016 Epoch[96/120] train loss: 0.35945, val loss: nan, lr: 0.0010000, time: 3.21\n",
      "2022-06-08 17:36:37,869 Epoch[97/120] train loss: 0.36074, val loss: nan, lr: 0.0010000, time: 2.85\n",
      "2022-06-08 17:36:40,466 Epoch[98/120] train loss: 0.35914, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:43,105 Epoch[99/120] train loss: 0.35894, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:36:45,710 Epoch[100/120] train loss: 0.35887, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:48,273 Epoch[101/120] train loss: 0.36011, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:36:50,869 Epoch[102/120] train loss: 0.35954, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:36:53,496 Epoch[103/120] train loss: 0.35931, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:36:56,073 Epoch[104/120] train loss: 0.35899, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:36:58,676 Epoch[105/120] train loss: 0.35897, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:37:01,275 Epoch[106/120] train loss: 0.35796, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:37:03,826 Epoch[107/120] train loss: 0.35832, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:37:06,477 Epoch[108/120] train loss: 0.36054, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:37:09,141 Epoch[109/120] train loss: 0.35785, val loss: nan, lr: 0.0010000, time: 2.66\n",
      "2022-06-08 17:37:11,738 Epoch[110/120] train loss: 0.35751, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:37:14,366 Epoch[111/120] train loss: 0.35751, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:37:17,025 Epoch[112/120] train loss: 0.35727, val loss: nan, lr: 0.0010000, time: 2.66\n",
      "2022-06-08 17:37:19,736 Epoch[113/120] train loss: 0.35736, val loss: nan, lr: 0.0010000, time: 2.71\n",
      "2022-06-08 17:37:22,353 Epoch[114/120] train loss: 0.35781, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:37:24,977 Epoch[115/120] train loss: 0.35712, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:37:27,571 Epoch[116/120] train loss: 0.35832, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:37:30,173 Epoch[117/120] train loss: 0.35757, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:37:32,814 Epoch[118/120] train loss: 0.35658, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:37:35,464 Epoch[119/120] train loss: 0.35746, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:37:38,110 Epoch[120/120] train loss: 0.35704, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:37:38,111 => end training\n",
      "2022-06-08 17:37:38,111 => calculating train scores\n",
      "2022-06-08 17:37:41,015 => train score\n",
      "accuracy: 0.9986114532709234\n",
      "presision: 0.8254593175853019\n",
      "recall: 0.9662058371735791\n",
      "f1: 0.8903043170559094\n",
      "2022-06-08 17:37:41,016 => calculating test scores\n",
      "2022-06-08 17:38:49,231 => test score\n",
      "accuracy: 0.9961301006173839\n",
      "presision: 0.2787878787878788\n",
      "recall: 0.323943661971831\n",
      "f1: 0.2996742671009772\n",
      "2022-06-08 17:38:49,247 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:38:51,884 Epoch[1/120] train loss: 0.77417, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:38:54,495 Epoch[2/120] train loss: 0.65317, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:38:57,061 Epoch[3/120] train loss: 0.56931, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:38:59,648 Epoch[4/120] train loss: 0.50770, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:39:02,221 Epoch[5/120] train loss: 0.46485, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:39:04,790 Epoch[6/120] train loss: 0.43945, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:39:07,419 Epoch[7/120] train loss: 0.42432, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:39:10,021 Epoch[8/120] train loss: 0.41406, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:39:12,568 Epoch[9/120] train loss: 0.40648, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:39:15,162 Epoch[10/120] train loss: 0.40187, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:39:17,740 Epoch[11/120] train loss: 0.39812, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:39:20,278 Epoch[12/120] train loss: 0.39574, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:39:22,911 Epoch[13/120] train loss: 0.39272, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:39:25,485 Epoch[14/120] train loss: 0.39046, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:39:28,042 Epoch[15/120] train loss: 0.38924, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:39:30,605 Epoch[16/120] train loss: 0.38813, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:39:33,188 Epoch[17/120] train loss: 0.38829, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:39:35,800 Epoch[18/120] train loss: 0.38641, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:39:38,392 Epoch[19/120] train loss: 0.38447, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:39:40,978 Epoch[20/120] train loss: 0.38474, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:39:43,552 Epoch[21/120] train loss: 0.38281, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:39:46,161 Epoch[22/120] train loss: 0.38325, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:39:48,670 Epoch[23/120] train loss: 0.38242, val loss: nan, lr: 0.0010000, time: 2.51\n",
      "2022-06-08 17:39:51,296 Epoch[24/120] train loss: 0.38129, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:39:53,921 Epoch[25/120] train loss: 0.38458, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:39:56,541 Epoch[26/120] train loss: 0.38070, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:39:59,137 Epoch[27/120] train loss: 0.37977, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:40:01,750 Epoch[28/120] train loss: 0.37971, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:40:04,365 Epoch[29/120] train loss: 0.37782, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:40:06,970 Epoch[30/120] train loss: 0.37723, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:40:09,595 Epoch[31/120] train loss: 0.37695, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:40:12,213 Epoch[32/120] train loss: 0.37742, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:40:14,776 Epoch[33/120] train loss: 0.37642, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:40:17,408 Epoch[34/120] train loss: 0.37684, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:40:20,053 Epoch[35/120] train loss: 0.37639, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:40:22,687 Epoch[36/120] train loss: 0.37402, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:40:25,261 Epoch[37/120] train loss: 0.37431, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:40:27,819 Epoch[38/120] train loss: 0.37446, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:40:30,421 Epoch[39/120] train loss: 0.37440, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:40:33,011 Epoch[40/120] train loss: 0.37444, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:40:35,572 Epoch[41/120] train loss: 0.37488, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:40:38,223 Epoch[42/120] train loss: 0.37333, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:40:40,878 Epoch[43/120] train loss: 0.37358, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:40:43,515 Epoch[44/120] train loss: 0.37314, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:40:46,151 Epoch[45/120] train loss: 0.37234, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:40:48,755 Epoch[46/120] train loss: 0.37168, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:40:51,332 Epoch[47/120] train loss: 0.37323, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:40:53,964 Epoch[48/120] train loss: 0.37123, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:40:56,585 Epoch[49/120] train loss: 0.37085, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:40:59,154 Epoch[50/120] train loss: 0.37127, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:41:01,712 Epoch[51/120] train loss: 0.37127, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:41:04,232 Epoch[52/120] train loss: 0.37202, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:41:06,778 Epoch[53/120] train loss: 0.37166, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:41:09,373 Epoch[54/120] train loss: 0.36962, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:41:11,995 Epoch[55/120] train loss: 0.36997, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:41:14,584 Epoch[56/120] train loss: 0.36928, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:41:17,145 Epoch[57/120] train loss: 0.37001, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:41:19,665 Epoch[58/120] train loss: 0.36993, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:41:22,249 Epoch[59/120] train loss: 0.37103, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:41:24,812 Epoch[60/120] train loss: 0.37024, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:41:27,468 Epoch[61/120] train loss: 0.36897, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:41:30,110 Epoch[62/120] train loss: 0.36743, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:41:32,698 Epoch[63/120] train loss: 0.37262, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:41:35,292 Epoch[64/120] train loss: 0.36797, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:41:37,856 Epoch[65/120] train loss: 0.36768, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:41:40,433 Epoch[66/120] train loss: 0.36930, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:41:43,052 Epoch[67/120] train loss: 0.36753, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:41:45,613 Epoch[68/120] train loss: 0.36846, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:41:48,199 Epoch[69/120] train loss: 0.36795, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:41:50,755 Epoch[70/120] train loss: 0.36835, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:41:53,282 Epoch[71/120] train loss: 0.36655, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:41:55,881 Epoch[72/120] train loss: 0.36671, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:41:58,483 Epoch[73/120] train loss: 0.36767, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:42:01,057 Epoch[74/120] train loss: 0.36955, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:42:03,673 Epoch[75/120] train loss: 0.36909, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:42:06,267 Epoch[76/120] train loss: 0.36790, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:42:08,843 Epoch[77/120] train loss: 0.36801, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:42:11,457 Epoch[78/120] train loss: 0.36600, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:42:14,050 Epoch[79/120] train loss: 0.36699, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:42:16,662 Epoch[80/120] train loss: 0.36822, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:42:19,254 Epoch[81/120] train loss: 0.36814, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:42:21,812 Epoch[82/120] train loss: 0.36845, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:42:24,464 Epoch[83/120] train loss: 0.36710, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:42:27,086 Epoch[84/120] train loss: 0.36775, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:42:29,712 Epoch[85/120] train loss: 0.36728, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:42:32,301 Epoch[86/120] train loss: 0.36650, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:42:34,938 Epoch[87/120] train loss: 0.36647, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:42:37,560 Epoch[88/120] train loss: 0.36716, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:42:40,166 Epoch[89/120] train loss: 0.36642, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:42:42,804 Epoch[90/120] train loss: 0.36614, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:42:45,453 Epoch[91/120] train loss: 0.37484, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:42:48,066 Epoch[92/120] train loss: 0.36715, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:42:50,641 Epoch[93/120] train loss: 0.36723, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:42:53,193 Epoch[94/120] train loss: 0.36549, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:42:55,804 Epoch[95/120] train loss: 0.36571, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:42:58,375 Epoch[96/120] train loss: 0.36587, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:43:00,997 Epoch[97/120] train loss: 0.36608, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:43:03,600 Epoch[98/120] train loss: 0.36727, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:43:06,179 Epoch[99/120] train loss: 0.36651, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:43:08,772 Epoch[100/120] train loss: 0.36702, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:43:11,355 Epoch[101/120] train loss: 0.36565, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:43:13,946 Epoch[102/120] train loss: 0.36621, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:43:16,510 Epoch[103/120] train loss: 0.36591, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:43:19,080 Epoch[104/120] train loss: 0.36598, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:43:21,681 Epoch[105/120] train loss: 0.36601, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:43:24,282 Epoch[106/120] train loss: 0.36516, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:43:26,816 Epoch[107/120] train loss: 0.36480, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:43:29,401 Epoch[108/120] train loss: 0.36523, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:43:31,957 Epoch[109/120] train loss: 0.36961, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:43:34,537 Epoch[110/120] train loss: 0.36803, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:43:37,091 Epoch[111/120] train loss: 0.36577, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:43:39,641 Epoch[112/120] train loss: 0.36415, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:43:42,239 Epoch[113/120] train loss: 0.36463, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:43:44,861 Epoch[114/120] train loss: 0.36526, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:43:47,444 Epoch[115/120] train loss: 0.36431, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:43:49,998 Epoch[116/120] train loss: 0.36617, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:43:52,608 Epoch[117/120] train loss: 0.36997, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:43:55,196 Epoch[118/120] train loss: 0.36548, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:43:57,780 Epoch[119/120] train loss: 0.36398, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:44:00,375 Epoch[120/120] train loss: 0.36373, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:44:00,375 => end training\n",
      "2022-06-08 17:44:00,376 => calculating train scores\n",
      "2022-06-08 17:44:03,278 => train score\n",
      "accuracy: 0.9985845781729412\n",
      "presision: 0.8069738480697385\n",
      "recall: 0.9953917050691244\n",
      "f1: 0.891334250343879\n",
      "2022-06-08 17:44:03,279 => calculating test scores\n",
      "2022-06-08 17:45:11,838 => test score\n",
      "accuracy: 0.9956261137210433\n",
      "presision: 0.2794759825327511\n",
      "recall: 0.4507042253521127\n",
      "f1: 0.34501347708894875\n",
      "2022-06-08 17:45:11,857 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:45:14,497 Epoch[1/120] train loss: 0.69524, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:45:17,094 Epoch[2/120] train loss: 0.59670, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:45:19,659 Epoch[3/120] train loss: 0.52795, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:45:22,233 Epoch[4/120] train loss: 0.48204, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:45:24,874 Epoch[5/120] train loss: 0.45692, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:45:27,456 Epoch[6/120] train loss: 0.44356, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:45:29,996 Epoch[7/120] train loss: 0.43417, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:45:32,573 Epoch[8/120] train loss: 0.42696, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:45:35,175 Epoch[9/120] train loss: 0.42251, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:45:37,782 Epoch[10/120] train loss: 0.41932, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:45:40,367 Epoch[11/120] train loss: 0.41485, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:45:42,959 Epoch[12/120] train loss: 0.41314, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:45:45,527 Epoch[13/120] train loss: 0.41129, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:45:48,095 Epoch[14/120] train loss: 0.41064, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:45:50,655 Epoch[15/120] train loss: 0.40799, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:45:53,223 Epoch[16/120] train loss: 0.40793, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:45:55,819 Epoch[17/120] train loss: 0.40630, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:45:58,387 Epoch[18/120] train loss: 0.40703, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:46:01,037 Epoch[19/120] train loss: 0.40469, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:46:03,667 Epoch[20/120] train loss: 0.40177, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:46:06,218 Epoch[21/120] train loss: 0.40429, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:46:08,801 Epoch[22/120] train loss: 0.40131, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:46:11,400 Epoch[23/120] train loss: 0.40124, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:46:14,020 Epoch[24/120] train loss: 0.39965, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:46:16,599 Epoch[25/120] train loss: 0.40271, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:46:19,145 Epoch[26/120] train loss: 0.39924, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:46:21,733 Epoch[27/120] train loss: 0.39798, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:46:24,330 Epoch[28/120] train loss: 0.39772, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:46:26,881 Epoch[29/120] train loss: 0.39814, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:46:29,401 Epoch[30/120] train loss: 0.39386, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:46:31,972 Epoch[31/120] train loss: 0.39680, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:46:34,510 Epoch[32/120] train loss: 0.39475, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:46:37,051 Epoch[33/120] train loss: 0.39490, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:46:39,630 Epoch[34/120] train loss: 0.39798, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:46:42,199 Epoch[35/120] train loss: 0.39445, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:46:44,784 Epoch[36/120] train loss: 0.39590, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:46:47,392 Epoch[37/120] train loss: 0.39368, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:46:49,912 Epoch[38/120] train loss: 0.39429, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:46:52,509 Epoch[39/120] train loss: 0.39270, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:46:55,067 Epoch[40/120] train loss: 0.39193, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:46:57,662 Epoch[41/120] train loss: 0.39192, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:47:00,288 Epoch[42/120] train loss: 0.39220, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:47:02,843 Epoch[43/120] train loss: 0.39061, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:47:05,413 Epoch[44/120] train loss: 0.39136, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:47:07,981 Epoch[45/120] train loss: 0.38919, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:47:10,561 Epoch[46/120] train loss: 0.39050, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:47:13,170 Epoch[47/120] train loss: 0.39229, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:47:15,816 Epoch[48/120] train loss: 0.38967, val loss: nan, lr: 0.0010000, time: 2.65\n",
      "2022-06-08 17:47:18,413 Epoch[49/120] train loss: 0.39406, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:47:21,044 Epoch[50/120] train loss: 0.38956, val loss: nan, lr: 0.0010000, time: 2.63\n",
      "2022-06-08 17:47:23,639 Epoch[51/120] train loss: 0.38898, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:47:26,211 Epoch[52/120] train loss: 0.38925, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:47:28,827 Epoch[53/120] train loss: 0.39129, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:47:31,431 Epoch[54/120] train loss: 0.38968, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:47:34,055 Epoch[55/120] train loss: 0.38941, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 17:47:36,628 Epoch[56/120] train loss: 0.38843, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:47:39,185 Epoch[57/120] train loss: 0.38999, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:47:41,735 Epoch[58/120] train loss: 0.39102, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:47:44,349 Epoch[59/120] train loss: 0.38720, val loss: nan, lr: 0.0010000, time: 2.61\n",
      "2022-06-08 17:47:46,870 Epoch[60/120] train loss: 0.38742, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:47:49,417 Epoch[61/120] train loss: 0.38768, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:47:52,062 Epoch[62/120] train loss: 0.38563, val loss: nan, lr: 0.0010000, time: 2.64\n",
      "2022-06-08 17:47:54,622 Epoch[63/120] train loss: 0.38863, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:47:57,217 Epoch[64/120] train loss: 0.38706, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:47:59,747 Epoch[65/120] train loss: 0.38676, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:02,280 Epoch[66/120] train loss: 0.38781, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:04,832 Epoch[67/120] train loss: 0.38937, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:48:07,366 Epoch[68/120] train loss: 0.38594, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:09,866 Epoch[69/120] train loss: 0.38532, val loss: nan, lr: 0.0010000, time: 2.50\n",
      "2022-06-08 17:48:12,452 Epoch[70/120] train loss: 0.38631, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:48:14,983 Epoch[71/120] train loss: 0.38767, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:17,504 Epoch[72/120] train loss: 0.38693, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:48:20,069 Epoch[73/120] train loss: 0.38523, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:48:22,632 Epoch[74/120] train loss: 0.38345, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:48:25,164 Epoch[75/120] train loss: 0.39109, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:27,724 Epoch[76/120] train loss: 0.38544, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:48:30,327 Epoch[77/120] train loss: 0.38404, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:48:32,862 Epoch[78/120] train loss: 0.38459, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:48:35,373 Epoch[79/120] train loss: 0.38639, val loss: nan, lr: 0.0010000, time: 2.51\n",
      "2022-06-08 17:48:37,930 Epoch[80/120] train loss: 0.38844, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:48:40,477 Epoch[81/120] train loss: 0.38537, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:48:43,054 Epoch[82/120] train loss: 0.38970, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:48:45,598 Epoch[83/120] train loss: 0.38782, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:48:48,122 Epoch[84/120] train loss: 0.38543, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:48:50,668 Epoch[85/120] train loss: 0.38536, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:48:53,255 Epoch[86/120] train loss: 0.38447, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:48:55,831 Epoch[87/120] train loss: 0.38423, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:48:58,392 Epoch[88/120] train loss: 0.38492, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:49:00,914 Epoch[89/120] train loss: 0.38299, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:49:03,477 Epoch[90/120] train loss: 0.38254, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:49:06,064 Epoch[91/120] train loss: 0.38371, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:49:08,601 Epoch[92/120] train loss: 0.38261, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:11,151 Epoch[93/120] train loss: 0.39014, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:49:13,709 Epoch[94/120] train loss: 0.38861, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:49:16,305 Epoch[95/120] train loss: 0.38364, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:49:18,848 Epoch[96/120] train loss: 0.38595, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:21,436 Epoch[97/120] train loss: 0.38241, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:49:23,974 Epoch[98/120] train loss: 0.38289, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:26,519 Epoch[99/120] train loss: 0.38354, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:29,056 Epoch[100/120] train loss: 0.38191, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:31,602 Epoch[101/120] train loss: 0.38320, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:34,144 Epoch[102/120] train loss: 0.38549, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:36,702 Epoch[103/120] train loss: 0.38297, val loss: nan, lr: 0.0010000, time: 2.56\n",
      "2022-06-08 17:49:39,294 Epoch[104/120] train loss: 0.38629, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:49:41,838 Epoch[105/120] train loss: 0.38278, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:44,382 Epoch[106/120] train loss: 0.38299, val loss: nan, lr: 0.0010000, time: 2.54\n",
      "2022-06-08 17:49:46,953 Epoch[107/120] train loss: 0.38510, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:49:49,505 Epoch[108/120] train loss: 0.38746, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:49:52,076 Epoch[109/120] train loss: 0.38272, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:49:54,663 Epoch[110/120] train loss: 0.38297, val loss: nan, lr: 0.0010000, time: 2.59\n",
      "2022-06-08 17:49:57,237 Epoch[111/120] train loss: 0.38873, val loss: nan, lr: 0.0010000, time: 2.57\n",
      "2022-06-08 17:49:59,821 Epoch[112/120] train loss: 0.38261, val loss: nan, lr: 0.0010000, time: 2.58\n",
      "2022-06-08 17:50:02,346 Epoch[113/120] train loss: 0.38347, val loss: nan, lr: 0.0010000, time: 2.52\n",
      "2022-06-08 17:50:04,856 Epoch[114/120] train loss: 0.38134, val loss: nan, lr: 0.0010000, time: 2.51\n",
      "2022-06-08 17:50:07,406 Epoch[115/120] train loss: 0.38297, val loss: nan, lr: 0.0010000, time: 2.55\n",
      "2022-06-08 17:50:09,936 Epoch[116/120] train loss: 0.38318, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:50:12,469 Epoch[117/120] train loss: 0.38040, val loss: nan, lr: 0.0010000, time: 2.53\n",
      "2022-06-08 17:50:15,068 Epoch[118/120] train loss: 0.38182, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:50:17,671 Epoch[119/120] train loss: 0.38299, val loss: nan, lr: 0.0010000, time: 2.60\n",
      "2022-06-08 17:50:20,182 Epoch[120/120] train loss: 0.38144, val loss: nan, lr: 0.0010000, time: 2.51\n",
      "2022-06-08 17:50:20,183 => end training\n",
      "2022-06-08 17:50:20,184 => calculating train scores\n",
      "2022-06-08 17:50:22,977 => train score\n",
      "accuracy: 0.9975498869006293\n",
      "presision: 0.7050516023900054\n",
      "recall: 0.9969278033794163\n",
      "f1: 0.8259624562519885\n",
      "2022-06-08 17:50:22,978 => calculating test scores\n",
      "2022-06-08 17:51:31,075 => test score\n",
      "accuracy: 0.9954641179329338\n",
      "presision: 0.20430107526881722\n",
      "recall: 0.2676056338028169\n",
      "f1: 0.23170731707317072\n",
      "2022-06-08 17:51:31,095 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:51:34,449 Epoch[1/120] train loss: 0.74124, val loss: nan, lr: 0.0010000, time: 3.35\n",
      "2022-06-08 17:51:37,533 Epoch[2/120] train loss: 0.60667, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:51:40,554 Epoch[3/120] train loss: 0.50964, val loss: nan, lr: 0.0010000, time: 3.02\n",
      "2022-06-08 17:51:43,616 Epoch[4/120] train loss: 0.45543, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:51:46,679 Epoch[5/120] train loss: 0.42548, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:51:49,712 Epoch[6/120] train loss: 0.40849, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 17:51:52,697 Epoch[7/120] train loss: 0.39857, val loss: nan, lr: 0.0010000, time: 2.98\n",
      "2022-06-08 17:51:55,820 Epoch[8/120] train loss: 0.39517, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:51:58,886 Epoch[9/120] train loss: 0.38718, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:52:01,986 Epoch[10/120] train loss: 0.38603, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:52:05,052 Epoch[11/120] train loss: 0.38207, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:52:08,102 Epoch[12/120] train loss: 0.37944, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:52:11,168 Epoch[13/120] train loss: 0.37858, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:52:14,227 Epoch[14/120] train loss: 0.37688, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:52:17,283 Epoch[15/120] train loss: 0.37612, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:52:20,349 Epoch[16/120] train loss: 0.37396, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:52:23,413 Epoch[17/120] train loss: 0.37289, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:52:26,439 Epoch[18/120] train loss: 0.37257, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 17:52:29,482 Epoch[19/120] train loss: 0.37204, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:52:32,607 Epoch[20/120] train loss: 0.37096, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:52:35,724 Epoch[21/120] train loss: 0.37110, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:52:38,830 Epoch[22/120] train loss: 0.37104, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:52:41,933 Epoch[23/120] train loss: 0.37050, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:52:44,969 Epoch[24/120] train loss: 0.36901, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:52:48,076 Epoch[25/120] train loss: 0.36904, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:52:51,146 Epoch[26/120] train loss: 0.36825, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:52:54,256 Epoch[27/120] train loss: 0.36816, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:52:57,361 Epoch[28/120] train loss: 0.36854, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:53:00,441 Epoch[29/120] train loss: 0.36794, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:53:03,523 Epoch[30/120] train loss: 0.36659, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:53:06,614 Epoch[31/120] train loss: 0.36721, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:53:09,790 Epoch[32/120] train loss: 0.36744, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 17:53:12,861 Epoch[33/120] train loss: 0.36659, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:53:15,961 Epoch[34/120] train loss: 0.36659, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:53:19,015 Epoch[35/120] train loss: 0.36563, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:53:22,106 Epoch[36/120] train loss: 0.36549, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:53:25,266 Epoch[37/120] train loss: 0.36534, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 17:53:28,362 Epoch[38/120] train loss: 0.36549, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:53:31,475 Epoch[39/120] train loss: 0.36424, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:53:34,594 Epoch[40/120] train loss: 0.36430, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:53:37,675 Epoch[41/120] train loss: 0.36423, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:53:40,748 Epoch[42/120] train loss: 0.36358, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:53:43,823 Epoch[43/120] train loss: 0.36541, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:53:46,997 Epoch[44/120] train loss: 0.36392, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 17:53:50,164 Epoch[45/120] train loss: 0.36322, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 17:53:53,258 Epoch[46/120] train loss: 0.36201, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:53:56,340 Epoch[47/120] train loss: 0.36163, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:53:59,433 Epoch[48/120] train loss: 0.36175, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:54:02,530 Epoch[49/120] train loss: 0.36232, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:54:05,626 Epoch[50/120] train loss: 0.36195, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:54:08,691 Epoch[51/120] train loss: 0.36189, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:54:11,787 Epoch[52/120] train loss: 0.36076, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:54:14,891 Epoch[53/120] train loss: 0.36240, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:54:18,011 Epoch[54/120] train loss: 0.36028, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:54:21,092 Epoch[55/120] train loss: 0.36065, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:54:24,128 Epoch[56/120] train loss: 0.36006, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:54:27,207 Epoch[57/120] train loss: 0.36326, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:54:30,334 Epoch[58/120] train loss: 0.36214, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:54:33,450 Epoch[59/120] train loss: 0.36071, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:54:36,499 Epoch[60/120] train loss: 0.36128, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:54:39,582 Epoch[61/120] train loss: 0.35955, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:54:42,610 Epoch[62/120] train loss: 0.35915, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 17:54:45,658 Epoch[63/120] train loss: 0.35957, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:54:48,730 Epoch[64/120] train loss: 0.35947, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:54:51,794 Epoch[65/120] train loss: 0.35964, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:54:54,876 Epoch[66/120] train loss: 0.35805, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:54:57,952 Epoch[67/120] train loss: 0.35785, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:55:01,038 Epoch[68/120] train loss: 0.35746, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:55:04,108 Epoch[69/120] train loss: 0.35852, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:55:07,176 Epoch[70/120] train loss: 0.35813, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:55:10,270 Epoch[71/120] train loss: 0.35837, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:55:13,340 Epoch[72/120] train loss: 0.35772, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:55:16,433 Epoch[73/120] train loss: 0.35773, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:55:19,485 Epoch[74/120] train loss: 0.35841, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:55:22,545 Epoch[75/120] train loss: 0.35750, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:55:25,649 Epoch[76/120] train loss: 0.35648, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:55:28,742 Epoch[77/120] train loss: 0.35717, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:55:31,803 Epoch[78/120] train loss: 0.35774, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:55:34,836 Epoch[79/120] train loss: 0.35648, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 17:55:37,893 Epoch[80/120] train loss: 0.35695, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:55:40,950 Epoch[81/120] train loss: 0.35754, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:55:44,042 Epoch[82/120] train loss: 0.35695, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:55:47,136 Epoch[83/120] train loss: 0.35655, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:55:50,305 Epoch[84/120] train loss: 0.35626, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 17:55:53,431 Epoch[85/120] train loss: 0.35649, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:55:56,513 Epoch[86/120] train loss: 0.35614, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:55:59,570 Epoch[87/120] train loss: 0.35527, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:56:02,642 Epoch[88/120] train loss: 0.35536, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:56:05,699 Epoch[89/120] train loss: 0.35684, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:56:08,779 Epoch[90/120] train loss: 0.35593, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:56:11,892 Epoch[91/120] train loss: 0.35646, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:56:14,996 Epoch[92/120] train loss: 0.35512, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:56:18,056 Epoch[93/120] train loss: 0.35620, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 17:56:21,163 Epoch[94/120] train loss: 0.35554, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:56:24,353 Epoch[95/120] train loss: 0.35532, val loss: nan, lr: 0.0010000, time: 3.19\n",
      "2022-06-08 17:56:27,459 Epoch[96/120] train loss: 0.35503, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:56:30,618 Epoch[97/120] train loss: 0.35584, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 17:56:33,737 Epoch[98/120] train loss: 0.35669, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:56:36,913 Epoch[99/120] train loss: 0.35611, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 17:56:40,089 Epoch[100/120] train loss: 0.35528, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 17:56:43,236 Epoch[101/120] train loss: 0.35591, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 17:56:46,358 Epoch[102/120] train loss: 0.35471, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:56:49,507 Epoch[103/120] train loss: 0.35523, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 17:56:52,579 Epoch[104/120] train loss: 0.35576, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:56:55,715 Epoch[105/120] train loss: 0.35491, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:56:58,812 Epoch[106/120] train loss: 0.35492, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:57:01,953 Epoch[107/120] train loss: 0.35612, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:57:05,135 Epoch[108/120] train loss: 0.35740, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 17:57:08,235 Epoch[109/120] train loss: 0.35488, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:57:11,368 Epoch[110/120] train loss: 0.35478, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:57:14,545 Epoch[111/120] train loss: 0.35623, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 17:57:17,669 Epoch[112/120] train loss: 0.35670, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:57:20,826 Epoch[113/120] train loss: 0.35497, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 17:57:23,968 Epoch[114/120] train loss: 0.35530, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:57:27,102 Epoch[115/120] train loss: 0.35408, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:57:30,256 Epoch[116/120] train loss: 0.35395, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 17:57:33,448 Epoch[117/120] train loss: 0.35399, val loss: nan, lr: 0.0010000, time: 3.19\n",
      "2022-06-08 17:57:36,591 Epoch[118/120] train loss: 0.35744, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:57:39,757 Epoch[119/120] train loss: 0.35568, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 17:57:42,878 Epoch[120/120] train loss: 0.35478, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 17:57:42,879 => end training\n",
      "2022-06-08 17:57:42,880 => calculating train scores\n",
      "2022-06-08 17:57:45,702 => train score\n",
      "accuracy: 0.9995341649683098\n",
      "presision: 0.9365889212827988\n",
      "recall: 0.9869431643625192\n",
      "f1: 0.9611069558713538\n",
      "2022-06-08 17:57:45,703 => calculating test scores\n",
      "2022-06-08 17:58:53,240 => test score\n",
      "accuracy: 0.9963280954695178\n",
      "presision: 0.3351063829787234\n",
      "recall: 0.44366197183098594\n",
      "f1: 0.38181818181818183\n",
      "2022-06-08 17:58:53,267 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 17:58:56,725 Epoch[1/120] train loss: 0.77693, val loss: nan, lr: 0.0010000, time: 3.46\n",
      "2022-06-08 17:58:59,801 Epoch[2/120] train loss: 0.65631, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:59:02,928 Epoch[3/120] train loss: 0.58447, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:59:06,109 Epoch[4/120] train loss: 0.52396, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 17:59:09,252 Epoch[5/120] train loss: 0.48418, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:59:12,390 Epoch[6/120] train loss: 0.45539, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:59:15,505 Epoch[7/120] train loss: 0.43497, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:59:18,635 Epoch[8/120] train loss: 0.42388, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:59:21,780 Epoch[9/120] train loss: 0.41367, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 17:59:24,818 Epoch[10/120] train loss: 0.40703, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:59:27,944 Epoch[11/120] train loss: 0.40100, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 17:59:31,015 Epoch[12/120] train loss: 0.39659, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 17:59:34,118 Epoch[13/120] train loss: 0.39513, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 17:59:37,167 Epoch[14/120] train loss: 0.39369, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 17:59:40,155 Epoch[15/120] train loss: 0.38905, val loss: nan, lr: 0.0010000, time: 2.99\n",
      "2022-06-08 17:59:43,232 Epoch[16/120] train loss: 0.38880, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 17:59:46,319 Epoch[17/120] train loss: 0.38877, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 17:59:49,356 Epoch[18/120] train loss: 0.38880, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:59:52,469 Epoch[19/120] train loss: 0.38581, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 17:59:55,506 Epoch[20/120] train loss: 0.38667, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 17:59:58,618 Epoch[21/120] train loss: 0.38370, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:00:01,698 Epoch[22/120] train loss: 0.38370, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:00:04,787 Epoch[23/120] train loss: 0.38147, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:00:07,833 Epoch[24/120] train loss: 0.38165, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 18:00:10,997 Epoch[25/120] train loss: 0.38043, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 18:00:14,044 Epoch[26/120] train loss: 0.38007, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 18:00:17,119 Epoch[27/120] train loss: 0.37866, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 18:00:20,215 Epoch[28/120] train loss: 0.37805, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:00:23,381 Epoch[29/120] train loss: 0.37765, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 18:00:26,458 Epoch[30/120] train loss: 0.38009, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:00:29,519 Epoch[31/120] train loss: 0.37744, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:00:32,550 Epoch[32/120] train loss: 0.37561, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 18:00:35,735 Epoch[33/120] train loss: 0.37704, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 18:00:38,840 Epoch[34/120] train loss: 0.37656, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:00:42,012 Epoch[35/120] train loss: 0.37480, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 18:00:45,098 Epoch[36/120] train loss: 0.37559, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:00:48,214 Epoch[37/120] train loss: 0.37541, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:00:51,336 Epoch[38/120] train loss: 0.37416, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:00:54,441 Epoch[39/120] train loss: 0.37364, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:00:57,605 Epoch[40/120] train loss: 0.37152, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 18:01:00,663 Epoch[41/120] train loss: 0.37264, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:01:03,728 Epoch[42/120] train loss: 0.37316, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:01:06,810 Epoch[43/120] train loss: 0.37322, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:01:09,869 Epoch[44/120] train loss: 0.37146, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:01:12,946 Epoch[45/120] train loss: 0.37130, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:01:16,062 Epoch[46/120] train loss: 0.37205, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:01:19,156 Epoch[47/120] train loss: 0.37389, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:01:22,277 Epoch[48/120] train loss: 0.37120, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:01:25,366 Epoch[49/120] train loss: 0.37052, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:01:28,529 Epoch[50/120] train loss: 0.37146, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 18:01:31,682 Epoch[51/120] train loss: 0.37097, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 18:01:34,825 Epoch[52/120] train loss: 0.37026, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 18:01:37,891 Epoch[53/120] train loss: 0.37132, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:01:41,074 Epoch[54/120] train loss: 0.36931, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 18:01:44,187 Epoch[55/120] train loss: 0.36890, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:01:47,284 Epoch[56/120] train loss: 0.36784, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:01:50,394 Epoch[57/120] train loss: 0.37317, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:01:53,531 Epoch[58/120] train loss: 0.36809, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 18:01:56,648 Epoch[59/120] train loss: 0.36820, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:01:59,728 Epoch[60/120] train loss: 0.36791, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:02:02,855 Epoch[61/120] train loss: 0.36772, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:02:06,053 Epoch[62/120] train loss: 0.36761, val loss: nan, lr: 0.0010000, time: 3.20\n",
      "2022-06-08 18:02:09,117 Epoch[63/120] train loss: 0.36831, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:02:12,252 Epoch[64/120] train loss: 0.36896, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:02:15,331 Epoch[65/120] train loss: 0.36763, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:02:18,420 Epoch[66/120] train loss: 0.37061, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:02:21,505 Epoch[67/120] train loss: 0.36703, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:02:24,610 Epoch[68/120] train loss: 0.36683, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:02:27,738 Epoch[69/120] train loss: 0.36671, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:02:30,811 Epoch[70/120] train loss: 0.36685, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 18:02:34,004 Epoch[71/120] train loss: 0.36733, val loss: nan, lr: 0.0010000, time: 3.19\n",
      "2022-06-08 18:02:37,168 Epoch[72/120] train loss: 0.36838, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 18:02:40,227 Epoch[73/120] train loss: 0.36619, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:02:43,296 Epoch[74/120] train loss: 0.36661, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 18:02:46,399 Epoch[75/120] train loss: 0.36687, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:02:49,437 Epoch[76/120] train loss: 0.36754, val loss: nan, lr: 0.0010000, time: 3.04\n",
      "2022-06-08 18:02:52,521 Epoch[77/120] train loss: 0.36657, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:02:55,590 Epoch[78/120] train loss: 0.36717, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 18:02:58,678 Epoch[79/120] train loss: 0.36520, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:03:01,793 Epoch[80/120] train loss: 0.36765, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:03:04,900 Epoch[81/120] train loss: 0.36554, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:03:07,971 Epoch[82/120] train loss: 0.36523, val loss: nan, lr: 0.0010000, time: 3.07\n",
      "2022-06-08 18:03:11,052 Epoch[83/120] train loss: 0.36507, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:03:14,201 Epoch[84/120] train loss: 0.36588, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 18:03:17,256 Epoch[85/120] train loss: 0.36727, val loss: nan, lr: 0.0010000, time: 3.05\n",
      "2022-06-08 18:03:20,338 Epoch[86/120] train loss: 0.37093, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:03:23,466 Epoch[87/120] train loss: 0.36579, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:03:26,586 Epoch[88/120] train loss: 0.36472, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:03:29,817 Epoch[89/120] train loss: 0.36527, val loss: nan, lr: 0.0010000, time: 3.23\n",
      "2022-06-08 18:03:33,008 Epoch[90/120] train loss: 0.36467, val loss: nan, lr: 0.0010000, time: 3.19\n",
      "2022-06-08 18:03:36,151 Epoch[91/120] train loss: 0.36503, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 18:03:39,338 Epoch[92/120] train loss: 0.36662, val loss: nan, lr: 0.0010000, time: 3.19\n",
      "2022-06-08 18:03:42,519 Epoch[93/120] train loss: 0.36609, val loss: nan, lr: 0.0010000, time: 3.18\n",
      "2022-06-08 18:03:45,686 Epoch[94/120] train loss: 0.36526, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 18:03:48,819 Epoch[95/120] train loss: 0.36946, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:03:51,952 Epoch[96/120] train loss: 0.36514, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:03:55,082 Epoch[97/120] train loss: 0.36710, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:03:58,223 Epoch[98/120] train loss: 0.36476, val loss: nan, lr: 0.0010000, time: 3.14\n",
      "2022-06-08 18:04:01,383 Epoch[99/120] train loss: 0.36426, val loss: nan, lr: 0.0010000, time: 3.16\n",
      "2022-06-08 18:04:04,504 Epoch[100/120] train loss: 0.36541, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:04:07,657 Epoch[101/120] train loss: 0.36528, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 18:04:10,812 Epoch[102/120] train loss: 0.36682, val loss: nan, lr: 0.0010000, time: 3.15\n",
      "2022-06-08 18:04:13,933 Epoch[103/120] train loss: 0.36457, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:04:17,049 Epoch[104/120] train loss: 0.36401, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:04:20,149 Epoch[105/120] train loss: 0.36354, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:04:23,241 Epoch[106/120] train loss: 0.36463, val loss: nan, lr: 0.0010000, time: 3.09\n",
      "2022-06-08 18:04:26,358 Epoch[107/120] train loss: 0.36348, val loss: nan, lr: 0.0010000, time: 3.12\n",
      "2022-06-08 18:04:29,464 Epoch[108/120] train loss: 0.36671, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:04:32,545 Epoch[109/120] train loss: 0.36550, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:04:35,675 Epoch[110/120] train loss: 0.36579, val loss: nan, lr: 0.0010000, time: 3.13\n",
      "2022-06-08 18:04:38,783 Epoch[111/120] train loss: 0.36331, val loss: nan, lr: 0.0010000, time: 3.11\n",
      "2022-06-08 18:04:41,841 Epoch[112/120] train loss: 0.36433, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:04:45,008 Epoch[113/120] train loss: 0.36677, val loss: nan, lr: 0.0010000, time: 3.17\n",
      "2022-06-08 18:04:48,108 Epoch[114/120] train loss: 0.36476, val loss: nan, lr: 0.0010000, time: 3.10\n",
      "2022-06-08 18:04:51,140 Epoch[115/120] train loss: 0.36462, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 18:04:54,204 Epoch[116/120] train loss: 0.36519, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:04:57,235 Epoch[117/120] train loss: 0.36342, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 18:05:00,293 Epoch[118/120] train loss: 0.36392, val loss: nan, lr: 0.0010000, time: 3.06\n",
      "2022-06-08 18:05:03,370 Epoch[119/120] train loss: 0.36471, val loss: nan, lr: 0.0010000, time: 3.08\n",
      "2022-06-08 18:05:06,405 Epoch[120/120] train loss: 0.36361, val loss: nan, lr: 0.0010000, time: 3.03\n",
      "2022-06-08 18:05:06,406 => end training\n",
      "2022-06-08 18:05:06,406 => calculating train scores\n",
      "2022-06-08 18:05:09,481 => train score\n",
      "accuracy: 0.9986920785648697\n",
      "presision: 0.8713235294117647\n",
      "recall: 0.9101382488479263\n",
      "f1: 0.8903080390683696\n",
      "2022-06-08 18:05:09,482 => calculating test scores\n",
      "2022-06-08 18:06:17,467 => test score\n",
      "accuracy: 0.9968500818978706\n",
      "presision: 0.4057142857142857\n",
      "recall: 0.5\n",
      "f1: 0.4479495268138801\n",
      "2022-06-08 18:06:17,501 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:06:20,118 Epoch[1/120] train loss: 0.74946, val loss: nan, lr: 0.0010000, time: 2.62\n",
      "2022-06-08 18:06:22,502 Epoch[2/120] train loss: 0.63393, val loss: nan, lr: 0.0010000, time: 2.38\n",
      "2022-06-08 18:06:24,899 Epoch[3/120] train loss: 0.55915, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:06:27,296 Epoch[4/120] train loss: 0.50858, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:06:29,700 Epoch[5/120] train loss: 0.47911, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:06:32,085 Epoch[6/120] train loss: 0.45546, val loss: nan, lr: 0.0010000, time: 2.38\n",
      "2022-06-08 18:06:34,486 Epoch[7/120] train loss: 0.44259, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:06:36,931 Epoch[8/120] train loss: 0.43739, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:06:39,319 Epoch[9/120] train loss: 0.42827, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:06:41,742 Epoch[10/120] train loss: 0.42641, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:06:44,125 Epoch[11/120] train loss: 0.42520, val loss: nan, lr: 0.0010000, time: 2.38\n",
      "2022-06-08 18:06:46,564 Epoch[12/120] train loss: 0.41859, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:06:48,930 Epoch[13/120] train loss: 0.41612, val loss: nan, lr: 0.0010000, time: 2.36\n",
      "2022-06-08 18:06:51,322 Epoch[14/120] train loss: 0.41467, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:06:53,724 Epoch[15/120] train loss: 0.41716, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:06:56,137 Epoch[16/120] train loss: 0.41378, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:06:58,546 Epoch[17/120] train loss: 0.41441, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:07:00,971 Epoch[18/120] train loss: 0.41170, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:03,377 Epoch[19/120] train loss: 0.41101, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:07:05,793 Epoch[20/120] train loss: 0.41198, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:08,262 Epoch[21/120] train loss: 0.40935, val loss: nan, lr: 0.0010000, time: 2.47\n",
      "2022-06-08 18:07:10,680 Epoch[22/120] train loss: 0.40867, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:13,097 Epoch[23/120] train loss: 0.41044, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:15,508 Epoch[24/120] train loss: 0.40655, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:07:17,925 Epoch[25/120] train loss: 0.40708, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:20,353 Epoch[26/120] train loss: 0.40725, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:07:22,796 Epoch[27/120] train loss: 0.40643, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:07:25,229 Epoch[28/120] train loss: 0.40950, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:07:27,648 Epoch[29/120] train loss: 0.40663, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:30,058 Epoch[30/120] train loss: 0.40389, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:07:32,477 Epoch[31/120] train loss: 0.40294, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:34,917 Epoch[32/120] train loss: 0.40426, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:07:37,339 Epoch[33/120] train loss: 0.40282, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:39,785 Epoch[34/120] train loss: 0.40027, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:07:42,205 Epoch[35/120] train loss: 0.39917, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:44,608 Epoch[36/120] train loss: 0.39956, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:07:47,062 Epoch[37/120] train loss: 0.39830, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:07:49,491 Epoch[38/120] train loss: 0.39756, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:07:51,888 Epoch[39/120] train loss: 0.39696, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:07:54,299 Epoch[40/120] train loss: 0.40244, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:07:56,720 Epoch[41/120] train loss: 0.39858, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:07:59,153 Epoch[42/120] train loss: 0.40153, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:01,562 Epoch[43/120] train loss: 0.39901, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:08:03,950 Epoch[44/120] train loss: 0.39667, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:08:06,347 Epoch[45/120] train loss: 0.39597, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:08:08,752 Epoch[46/120] train loss: 0.39454, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:08:11,196 Epoch[47/120] train loss: 0.39808, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:08:13,599 Epoch[48/120] train loss: 0.39298, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:08:16,030 Epoch[49/120] train loss: 0.39414, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:18,449 Epoch[50/120] train loss: 0.39303, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:08:20,862 Epoch[51/120] train loss: 0.39336, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:08:23,318 Epoch[52/120] train loss: 0.39274, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:08:25,745 Epoch[53/120] train loss: 0.39090, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:28,161 Epoch[54/120] train loss: 0.39395, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:08:30,585 Epoch[55/120] train loss: 0.39291, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:08:32,998 Epoch[56/120] train loss: 0.39107, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:08:35,429 Epoch[57/120] train loss: 0.39118, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:37,839 Epoch[58/120] train loss: 0.39156, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:08:40,278 Epoch[59/120] train loss: 0.38992, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:08:42,710 Epoch[60/120] train loss: 0.38888, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:45,111 Epoch[61/120] train loss: 0.39321, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:08:47,539 Epoch[62/120] train loss: 0.38997, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:08:49,959 Epoch[63/120] train loss: 0.39371, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:08:52,361 Epoch[64/120] train loss: 0.38939, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:08:54,771 Epoch[65/120] train loss: 0.38878, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:08:57,225 Epoch[66/120] train loss: 0.38980, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:08:59,630 Epoch[67/120] train loss: 0.39751, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:09:02,018 Epoch[68/120] train loss: 0.38867, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:09:04,473 Epoch[69/120] train loss: 0.38834, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:09:06,867 Epoch[70/120] train loss: 0.39092, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:09:09,261 Epoch[71/120] train loss: 0.38598, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:09:11,710 Epoch[72/120] train loss: 0.38820, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:09:14,143 Epoch[73/120] train loss: 0.38711, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:09:16,573 Epoch[74/120] train loss: 0.38422, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:09:18,973 Epoch[75/120] train loss: 0.38791, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:09:21,407 Epoch[76/120] train loss: 0.38771, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:09:23,830 Epoch[77/120] train loss: 0.38572, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:09:26,242 Epoch[78/120] train loss: 0.38804, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:09:28,651 Epoch[79/120] train loss: 0.38632, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:09:31,031 Epoch[80/120] train loss: 0.38681, val loss: nan, lr: 0.0010000, time: 2.38\n",
      "2022-06-08 18:09:33,466 Epoch[81/120] train loss: 0.38553, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:09:35,868 Epoch[82/120] train loss: 0.38456, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:09:38,260 Epoch[83/120] train loss: 0.38329, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:09:40,714 Epoch[84/120] train loss: 0.39036, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:09:43,124 Epoch[85/120] train loss: 0.38698, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:09:45,545 Epoch[86/120] train loss: 0.38487, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:09:47,937 Epoch[87/120] train loss: 0.39462, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:09:50,353 Epoch[88/120] train loss: 0.38535, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:09:52,765 Epoch[89/120] train loss: 0.38340, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:09:55,214 Epoch[90/120] train loss: 0.38358, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:09:57,652 Epoch[91/120] train loss: 0.38267, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:00,080 Epoch[92/120] train loss: 0.39166, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:10:02,517 Epoch[93/120] train loss: 0.38410, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:04,944 Epoch[94/120] train loss: 0.38787, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:10:07,413 Epoch[95/120] train loss: 0.38687, val loss: nan, lr: 0.0010000, time: 2.47\n",
      "2022-06-08 18:10:09,841 Epoch[96/120] train loss: 0.38363, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:10:12,267 Epoch[97/120] train loss: 0.38383, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:10:14,693 Epoch[98/120] train loss: 0.38469, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:17,110 Epoch[99/120] train loss: 0.38287, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:19,513 Epoch[100/120] train loss: 0.38391, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:10:21,961 Epoch[101/120] train loss: 0.38468, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:10:24,412 Epoch[102/120] train loss: 0.38243, val loss: nan, lr: 0.0010000, time: 2.45\n",
      "2022-06-08 18:10:26,857 Epoch[103/120] train loss: 0.38158, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:29,276 Epoch[104/120] train loss: 0.38388, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:31,736 Epoch[105/120] train loss: 0.38174, val loss: nan, lr: 0.0010000, time: 2.46\n",
      "2022-06-08 18:10:34,151 Epoch[106/120] train loss: 0.38237, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:10:36,542 Epoch[107/120] train loss: 0.38643, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:10:38,935 Epoch[108/120] train loss: 0.38557, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:10:41,344 Epoch[109/120] train loss: 0.38492, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:10:43,761 Epoch[110/120] train loss: 0.38440, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:46,200 Epoch[111/120] train loss: 0.38220, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:48,638 Epoch[112/120] train loss: 0.38334, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:51,061 Epoch[113/120] train loss: 0.38773, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:53,485 Epoch[114/120] train loss: 0.38210, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:10:55,931 Epoch[115/120] train loss: 0.38178, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:10:58,347 Epoch[116/120] train loss: 0.38694, val loss: nan, lr: 0.0010000, time: 2.41\n",
      "2022-06-08 18:11:00,734 Epoch[117/120] train loss: 0.38514, val loss: nan, lr: 0.0010000, time: 2.39\n",
      "2022-06-08 18:11:03,172 Epoch[118/120] train loss: 0.38419, val loss: nan, lr: 0.0010000, time: 2.44\n",
      "2022-06-08 18:11:05,590 Epoch[119/120] train loss: 0.38136, val loss: nan, lr: 0.0010000, time: 2.42\n",
      "2022-06-08 18:11:07,992 Epoch[120/120] train loss: 0.38113, val loss: nan, lr: 0.0010000, time: 2.40\n",
      "2022-06-08 18:11:07,992 => end training\n",
      "2022-06-08 18:11:07,993 => calculating train scores\n",
      "2022-06-08 18:11:10,209 => train score\n",
      "accuracy: 0.9981859308862063\n",
      "presision: 0.7642899233942251\n",
      "recall: 0.9961597542242704\n",
      "f1: 0.8649549849949983\n",
      "2022-06-08 18:11:10,210 => calculating test scores\n",
      "2022-06-08 18:12:18,586 => test score\n",
      "accuracy: 0.9960941015533596\n",
      "presision: 0.3271889400921659\n",
      "recall: 0.5\n",
      "f1: 0.3955431754874652\n",
      "2022-06-08 18:12:18,620 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:12:23,629 Epoch[1/120] train loss: 0.75627, val loss: nan, lr: 0.0010000, time: 5.01\n",
      "2022-06-08 18:12:28,407 Epoch[2/120] train loss: 0.64367, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:12:33,199 Epoch[3/120] train loss: 0.55922, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:12:38,017 Epoch[4/120] train loss: 0.50376, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:12:42,860 Epoch[5/120] train loss: 0.47384, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:12:47,652 Epoch[6/120] train loss: 0.44877, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:12:52,450 Epoch[7/120] train loss: 0.43171, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:12:57,299 Epoch[8/120] train loss: 0.41828, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:13:02,117 Epoch[9/120] train loss: 0.40890, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:13:06,907 Epoch[10/120] train loss: 0.40098, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:13:11,669 Epoch[11/120] train loss: 0.39400, val loss: nan, lr: 0.0010000, time: 4.76\n",
      "2022-06-08 18:13:16,470 Epoch[12/120] train loss: 0.39019, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:13:21,286 Epoch[13/120] train loss: 0.38565, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:13:26,081 Epoch[14/120] train loss: 0.38421, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:13:30,884 Epoch[15/120] train loss: 0.38229, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:13:35,676 Epoch[16/120] train loss: 0.37993, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:13:40,478 Epoch[17/120] train loss: 0.37790, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:13:45,301 Epoch[18/120] train loss: 0.37611, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:13:50,056 Epoch[19/120] train loss: 0.37679, val loss: nan, lr: 0.0010000, time: 4.75\n",
      "2022-06-08 18:13:54,841 Epoch[20/120] train loss: 0.37512, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:13:59,616 Epoch[21/120] train loss: 0.37399, val loss: nan, lr: 0.0010000, time: 4.77\n",
      "2022-06-08 18:14:04,429 Epoch[22/120] train loss: 0.37468, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:14:09,201 Epoch[23/120] train loss: 0.37244, val loss: nan, lr: 0.0010000, time: 4.77\n",
      "2022-06-08 18:14:14,008 Epoch[24/120] train loss: 0.37202, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:14:18,865 Epoch[25/120] train loss: 0.37068, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:14:23,734 Epoch[26/120] train loss: 0.37050, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:14:28,610 Epoch[27/120] train loss: 0.37021, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:14:33,430 Epoch[28/120] train loss: 0.37000, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:14:38,276 Epoch[29/120] train loss: 0.36937, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:14:43,101 Epoch[30/120] train loss: 0.36785, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:14:47,938 Epoch[31/120] train loss: 0.36778, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:14:52,818 Epoch[32/120] train loss: 0.36868, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:14:57,703 Epoch[33/120] train loss: 0.36768, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:15:02,550 Epoch[34/120] train loss: 0.36640, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:15:07,407 Epoch[35/120] train loss: 0.36637, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:15:12,245 Epoch[36/120] train loss: 0.36530, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:15:17,126 Epoch[37/120] train loss: 0.36582, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:15:21,964 Epoch[38/120] train loss: 0.36635, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:15:26,819 Epoch[39/120] train loss: 0.36666, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:15:31,660 Epoch[40/120] train loss: 0.36576, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:15:36,571 Epoch[41/120] train loss: 0.36522, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:15:41,403 Epoch[42/120] train loss: 0.36537, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:15:46,200 Epoch[43/120] train loss: 0.36378, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:15:51,097 Epoch[44/120] train loss: 0.36480, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:15:55,967 Epoch[45/120] train loss: 0.36410, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:16:00,828 Epoch[46/120] train loss: 0.36320, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:16:05,696 Epoch[47/120] train loss: 0.36326, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:16:10,562 Epoch[48/120] train loss: 0.36235, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:16:15,455 Epoch[49/120] train loss: 0.36165, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:16:20,288 Epoch[50/120] train loss: 0.36243, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:16:25,170 Epoch[51/120] train loss: 0.36143, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:16:30,011 Epoch[52/120] train loss: 0.36127, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:16:34,822 Epoch[53/120] train loss: 0.36274, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:16:39,684 Epoch[54/120] train loss: 0.36290, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:16:44,544 Epoch[55/120] train loss: 0.36185, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:16:49,410 Epoch[56/120] train loss: 0.36214, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:16:54,262 Epoch[57/120] train loss: 0.36141, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:16:59,138 Epoch[58/120] train loss: 0.36095, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:17:04,014 Epoch[59/120] train loss: 0.35995, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:17:08,872 Epoch[60/120] train loss: 0.35968, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:17:13,771 Epoch[61/120] train loss: 0.35987, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:17:18,594 Epoch[62/120] train loss: 0.35940, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:17:23,453 Epoch[63/120] train loss: 0.36113, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:17:28,331 Epoch[64/120] train loss: 0.35926, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:17:33,193 Epoch[65/120] train loss: 0.36054, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:17:38,025 Epoch[66/120] train loss: 0.35814, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:17:42,887 Epoch[67/120] train loss: 0.35861, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:17:47,739 Epoch[68/120] train loss: 0.35967, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:17:52,622 Epoch[69/120] train loss: 0.35881, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:17:57,486 Epoch[70/120] train loss: 0.35999, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:18:02,336 Epoch[71/120] train loss: 0.35846, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:18:07,228 Epoch[72/120] train loss: 0.35891, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:18:12,077 Epoch[73/120] train loss: 0.35806, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:18:16,942 Epoch[74/120] train loss: 0.35838, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:18:21,816 Epoch[75/120] train loss: 0.35724, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:18:26,711 Epoch[76/120] train loss: 0.35804, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:18:31,587 Epoch[77/120] train loss: 0.35820, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:18:36,457 Epoch[78/120] train loss: 0.35768, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:18:41,303 Epoch[79/120] train loss: 0.35748, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:18:46,166 Epoch[80/120] train loss: 0.35861, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:18:51,040 Epoch[81/120] train loss: 0.35722, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:18:55,918 Epoch[82/120] train loss: 0.35683, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:19:00,806 Epoch[83/120] train loss: 0.35660, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:19:05,695 Epoch[84/120] train loss: 0.35615, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:19:10,591 Epoch[85/120] train loss: 0.35667, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:19:15,483 Epoch[86/120] train loss: 0.35706, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:19:20,354 Epoch[87/120] train loss: 0.35841, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:19:25,225 Epoch[88/120] train loss: 0.35605, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:19:30,138 Epoch[89/120] train loss: 0.35607, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:19:34,995 Epoch[90/120] train loss: 0.35727, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:19:39,830 Epoch[91/120] train loss: 0.35676, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:19:44,691 Epoch[92/120] train loss: 0.35599, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:19:49,512 Epoch[93/120] train loss: 0.35581, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:19:54,420 Epoch[94/120] train loss: 0.35597, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:19:59,288 Epoch[95/120] train loss: 0.35571, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:20:04,095 Epoch[96/120] train loss: 0.35726, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:20:08,888 Epoch[97/120] train loss: 0.35647, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:20:13,670 Epoch[98/120] train loss: 0.35569, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:20:18,473 Epoch[99/120] train loss: 0.35569, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:20:23,247 Epoch[100/120] train loss: 0.35536, val loss: nan, lr: 0.0010000, time: 4.77\n",
      "2022-06-08 18:20:28,047 Epoch[101/120] train loss: 0.35528, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:20:32,826 Epoch[102/120] train loss: 0.35637, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:20:37,644 Epoch[103/120] train loss: 0.35660, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:20:42,477 Epoch[104/120] train loss: 0.35577, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:20:47,262 Epoch[105/120] train loss: 0.35583, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:20:52,078 Epoch[106/120] train loss: 0.35601, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:20:56,837 Epoch[107/120] train loss: 0.35496, val loss: nan, lr: 0.0010000, time: 4.76\n",
      "2022-06-08 18:21:01,627 Epoch[108/120] train loss: 0.35631, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:21:06,412 Epoch[109/120] train loss: 0.35637, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:21:11,192 Epoch[110/120] train loss: 0.35507, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:21:16,017 Epoch[111/120] train loss: 0.35525, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:21:20,815 Epoch[112/120] train loss: 0.35659, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:21:25,615 Epoch[113/120] train loss: 0.35515, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:21:30,418 Epoch[114/120] train loss: 0.35462, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:21:35,222 Epoch[115/120] train loss: 0.35556, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:21:40,044 Epoch[116/120] train loss: 0.35581, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:21:44,876 Epoch[117/120] train loss: 0.35547, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:21:49,697 Epoch[118/120] train loss: 0.35529, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:21:54,488 Epoch[119/120] train loss: 0.35531, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:21:59,296 Epoch[120/120] train loss: 0.35583, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:21:59,296 => end training\n",
      "2022-06-08 18:21:59,297 => calculating train scores\n",
      "2022-06-08 18:22:01,597 => train score\n",
      "accuracy: 0.9985756198069472\n",
      "presision: 0.8236842105263158\n",
      "recall: 0.9615975422427036\n",
      "f1: 0.88731396172927\n",
      "2022-06-08 18:22:01,598 => calculating test scores\n",
      "2022-06-08 18:23:08,790 => test score\n",
      "accuracy: 0.9970480767500045\n",
      "presision: 0.44086021505376344\n",
      "recall: 0.5774647887323944\n",
      "f1: 0.5\n",
      "2022-06-08 18:23:08,845 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:23:13,784 Epoch[1/120] train loss: 0.70558, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:23:18,529 Epoch[2/120] train loss: 0.62561, val loss: nan, lr: 0.0010000, time: 4.74\n",
      "2022-06-08 18:23:23,327 Epoch[3/120] train loss: 0.55888, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:23:28,134 Epoch[4/120] train loss: 0.51276, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:23:32,931 Epoch[5/120] train loss: 0.47364, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:23:37,724 Epoch[6/120] train loss: 0.44813, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:23:42,530 Epoch[7/120] train loss: 0.43114, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:23:47,369 Epoch[8/120] train loss: 0.41609, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:23:52,146 Epoch[9/120] train loss: 0.41069, val loss: nan, lr: 0.0010000, time: 4.78\n",
      "2022-06-08 18:23:56,945 Epoch[10/120] train loss: 0.40384, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:24:01,760 Epoch[11/120] train loss: 0.39872, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:24:06,572 Epoch[12/120] train loss: 0.39700, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:24:11,375 Epoch[13/120] train loss: 0.39285, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:24:16,193 Epoch[14/120] train loss: 0.39431, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:24:20,952 Epoch[15/120] train loss: 0.39174, val loss: nan, lr: 0.0010000, time: 4.76\n",
      "2022-06-08 18:24:25,743 Epoch[16/120] train loss: 0.39021, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:24:30,567 Epoch[17/120] train loss: 0.38812, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:24:35,355 Epoch[18/120] train loss: 0.38607, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:24:40,146 Epoch[19/120] train loss: 0.38741, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:24:44,948 Epoch[20/120] train loss: 0.38326, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:24:49,763 Epoch[21/120] train loss: 0.38436, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:24:54,529 Epoch[22/120] train loss: 0.38216, val loss: nan, lr: 0.0010000, time: 4.77\n",
      "2022-06-08 18:24:59,332 Epoch[23/120] train loss: 0.38188, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:25:04,118 Epoch[24/120] train loss: 0.38221, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:25:09,270 Epoch[25/120] train loss: 0.38835, val loss: nan, lr: 0.0010000, time: 5.15\n",
      "2022-06-08 18:25:14,151 Epoch[26/120] train loss: 0.38174, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:25:19,040 Epoch[27/120] train loss: 0.38067, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:25:23,917 Epoch[28/120] train loss: 0.38086, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:25:28,813 Epoch[29/120] train loss: 0.38080, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:25:33,654 Epoch[30/120] train loss: 0.37869, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:25:38,536 Epoch[31/120] train loss: 0.38196, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:25:43,379 Epoch[32/120] train loss: 0.37989, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:25:48,234 Epoch[33/120] train loss: 0.37799, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:25:53,107 Epoch[34/120] train loss: 0.37775, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:25:57,957 Epoch[35/120] train loss: 0.37711, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:26:02,814 Epoch[36/120] train loss: 0.37696, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:26:07,672 Epoch[37/120] train loss: 0.37688, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:26:12,530 Epoch[38/120] train loss: 0.37618, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:26:17,336 Epoch[39/120] train loss: 0.37626, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:26:22,185 Epoch[40/120] train loss: 0.37596, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:26:27,041 Epoch[41/120] train loss: 0.37661, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:26:31,877 Epoch[42/120] train loss: 0.37614, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:26:36,745 Epoch[43/120] train loss: 0.37502, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:26:41,663 Epoch[44/120] train loss: 0.37560, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:26:46,555 Epoch[45/120] train loss: 0.37455, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:26:51,444 Epoch[46/120] train loss: 0.37403, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:26:56,303 Epoch[47/120] train loss: 0.37432, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:27:01,198 Epoch[48/120] train loss: 0.37332, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:27:06,118 Epoch[49/120] train loss: 0.37442, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:27:11,014 Epoch[50/120] train loss: 0.37466, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:27:15,919 Epoch[51/120] train loss: 0.37205, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:27:20,833 Epoch[52/120] train loss: 0.37466, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:27:25,696 Epoch[53/120] train loss: 0.37358, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:27:30,594 Epoch[54/120] train loss: 0.37209, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:27:35,499 Epoch[55/120] train loss: 0.37211, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:27:40,436 Epoch[56/120] train loss: 0.37441, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:27:45,292 Epoch[57/120] train loss: 0.37246, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:27:50,214 Epoch[58/120] train loss: 0.37163, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:27:55,120 Epoch[59/120] train loss: 0.37211, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:28:00,000 Epoch[60/120] train loss: 0.37386, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:28:04,895 Epoch[61/120] train loss: 0.37262, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:28:09,793 Epoch[62/120] train loss: 0.37145, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:28:14,662 Epoch[63/120] train loss: 0.37155, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:28:19,562 Epoch[64/120] train loss: 0.37051, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:28:24,454 Epoch[65/120] train loss: 0.37018, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:28:29,348 Epoch[66/120] train loss: 0.37103, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:28:34,237 Epoch[67/120] train loss: 0.37147, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:28:39,148 Epoch[68/120] train loss: 0.37030, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:28:44,044 Epoch[69/120] train loss: 0.36947, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:28:48,942 Epoch[70/120] train loss: 0.36785, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:28:53,819 Epoch[71/120] train loss: 0.36795, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:28:58,751 Epoch[72/120] train loss: 0.37216, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:29:03,669 Epoch[73/120] train loss: 0.36928, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:29:08,580 Epoch[74/120] train loss: 0.36987, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:29:13,459 Epoch[75/120] train loss: 0.36785, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:29:18,325 Epoch[76/120] train loss: 0.36725, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:29:23,209 Epoch[77/120] train loss: 0.37022, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:29:28,109 Epoch[78/120] train loss: 0.36906, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:29:33,034 Epoch[79/120] train loss: 0.36698, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:29:37,934 Epoch[80/120] train loss: 0.36714, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:29:42,832 Epoch[81/120] train loss: 0.36656, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:29:47,742 Epoch[82/120] train loss: 0.36880, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:29:52,662 Epoch[83/120] train loss: 0.36755, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:29:57,546 Epoch[84/120] train loss: 0.36709, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:30:02,456 Epoch[85/120] train loss: 0.36999, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:30:07,346 Epoch[86/120] train loss: 0.37038, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:30:12,274 Epoch[87/120] train loss: 0.36619, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:30:17,179 Epoch[88/120] train loss: 0.36747, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:30:22,079 Epoch[89/120] train loss: 0.36751, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:30:27,001 Epoch[90/120] train loss: 0.36607, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:30:31,918 Epoch[91/120] train loss: 0.36518, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:30:36,804 Epoch[92/120] train loss: 0.36993, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:30:41,666 Epoch[93/120] train loss: 0.36813, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:30:46,574 Epoch[94/120] train loss: 0.36565, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:30:51,460 Epoch[95/120] train loss: 0.36708, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:30:56,342 Epoch[96/120] train loss: 0.36704, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:31:01,252 Epoch[97/120] train loss: 0.36573, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:31:06,189 Epoch[98/120] train loss: 0.36501, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:31:11,141 Epoch[99/120] train loss: 0.36594, val loss: nan, lr: 0.0010000, time: 4.95\n",
      "2022-06-08 18:31:16,158 Epoch[100/120] train loss: 0.36577, val loss: nan, lr: 0.0010000, time: 5.02\n",
      "2022-06-08 18:31:21,035 Epoch[101/120] train loss: 0.36659, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:31:25,935 Epoch[102/120] train loss: 0.36542, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:31:30,828 Epoch[103/120] train loss: 0.36448, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:31:35,744 Epoch[104/120] train loss: 0.36408, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:31:40,616 Epoch[105/120] train loss: 0.36436, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:31:45,528 Epoch[106/120] train loss: 0.36750, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:31:50,411 Epoch[107/120] train loss: 0.36817, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:31:55,320 Epoch[108/120] train loss: 0.36509, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:00,232 Epoch[109/120] train loss: 0.36393, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:05,121 Epoch[110/120] train loss: 0.36474, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:32:10,026 Epoch[111/120] train loss: 0.37013, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:32:14,968 Epoch[112/120] train loss: 0.36573, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:32:19,879 Epoch[113/120] train loss: 0.36425, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:24,772 Epoch[114/120] train loss: 0.36447, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:32:29,660 Epoch[115/120] train loss: 0.36456, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:32:34,572 Epoch[116/120] train loss: 0.36409, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:39,472 Epoch[117/120] train loss: 0.36527, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:32:44,383 Epoch[118/120] train loss: 0.36447, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:49,269 Epoch[119/120] train loss: 0.36416, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:32:54,180 Epoch[120/120] train loss: 0.36353, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:32:54,181 => end training\n",
      "2022-06-08 18:32:54,182 => calculating train scores\n",
      "2022-06-08 18:32:56,548 => train score\n",
      "accuracy: 0.999377393563414\n",
      "presision: 0.9080701754385965\n",
      "recall: 0.9938556067588326\n",
      "f1: 0.9490282361569491\n",
      "2022-06-08 18:32:56,549 => calculating test scores\n",
      "2022-06-08 18:34:04,231 => test score\n",
      "accuracy: 0.9962020987454326\n",
      "presision: 0.31746031746031744\n",
      "recall: 0.4225352112676056\n",
      "f1: 0.36253776435045315\n",
      "2022-06-08 18:34:04,287 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:34:09,225 Epoch[1/120] train loss: 0.86205, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:34:14,036 Epoch[2/120] train loss: 0.74363, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:34:18,889 Epoch[3/120] train loss: 0.64237, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:34:23,714 Epoch[4/120] train loss: 0.56876, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:34:28,521 Epoch[5/120] train loss: 0.52543, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:34:33,353 Epoch[6/120] train loss: 0.49496, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:34:38,152 Epoch[7/120] train loss: 0.47399, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:34:42,980 Epoch[8/120] train loss: 0.45795, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:34:47,812 Epoch[9/120] train loss: 0.44710, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:34:52,630 Epoch[10/120] train loss: 0.44201, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:34:57,452 Epoch[11/120] train loss: 0.43310, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:35:02,256 Epoch[12/120] train loss: 0.43024, val loss: nan, lr: 0.0010000, time: 4.80\n",
      "2022-06-08 18:35:07,079 Epoch[13/120] train loss: 0.42459, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:35:11,895 Epoch[14/120] train loss: 0.42438, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:35:16,722 Epoch[15/120] train loss: 0.42191, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:35:21,554 Epoch[16/120] train loss: 0.41816, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:35:26,430 Epoch[17/120] train loss: 0.41519, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:35:31,308 Epoch[18/120] train loss: 0.41290, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:35:36,142 Epoch[19/120] train loss: 0.41173, val loss: nan, lr: 0.0010000, time: 4.83\n",
      "2022-06-08 18:35:40,993 Epoch[20/120] train loss: 0.41207, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:35:45,805 Epoch[21/120] train loss: 0.41176, val loss: nan, lr: 0.0010000, time: 4.81\n",
      "2022-06-08 18:35:50,643 Epoch[22/120] train loss: 0.40834, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:35:55,509 Epoch[23/120] train loss: 0.40797, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:36:00,377 Epoch[24/120] train loss: 0.40804, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:36:05,169 Epoch[25/120] train loss: 0.40399, val loss: nan, lr: 0.0010000, time: 4.79\n",
      "2022-06-08 18:36:09,992 Epoch[26/120] train loss: 0.40450, val loss: nan, lr: 0.0010000, time: 4.82\n",
      "2022-06-08 18:36:14,850 Epoch[27/120] train loss: 0.40436, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:36:19,701 Epoch[28/120] train loss: 0.40433, val loss: nan, lr: 0.0010000, time: 4.85\n",
      "2022-06-08 18:36:24,572 Epoch[29/120] train loss: 0.40311, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:36:29,505 Epoch[30/120] train loss: 0.40224, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:36:34,455 Epoch[31/120] train loss: 0.40130, val loss: nan, lr: 0.0010000, time: 4.95\n",
      "2022-06-08 18:36:39,383 Epoch[32/120] train loss: 0.40331, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:36:44,310 Epoch[33/120] train loss: 0.40076, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:36:49,260 Epoch[34/120] train loss: 0.40736, val loss: nan, lr: 0.0010000, time: 4.95\n",
      "2022-06-08 18:36:54,226 Epoch[35/120] train loss: 0.40013, val loss: nan, lr: 0.0010000, time: 4.97\n",
      "2022-06-08 18:36:59,166 Epoch[36/120] train loss: 0.40050, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:37:04,062 Epoch[37/120] train loss: 0.39962, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:37:08,960 Epoch[38/120] train loss: 0.39629, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:37:13,874 Epoch[39/120] train loss: 0.39612, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:37:18,786 Epoch[40/120] train loss: 0.39688, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:37:23,721 Epoch[41/120] train loss: 0.39943, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:37:28,624 Epoch[42/120] train loss: 0.39633, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:37:33,525 Epoch[43/120] train loss: 0.39521, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:37:38,426 Epoch[44/120] train loss: 0.39762, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:37:43,373 Epoch[45/120] train loss: 0.39638, val loss: nan, lr: 0.0010000, time: 4.95\n",
      "2022-06-08 18:37:48,291 Epoch[46/120] train loss: 0.39417, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:37:53,197 Epoch[47/120] train loss: 0.39399, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:37:58,119 Epoch[48/120] train loss: 0.39418, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:38:03,004 Epoch[49/120] train loss: 0.39550, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:38:07,918 Epoch[50/120] train loss: 0.39384, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:38:12,880 Epoch[51/120] train loss: 0.39158, val loss: nan, lr: 0.0010000, time: 4.96\n",
      "2022-06-08 18:38:17,816 Epoch[52/120] train loss: 0.39663, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:38:22,774 Epoch[53/120] train loss: 0.39254, val loss: nan, lr: 0.0010000, time: 4.96\n",
      "2022-06-08 18:38:27,709 Epoch[54/120] train loss: 0.39101, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:38:32,614 Epoch[55/120] train loss: 0.39349, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:38:37,552 Epoch[56/120] train loss: 0.39194, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:38:42,504 Epoch[57/120] train loss: 0.39050, val loss: nan, lr: 0.0010000, time: 4.95\n",
      "2022-06-08 18:38:47,422 Epoch[58/120] train loss: 0.39431, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:38:52,361 Epoch[59/120] train loss: 0.39997, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:38:57,228 Epoch[60/120] train loss: 0.39254, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:39:02,095 Epoch[61/120] train loss: 0.39250, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:39:07,028 Epoch[62/120] train loss: 0.39197, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:39:11,952 Epoch[63/120] train loss: 0.39006, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:39:16,832 Epoch[64/120] train loss: 0.39004, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:39:21,721 Epoch[65/120] train loss: 0.39012, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:39:26,632 Epoch[66/120] train loss: 0.38784, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:39:31,549 Epoch[67/120] train loss: 0.39037, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:39:36,451 Epoch[68/120] train loss: 0.39030, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:39:41,370 Epoch[69/120] train loss: 0.38981, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:39:46,268 Epoch[70/120] train loss: 0.38863, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:39:51,152 Epoch[71/120] train loss: 0.38729, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:39:56,072 Epoch[72/120] train loss: 0.38838, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:40:00,974 Epoch[73/120] train loss: 0.38920, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:40:05,906 Epoch[74/120] train loss: 0.38957, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:40:10,789 Epoch[75/120] train loss: 0.38890, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:40:15,718 Epoch[76/120] train loss: 0.39303, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:40:20,610 Epoch[77/120] train loss: 0.38715, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:40:25,513 Epoch[78/120] train loss: 0.38975, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:40:30,436 Epoch[79/120] train loss: 0.38666, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:40:35,274 Epoch[80/120] train loss: 0.38981, val loss: nan, lr: 0.0010000, time: 4.84\n",
      "2022-06-08 18:40:40,152 Epoch[81/120] train loss: 0.38654, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:40:45,056 Epoch[82/120] train loss: 0.38469, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:40:49,940 Epoch[83/120] train loss: 0.38484, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:40:54,804 Epoch[84/120] train loss: 0.38936, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:40:59,694 Epoch[85/120] train loss: 0.38738, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:41:04,594 Epoch[86/120] train loss: 0.38526, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:41:09,475 Epoch[87/120] train loss: 0.38768, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:41:14,385 Epoch[88/120] train loss: 0.38591, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:41:19,300 Epoch[89/120] train loss: 0.38738, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:41:24,218 Epoch[90/120] train loss: 0.38618, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:41:29,129 Epoch[91/120] train loss: 0.38615, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:41:34,023 Epoch[92/120] train loss: 0.38706, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:41:38,950 Epoch[93/120] train loss: 0.38513, val loss: nan, lr: 0.0010000, time: 4.93\n",
      "2022-06-08 18:41:43,847 Epoch[94/120] train loss: 0.38450, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:41:48,747 Epoch[95/120] train loss: 0.38395, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:41:53,637 Epoch[96/120] train loss: 0.38408, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:41:58,545 Epoch[97/120] train loss: 0.38489, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:42:03,440 Epoch[98/120] train loss: 0.38446, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:42:08,358 Epoch[99/120] train loss: 0.38459, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:42:13,297 Epoch[100/120] train loss: 0.38972, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:42:18,219 Epoch[101/120] train loss: 0.38539, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:42:23,164 Epoch[102/120] train loss: 0.38371, val loss: nan, lr: 0.0010000, time: 4.94\n",
      "2022-06-08 18:42:28,069 Epoch[103/120] train loss: 0.38304, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:42:32,969 Epoch[104/120] train loss: 0.38234, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:42:37,850 Epoch[105/120] train loss: 0.38276, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:42:42,733 Epoch[106/120] train loss: 0.38376, val loss: nan, lr: 0.0010000, time: 4.88\n",
      "2022-06-08 18:42:47,620 Epoch[107/120] train loss: 0.39184, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:42:52,525 Epoch[108/120] train loss: 0.38616, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:42:57,438 Epoch[109/120] train loss: 0.38347, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:43:02,307 Epoch[110/120] train loss: 0.38531, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:43:07,173 Epoch[111/120] train loss: 0.38406, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:43:12,081 Epoch[112/120] train loss: 0.38261, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:43:16,971 Epoch[113/120] train loss: 0.38472, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:43:21,832 Epoch[114/120] train loss: 0.38307, val loss: nan, lr: 0.0010000, time: 4.86\n",
      "2022-06-08 18:43:26,721 Epoch[115/120] train loss: 0.38229, val loss: nan, lr: 0.0010000, time: 4.89\n",
      "2022-06-08 18:43:31,620 Epoch[116/120] train loss: 0.38400, val loss: nan, lr: 0.0010000, time: 4.90\n",
      "2022-06-08 18:43:36,492 Epoch[117/120] train loss: 0.38324, val loss: nan, lr: 0.0010000, time: 4.87\n",
      "2022-06-08 18:43:41,414 Epoch[118/120] train loss: 0.38142, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:43:46,332 Epoch[119/120] train loss: 0.38235, val loss: nan, lr: 0.0010000, time: 4.92\n",
      "2022-06-08 18:43:51,245 Epoch[120/120] train loss: 0.39391, val loss: nan, lr: 0.0010000, time: 4.91\n",
      "2022-06-08 18:43:51,246 => end training\n",
      "2022-06-08 18:43:51,246 => calculating train scores\n",
      "2022-06-08 18:43:53,579 => train score\n",
      "accuracy: 0.9948713354684106\n",
      "presision: 0.5340268747290854\n",
      "recall: 0.946236559139785\n",
      "f1: 0.6827376004433361\n",
      "2022-06-08 18:43:53,580 => calculating test scores\n",
      "2022-06-08 18:45:02,180 => test score\n",
      "accuracy: 0.9936821642637291\n",
      "presision: 0.23273657289002558\n",
      "recall: 0.6408450704225352\n",
      "f1: 0.3414634146341463\n",
      "2022-06-08 18:45:02,228 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:45:04,416 Epoch[1/120] train loss: 0.59926, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:45:06,593 Epoch[2/120] train loss: 0.50883, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:45:08,727 Epoch[3/120] train loss: 0.46485, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:45:10,884 Epoch[4/120] train loss: 0.43693, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:13,012 Epoch[5/120] train loss: 0.41808, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:45:15,209 Epoch[6/120] train loss: 0.40572, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:45:17,373 Epoch[7/120] train loss: 0.39742, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:19,531 Epoch[8/120] train loss: 0.39139, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:21,696 Epoch[9/120] train loss: 0.38717, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:23,855 Epoch[10/120] train loss: 0.38417, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:26,023 Epoch[11/120] train loss: 0.38035, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:45:28,216 Epoch[12/120] train loss: 0.37868, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:45:30,373 Epoch[13/120] train loss: 0.37754, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:32,521 Epoch[14/120] train loss: 0.37641, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:45:34,668 Epoch[15/120] train loss: 0.37621, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:45:36,798 Epoch[16/120] train loss: 0.37434, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:45:38,941 Epoch[17/120] train loss: 0.37364, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:45:41,102 Epoch[18/120] train loss: 0.37279, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:43,277 Epoch[19/120] train loss: 0.37214, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:45:45,451 Epoch[20/120] train loss: 0.37153, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:45:47,617 Epoch[21/120] train loss: 0.37049, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:45:49,782 Epoch[22/120] train loss: 0.37013, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:51,940 Epoch[23/120] train loss: 0.36819, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:54,132 Epoch[24/120] train loss: 0.36799, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:45:56,290 Epoch[25/120] train loss: 0.36773, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:45:58,470 Epoch[26/120] train loss: 0.36790, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:46:00,634 Epoch[27/120] train loss: 0.36652, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:46:02,816 Epoch[28/120] train loss: 0.36735, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:46:04,985 Epoch[29/120] train loss: 0.36617, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:46:07,169 Epoch[30/120] train loss: 0.36661, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:46:09,336 Epoch[31/120] train loss: 0.36453, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:46:11,505 Epoch[32/120] train loss: 0.36595, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:46:13,682 Epoch[33/120] train loss: 0.36479, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:46:15,843 Epoch[34/120] train loss: 0.36553, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:46:18,016 Epoch[35/120] train loss: 0.36534, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:46:20,185 Epoch[36/120] train loss: 0.36321, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:46:22,309 Epoch[37/120] train loss: 0.36308, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:46:24,451 Epoch[38/120] train loss: 0.36353, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:46:26,569 Epoch[39/120] train loss: 0.36278, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:46:28,727 Epoch[40/120] train loss: 0.36203, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:46:30,880 Epoch[41/120] train loss: 0.36189, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:46:33,035 Epoch[42/120] train loss: 0.36254, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:46:35,183 Epoch[43/120] train loss: 0.36198, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:46:37,338 Epoch[44/120] train loss: 0.36253, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:46:39,454 Epoch[45/120] train loss: 0.36312, val loss: nan, lr: 0.0010000, time: 2.11\n",
      "2022-06-08 18:46:41,568 Epoch[46/120] train loss: 0.36370, val loss: nan, lr: 0.0010000, time: 2.11\n",
      "2022-06-08 18:46:43,703 Epoch[47/120] train loss: 0.36175, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:46:45,853 Epoch[48/120] train loss: 0.36094, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:46:48,015 Epoch[49/120] train loss: 0.36122, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:46:50,146 Epoch[50/120] train loss: 0.36058, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:46:52,308 Epoch[51/120] train loss: 0.35970, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:46:54,436 Epoch[52/120] train loss: 0.35885, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:46:56,580 Epoch[53/120] train loss: 0.36038, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:46:58,704 Epoch[54/120] train loss: 0.36000, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:00,865 Epoch[55/120] train loss: 0.35999, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:47:02,994 Epoch[56/120] train loss: 0.35816, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:05,134 Epoch[57/120] train loss: 0.35986, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:47:07,270 Epoch[58/120] train loss: 0.35862, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:09,394 Epoch[59/120] train loss: 0.36020, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:11,511 Epoch[60/120] train loss: 0.35905, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:13,658 Epoch[61/120] train loss: 0.35904, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:47:15,829 Epoch[62/120] train loss: 0.35763, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:47:18,001 Epoch[63/120] train loss: 0.35842, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:47:20,136 Epoch[64/120] train loss: 0.35859, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:22,295 Epoch[65/120] train loss: 0.35719, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:47:24,459 Epoch[66/120] train loss: 0.35751, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:47:26,620 Epoch[67/120] train loss: 0.35776, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:47:28,754 Epoch[68/120] train loss: 0.35740, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:30,884 Epoch[69/120] train loss: 0.36040, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:33,003 Epoch[70/120] train loss: 0.35683, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:35,116 Epoch[71/120] train loss: 0.35713, val loss: nan, lr: 0.0010000, time: 2.11\n",
      "2022-06-08 18:47:37,286 Epoch[72/120] train loss: 0.35598, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:47:39,441 Epoch[73/120] train loss: 0.35770, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:47:41,611 Epoch[74/120] train loss: 0.35725, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:47:43,775 Epoch[75/120] train loss: 0.35747, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:47:45,895 Epoch[76/120] train loss: 0.35643, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:48,020 Epoch[77/120] train loss: 0.35802, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:50,151 Epoch[78/120] train loss: 0.35741, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:52,279 Epoch[79/120] train loss: 0.35657, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:47:54,429 Epoch[80/120] train loss: 0.35579, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:47:56,552 Epoch[81/120] train loss: 0.35608, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:47:58,682 Epoch[82/120] train loss: 0.35598, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:48:00,838 Epoch[83/120] train loss: 0.35517, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:48:03,002 Epoch[84/120] train loss: 0.35797, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:48:05,107 Epoch[85/120] train loss: 0.35669, val loss: nan, lr: 0.0010000, time: 2.10\n",
      "2022-06-08 18:48:07,261 Epoch[86/120] train loss: 0.35554, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:48:09,399 Epoch[87/120] train loss: 0.35811, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:11,529 Epoch[88/120] train loss: 0.35627, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:48:13,708 Epoch[89/120] train loss: 0.35584, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:48:15,845 Epoch[90/120] train loss: 0.35750, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:17,981 Epoch[91/120] train loss: 0.35769, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:20,120 Epoch[92/120] train loss: 0.35580, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:22,255 Epoch[93/120] train loss: 0.35533, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:48:24,403 Epoch[94/120] train loss: 0.35508, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:48:26,560 Epoch[95/120] train loss: 0.35792, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:48:28,715 Epoch[96/120] train loss: 0.35689, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:48:30,858 Epoch[97/120] train loss: 0.35721, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:33,286 Epoch[98/120] train loss: 0.35680, val loss: nan, lr: 0.0010000, time: 2.43\n",
      "2022-06-08 18:48:35,431 Epoch[99/120] train loss: 0.35497, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:48:37,643 Epoch[100/120] train loss: 0.35584, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:48:39,857 Epoch[101/120] train loss: 0.35515, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:48:42,041 Epoch[102/120] train loss: 0.35465, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:48:44,216 Epoch[103/120] train loss: 0.35459, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:48:46,403 Epoch[104/120] train loss: 0.35446, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:48:48,585 Epoch[105/120] train loss: 0.35539, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:48:50,792 Epoch[106/120] train loss: 0.35522, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:48:52,972 Epoch[107/120] train loss: 0.35854, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:48:55,141 Epoch[108/120] train loss: 0.35471, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:48:57,315 Epoch[109/120] train loss: 0.35429, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:48:59,481 Epoch[110/120] train loss: 0.35738, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:49:01,626 Epoch[111/120] train loss: 0.35719, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:49:03,774 Epoch[112/120] train loss: 0.35635, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:49:05,951 Epoch[113/120] train loss: 0.35589, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:49:08,106 Epoch[114/120] train loss: 0.35522, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:49:10,282 Epoch[115/120] train loss: 0.35435, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:49:12,485 Epoch[116/120] train loss: 0.35445, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:49:14,648 Epoch[117/120] train loss: 0.35439, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:49:16,825 Epoch[118/120] train loss: 0.35739, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:49:19,012 Epoch[119/120] train loss: 0.35416, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:49:21,203 Epoch[120/120] train loss: 0.35375, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:49:21,204 => end training\n",
      "2022-06-08 18:49:21,204 => calculating train scores\n",
      "2022-06-08 18:49:23,519 => train score\n",
      "accuracy: 0.9987727038588161\n",
      "presision: 0.8265565438373571\n",
      "recall: 0.999231950844854\n",
      "f1: 0.9047287899860917\n",
      "2022-06-08 18:49:23,520 => calculating test scores\n",
      "2022-06-08 18:50:45,259 => test score\n",
      "accuracy: 0.9953741202728729\n",
      "presision: 0.26907630522088355\n",
      "recall: 0.47183098591549294\n",
      "f1: 0.3427109974424552\n",
      "2022-06-08 18:50:45,282 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:50:47,599 Epoch[1/120] train loss: 0.65150, val loss: nan, lr: 0.0010000, time: 2.32\n",
      "2022-06-08 18:50:49,780 Epoch[2/120] train loss: 0.54940, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:50:51,976 Epoch[3/120] train loss: 0.48615, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:50:54,108 Epoch[4/120] train loss: 0.44762, val loss: nan, lr: 0.0010000, time: 2.13\n",
      "2022-06-08 18:50:56,255 Epoch[5/120] train loss: 0.42620, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:50:58,404 Epoch[6/120] train loss: 0.41406, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:51:00,591 Epoch[7/120] train loss: 0.40617, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:02,783 Epoch[8/120] train loss: 0.40113, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:04,940 Epoch[9/120] train loss: 0.39814, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:51:07,120 Epoch[10/120] train loss: 0.39555, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:09,375 Epoch[11/120] train loss: 0.39243, val loss: nan, lr: 0.0010000, time: 2.25\n",
      "2022-06-08 18:51:11,554 Epoch[12/120] train loss: 0.38914, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:13,733 Epoch[13/120] train loss: 0.39287, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:15,926 Epoch[14/120] train loss: 0.38775, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:18,095 Epoch[15/120] train loss: 0.38600, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:51:20,283 Epoch[16/120] train loss: 0.38570, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:22,483 Epoch[17/120] train loss: 0.38435, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:51:24,698 Epoch[18/120] train loss: 0.38442, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:51:26,874 Epoch[19/120] train loss: 0.38778, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:29,048 Epoch[20/120] train loss: 0.38378, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:51:31,215 Epoch[21/120] train loss: 0.38318, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:51:33,405 Epoch[22/120] train loss: 0.38588, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:35,583 Epoch[23/120] train loss: 0.38274, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:37,781 Epoch[24/120] train loss: 0.38080, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:51:39,930 Epoch[25/120] train loss: 0.38065, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:51:42,135 Epoch[26/120] train loss: 0.37970, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:51:44,321 Epoch[27/120] train loss: 0.37877, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:46,504 Epoch[28/120] train loss: 0.37699, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:48,682 Epoch[29/120] train loss: 0.37781, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:51:50,853 Epoch[30/120] train loss: 0.37692, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:51:53,020 Epoch[31/120] train loss: 0.37884, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:51:55,229 Epoch[32/120] train loss: 0.37692, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:51:57,418 Epoch[33/120] train loss: 0.37537, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:51:59,586 Epoch[34/120] train loss: 0.37843, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:52:01,736 Epoch[35/120] train loss: 0.37746, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:52:03,924 Epoch[36/120] train loss: 0.37469, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:06,104 Epoch[37/120] train loss: 0.37359, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:08,263 Epoch[38/120] train loss: 0.37451, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:52:10,469 Epoch[39/120] train loss: 0.37475, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:52:12,634 Epoch[40/120] train loss: 0.37684, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:52:14,814 Epoch[41/120] train loss: 0.37300, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:16,988 Epoch[42/120] train loss: 0.37322, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:52:19,156 Epoch[43/120] train loss: 0.37313, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:52:21,347 Epoch[44/120] train loss: 0.37337, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:23,492 Epoch[45/120] train loss: 0.37607, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:52:25,667 Epoch[46/120] train loss: 0.37108, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:52:27,858 Epoch[47/120] train loss: 0.36977, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:30,049 Epoch[48/120] train loss: 0.38088, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:32,233 Epoch[49/120] train loss: 0.37528, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:34,423 Epoch[50/120] train loss: 0.37195, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:36,595 Epoch[51/120] train loss: 0.36954, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:52:38,774 Epoch[52/120] train loss: 0.37031, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:40,922 Epoch[53/120] train loss: 0.37672, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:52:43,085 Epoch[54/120] train loss: 0.37512, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:52:45,268 Epoch[55/120] train loss: 0.37129, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:47,462 Epoch[56/120] train loss: 0.37116, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:49,646 Epoch[57/120] train loss: 0.36961, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:52:51,832 Epoch[58/120] train loss: 0.36935, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:52:53,979 Epoch[59/120] train loss: 0.36887, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:52:56,144 Epoch[60/120] train loss: 0.37213, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:52:58,304 Epoch[61/120] train loss: 0.36873, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:53:00,474 Epoch[62/120] train loss: 0.36736, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:53:02,655 Epoch[63/120] train loss: 0.36800, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:53:04,827 Epoch[64/120] train loss: 0.36965, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:53:07,023 Epoch[65/120] train loss: 0.36714, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:53:09,231 Epoch[66/120] train loss: 0.37099, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:53:11,383 Epoch[67/120] train loss: 0.36741, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:53:13,541 Epoch[68/120] train loss: 0.36676, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:53:15,715 Epoch[69/120] train loss: 0.37481, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:53:17,862 Epoch[70/120] train loss: 0.36774, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:53:20,027 Epoch[71/120] train loss: 0.36722, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:53:22,179 Epoch[72/120] train loss: 0.36869, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:53:24,382 Epoch[73/120] train loss: 0.36686, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:53:26,563 Epoch[74/120] train loss: 0.36625, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:53:28,720 Epoch[75/120] train loss: 0.36778, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:53:30,916 Epoch[76/120] train loss: 0.36791, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:53:33,069 Epoch[77/120] train loss: 0.36734, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:53:35,277 Epoch[78/120] train loss: 0.36608, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:53:37,491 Epoch[79/120] train loss: 0.36535, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:53:39,681 Epoch[80/120] train loss: 0.36584, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:53:41,857 Epoch[81/120] train loss: 0.36488, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:53:44,048 Epoch[82/120] train loss: 0.36752, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:53:46,231 Epoch[83/120] train loss: 0.36578, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:53:48,444 Epoch[84/120] train loss: 0.36661, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:53:50,605 Epoch[85/120] train loss: 0.36629, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:53:52,835 Epoch[86/120] train loss: 0.36692, val loss: nan, lr: 0.0010000, time: 2.23\n",
      "2022-06-08 18:53:55,046 Epoch[87/120] train loss: 0.36569, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:53:57,235 Epoch[88/120] train loss: 0.36782, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:53:59,421 Epoch[89/120] train loss: 0.36597, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:01,644 Epoch[90/120] train loss: 0.36800, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:54:03,822 Epoch[91/120] train loss: 0.36824, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:06,028 Epoch[92/120] train loss: 0.36607, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:54:08,202 Epoch[93/120] train loss: 0.36511, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:10,378 Epoch[94/120] train loss: 0.36656, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:12,560 Epoch[95/120] train loss: 0.36515, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:14,755 Epoch[96/120] train loss: 0.36397, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:54:16,924 Epoch[97/120] train loss: 0.37093, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:19,116 Epoch[98/120] train loss: 0.36474, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:54:21,289 Epoch[99/120] train loss: 0.36365, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:23,446 Epoch[100/120] train loss: 0.36322, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:54:25,643 Epoch[101/120] train loss: 0.36591, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:54:27,848 Epoch[102/120] train loss: 0.36477, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:54:30,028 Epoch[103/120] train loss: 0.36390, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:32,210 Epoch[104/120] train loss: 0.36533, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:34,404 Epoch[105/120] train loss: 0.36537, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:54:36,570 Epoch[106/120] train loss: 0.36515, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:54:38,757 Epoch[107/120] train loss: 0.36420, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:54:40,956 Epoch[108/120] train loss: 0.36430, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:54:43,166 Epoch[109/120] train loss: 0.36355, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:54:45,340 Epoch[110/120] train loss: 0.36489, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:47,545 Epoch[111/120] train loss: 0.36483, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:54:49,745 Epoch[112/120] train loss: 0.36424, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:54:51,916 Epoch[113/120] train loss: 0.36407, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:54:54,094 Epoch[114/120] train loss: 0.36574, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:54:56,282 Epoch[115/120] train loss: 0.36802, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:54:58,467 Epoch[116/120] train loss: 0.36395, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:55:00,706 Epoch[117/120] train loss: 0.36259, val loss: nan, lr: 0.0010000, time: 2.24\n",
      "2022-06-08 18:55:02,912 Epoch[118/120] train loss: 0.36313, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:55:05,091 Epoch[119/120] train loss: 0.36415, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:55:07,269 Epoch[120/120] train loss: 0.36385, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:55:07,270 => end training\n",
      "2022-06-08 18:55:07,270 => calculating train scores\n",
      "2022-06-08 18:55:09,511 => train score\n",
      "accuracy: 0.9979574925533583\n",
      "presision: 0.7529904306220095\n",
      "recall: 0.966973886328725\n",
      "f1: 0.8466711499663752\n",
      "2022-06-08 18:55:09,512 => calculating test scores\n",
      "2022-06-08 18:56:31,546 => test score\n",
      "accuracy: 0.9954461184009216\n",
      "presision: 0.23696682464454977\n",
      "recall: 0.352112676056338\n",
      "f1: 0.28328611898017\n",
      "2022-06-08 18:56:31,571 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 18:56:33,788 Epoch[1/120] train loss: 0.79522, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:56:35,971 Epoch[2/120] train loss: 0.64593, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:38,125 Epoch[3/120] train loss: 0.57994, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:56:40,306 Epoch[4/120] train loss: 0.53139, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:42,489 Epoch[5/120] train loss: 0.49569, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:44,651 Epoch[6/120] train loss: 0.47479, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:56:46,792 Epoch[7/120] train loss: 0.45800, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:56:48,968 Epoch[8/120] train loss: 0.44615, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:51,185 Epoch[9/120] train loss: 0.43972, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:56:53,401 Epoch[10/120] train loss: 0.43386, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:56:55,582 Epoch[11/120] train loss: 0.42803, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:57,767 Epoch[12/120] train loss: 0.42677, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:56:59,955 Epoch[13/120] train loss: 0.42185, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:57:02,103 Epoch[14/120] train loss: 0.41873, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:57:04,285 Epoch[15/120] train loss: 0.41865, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:57:06,445 Epoch[16/120] train loss: 0.41530, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:57:08,615 Epoch[17/120] train loss: 0.41269, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:10,798 Epoch[18/120] train loss: 0.41185, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:57:12,963 Epoch[19/120] train loss: 0.41434, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:57:15,135 Epoch[20/120] train loss: 0.41117, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:17,301 Epoch[21/120] train loss: 0.40651, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:19,478 Epoch[22/120] train loss: 0.40884, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:57:21,629 Epoch[23/120] train loss: 0.40707, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:57:23,817 Epoch[24/120] train loss: 0.40720, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:57:26,006 Epoch[25/120] train loss: 0.40458, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:57:28,203 Epoch[26/120] train loss: 0.40268, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:57:30,373 Epoch[27/120] train loss: 0.40579, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:32,562 Epoch[28/120] train loss: 0.40163, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:57:34,798 Epoch[29/120] train loss: 0.40449, val loss: nan, lr: 0.0010000, time: 2.23\n",
      "2022-06-08 18:57:36,922 Epoch[30/120] train loss: 0.39917, val loss: nan, lr: 0.0010000, time: 2.12\n",
      "2022-06-08 18:57:39,120 Epoch[31/120] train loss: 0.39819, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:57:41,284 Epoch[32/120] train loss: 0.39826, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:57:43,485 Epoch[33/120] train loss: 0.40371, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:57:45,688 Epoch[34/120] train loss: 0.39982, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:57:47,855 Epoch[35/120] train loss: 0.39715, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:50,027 Epoch[36/120] train loss: 0.39530, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:57:52,225 Epoch[37/120] train loss: 0.39616, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:57:54,389 Epoch[38/120] train loss: 0.39496, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:57:56,532 Epoch[39/120] train loss: 0.39725, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:57:58,708 Epoch[40/120] train loss: 0.40162, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:00,913 Epoch[41/120] train loss: 0.39659, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:03,070 Epoch[42/120] train loss: 0.39643, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:05,232 Epoch[43/120] train loss: 0.39386, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:07,410 Epoch[44/120] train loss: 0.39070, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:58:09,555 Epoch[45/120] train loss: 0.39325, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:58:11,759 Epoch[46/120] train loss: 0.39434, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:13,977 Epoch[47/120] train loss: 0.39306, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:58:16,140 Epoch[48/120] train loss: 0.39420, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:18,309 Epoch[49/120] train loss: 0.39395, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:20,481 Epoch[50/120] train loss: 0.38828, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:22,649 Epoch[51/120] train loss: 0.39117, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:24,848 Epoch[52/120] train loss: 0.39647, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:27,035 Epoch[53/120] train loss: 0.38959, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:58:29,239 Epoch[54/120] train loss: 0.39141, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:31,431 Epoch[55/120] train loss: 0.38697, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:58:33,629 Epoch[56/120] train loss: 0.38812, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:35,803 Epoch[57/120] train loss: 0.38954, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:37,999 Epoch[58/120] train loss: 0.38743, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:40,172 Epoch[59/120] train loss: 0.39323, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:42,380 Epoch[60/120] train loss: 0.38894, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:58:44,579 Epoch[61/120] train loss: 0.39083, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:58:46,743 Epoch[62/120] train loss: 0.38837, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:48,963 Epoch[63/120] train loss: 0.39294, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:58:51,127 Epoch[64/120] train loss: 0.38758, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:53,296 Epoch[65/120] train loss: 0.39252, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:55,470 Epoch[66/120] train loss: 0.39248, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:58:57,627 Epoch[67/120] train loss: 0.38854, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:58:59,785 Epoch[68/120] train loss: 0.39416, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:59:01,984 Epoch[69/120] train loss: 0.38635, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:59:04,169 Epoch[70/120] train loss: 0.38834, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:06,308 Epoch[71/120] train loss: 0.38723, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 18:59:08,504 Epoch[72/120] train loss: 0.38574, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:59:10,671 Epoch[73/120] train loss: 0.38549, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:59:12,868 Epoch[74/120] train loss: 0.38509, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:59:15,037 Epoch[75/120] train loss: 0.39175, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:59:17,230 Epoch[76/120] train loss: 0.39129, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:59:19,388 Epoch[77/120] train loss: 0.38563, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:59:21,582 Epoch[78/120] train loss: 0.38923, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 18:59:23,798 Epoch[79/120] train loss: 0.38499, val loss: nan, lr: 0.0010000, time: 2.22\n",
      "2022-06-08 18:59:25,978 Epoch[80/120] train loss: 0.38416, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:28,143 Epoch[81/120] train loss: 0.38955, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 18:59:30,321 Epoch[82/120] train loss: 0.38739, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:32,523 Epoch[83/120] train loss: 0.38443, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 18:59:34,701 Epoch[84/120] train loss: 0.39226, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:36,880 Epoch[85/120] train loss: 0.40007, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:39,033 Epoch[86/120] train loss: 0.39475, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:59:41,219 Epoch[87/120] train loss: 0.38915, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 18:59:43,388 Epoch[88/120] train loss: 0.39154, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:59:45,560 Epoch[89/120] train loss: 0.38590, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:59:47,733 Epoch[90/120] train loss: 0.38610, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 18:59:49,947 Epoch[91/120] train loss: 0.38596, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:59:52,161 Epoch[92/120] train loss: 0.38554, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:59:54,315 Epoch[93/120] train loss: 0.38460, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 18:59:56,524 Epoch[94/120] train loss: 0.38885, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 18:59:58,687 Epoch[95/120] train loss: 0.38430, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 19:00:00,833 Epoch[96/120] train loss: 0.38188, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 19:00:03,034 Epoch[97/120] train loss: 0.39030, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 19:00:05,192 Epoch[98/120] train loss: 0.38250, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 19:00:07,380 Epoch[99/120] train loss: 0.38160, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 19:00:09,592 Epoch[100/120] train loss: 0.38570, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 19:00:11,791 Epoch[101/120] train loss: 0.38832, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 19:00:13,971 Epoch[102/120] train loss: 0.38309, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 19:00:16,147 Epoch[103/120] train loss: 0.38736, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 19:00:18,342 Epoch[104/120] train loss: 0.38659, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 19:00:20,501 Epoch[105/120] train loss: 0.38182, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 19:00:22,693 Epoch[106/120] train loss: 0.38249, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 19:00:24,870 Epoch[107/120] train loss: 0.38565, val loss: nan, lr: 0.0010000, time: 2.18\n",
      "2022-06-08 19:00:27,075 Epoch[108/120] train loss: 0.38231, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 19:00:29,241 Epoch[109/120] train loss: 0.38349, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 19:00:31,449 Epoch[110/120] train loss: 0.38477, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 19:00:33,740 Epoch[111/120] train loss: 0.38174, val loss: nan, lr: 0.0010000, time: 2.29\n",
      "2022-06-08 19:00:35,934 Epoch[112/120] train loss: 0.38207, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 19:00:38,103 Epoch[113/120] train loss: 0.38566, val loss: nan, lr: 0.0010000, time: 2.17\n",
      "2022-06-08 19:00:40,301 Epoch[114/120] train loss: 0.38542, val loss: nan, lr: 0.0010000, time: 2.20\n",
      "2022-06-08 19:00:42,490 Epoch[115/120] train loss: 0.38365, val loss: nan, lr: 0.0010000, time: 2.19\n",
      "2022-06-08 19:00:44,633 Epoch[116/120] train loss: 0.38075, val loss: nan, lr: 0.0010000, time: 2.14\n",
      "2022-06-08 19:00:46,787 Epoch[117/120] train loss: 0.38029, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 19:00:48,951 Epoch[118/120] train loss: 0.38215, val loss: nan, lr: 0.0010000, time: 2.16\n",
      "2022-06-08 19:00:51,166 Epoch[119/120] train loss: 0.38233, val loss: nan, lr: 0.0010000, time: 2.21\n",
      "2022-06-08 19:00:53,321 Epoch[120/120] train loss: 0.38237, val loss: nan, lr: 0.0010000, time: 2.15\n",
      "2022-06-08 19:00:53,321 => end training\n",
      "2022-06-08 19:00:53,322 => calculating train scores\n",
      "2022-06-08 19:00:55,651 => train score\n",
      "accuracy: 0.998118743141251\n",
      "presision: 0.7653429602888087\n",
      "recall: 0.9769585253456221\n",
      "f1: 0.8582995951417004\n",
      "2022-06-08 19:00:55,652 => calculating test scores\n",
      "2022-06-08 19:02:17,661 => test score\n",
      "accuracy: 0.9952301240167756\n",
      "presision: 0.27472527472527475\n",
      "recall: 0.528169014084507\n",
      "f1: 0.3614457831325301\n",
      "2022-06-08 19:02:17,689 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:02:22,089 Epoch[1/120] train loss: 0.69718, val loss: nan, lr: 0.0010000, time: 4.40\n",
      "2022-06-08 19:02:26,290 Epoch[2/120] train loss: 0.58475, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:02:30,447 Epoch[3/120] train loss: 0.50856, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:02:34,650 Epoch[4/120] train loss: 0.45912, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:02:38,820 Epoch[5/120] train loss: 0.42976, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:02:42,959 Epoch[6/120] train loss: 0.41089, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:02:47,165 Epoch[7/120] train loss: 0.39850, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:02:51,355 Epoch[8/120] train loss: 0.39243, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:02:55,513 Epoch[9/120] train loss: 0.38701, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:02:59,701 Epoch[10/120] train loss: 0.38350, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:03:03,920 Epoch[11/120] train loss: 0.38002, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:03:08,163 Epoch[12/120] train loss: 0.37862, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:03:12,353 Epoch[13/120] train loss: 0.37762, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:03:16,552 Epoch[14/120] train loss: 0.37603, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:03:20,736 Epoch[15/120] train loss: 0.37382, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:03:24,929 Epoch[16/120] train loss: 0.37540, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:03:29,106 Epoch[17/120] train loss: 0.37668, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:03:33,335 Epoch[18/120] train loss: 0.37362, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:03:37,512 Epoch[19/120] train loss: 0.37161, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:03:41,792 Epoch[20/120] train loss: 0.37304, val loss: nan, lr: 0.0010000, time: 4.28\n",
      "2022-06-08 19:03:46,033 Epoch[21/120] train loss: 0.37967, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:03:50,187 Epoch[22/120] train loss: 0.37407, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:03:54,397 Epoch[23/120] train loss: 0.37258, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:03:58,587 Epoch[24/120] train loss: 0.37117, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:04:02,765 Epoch[25/120] train loss: 0.37387, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:04:06,912 Epoch[26/120] train loss: 0.37123, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:04:11,068 Epoch[27/120] train loss: 0.37040, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:04:15,249 Epoch[28/120] train loss: 0.36919, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:04:19,457 Epoch[29/120] train loss: 0.36991, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:04:23,655 Epoch[30/120] train loss: 0.36848, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:04:27,824 Epoch[31/120] train loss: 0.36890, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:04:32,002 Epoch[32/120] train loss: 0.36872, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:04:36,155 Epoch[33/120] train loss: 0.36755, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:04:40,353 Epoch[34/120] train loss: 0.36981, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:04:44,573 Epoch[35/120] train loss: 0.36708, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:04:48,761 Epoch[36/120] train loss: 0.36785, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:04:52,969 Epoch[37/120] train loss: 0.36686, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:04:57,209 Epoch[38/120] train loss: 0.37014, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:05:01,418 Epoch[39/120] train loss: 0.36592, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:05:05,632 Epoch[40/120] train loss: 0.36764, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:05:09,822 Epoch[41/120] train loss: 0.36524, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:05:14,003 Epoch[42/120] train loss: 0.36624, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:05:18,183 Epoch[43/120] train loss: 0.36506, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:05:22,322 Epoch[44/120] train loss: 0.36488, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:05:26,491 Epoch[45/120] train loss: 0.36331, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:05:30,685 Epoch[46/120] train loss: 0.36489, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:05:34,848 Epoch[47/120] train loss: 0.36381, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:05:39,064 Epoch[48/120] train loss: 0.36291, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:05:43,237 Epoch[49/120] train loss: 0.36528, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:05:47,403 Epoch[50/120] train loss: 0.36608, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:05:51,530 Epoch[51/120] train loss: 0.37033, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:05:55,694 Epoch[52/120] train loss: 0.36473, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:05:59,853 Epoch[53/120] train loss: 0.36553, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:06:04,022 Epoch[54/120] train loss: 0.36466, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:06:08,189 Epoch[55/120] train loss: 0.36319, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:06:12,354 Epoch[56/120] train loss: 0.36396, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:06:16,551 Epoch[57/120] train loss: 0.36271, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:06:20,700 Epoch[58/120] train loss: 0.36409, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:06:24,891 Epoch[59/120] train loss: 0.36319, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:06:29,060 Epoch[60/120] train loss: 0.36959, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:06:33,250 Epoch[61/120] train loss: 0.36626, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:06:37,420 Epoch[62/120] train loss: 0.36565, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:06:41,634 Epoch[63/120] train loss: 0.36395, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:06:45,779 Epoch[64/120] train loss: 0.36526, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:06:50,006 Epoch[65/120] train loss: 0.36160, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:06:54,183 Epoch[66/120] train loss: 0.36041, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:06:58,387 Epoch[67/120] train loss: 0.36021, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:07:02,567 Epoch[68/120] train loss: 0.36105, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:07:06,714 Epoch[69/120] train loss: 0.35974, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:07:10,898 Epoch[70/120] train loss: 0.35944, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:07:15,119 Epoch[71/120] train loss: 0.36122, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:07:19,275 Epoch[72/120] train loss: 0.36005, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:07:23,432 Epoch[73/120] train loss: 0.36708, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:07:27,629 Epoch[74/120] train loss: 0.36230, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:07:31,804 Epoch[75/120] train loss: 0.36129, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:07:36,019 Epoch[76/120] train loss: 0.36056, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:07:40,220 Epoch[77/120] train loss: 0.36021, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:07:44,380 Epoch[78/120] train loss: 0.36010, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:07:48,564 Epoch[79/120] train loss: 0.36028, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:07:52,757 Epoch[80/120] train loss: 0.35913, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:07:56,959 Epoch[81/120] train loss: 0.35819, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:08:01,146 Epoch[82/120] train loss: 0.36494, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:08:05,341 Epoch[83/120] train loss: 0.37793, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:08:09,546 Epoch[84/120] train loss: 0.36922, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:08:13,698 Epoch[85/120] train loss: 0.36531, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:08:17,895 Epoch[86/120] train loss: 0.36370, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:08:22,081 Epoch[87/120] train loss: 0.36265, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:08:26,303 Epoch[88/120] train loss: 0.36196, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:08:30,479 Epoch[89/120] train loss: 0.36064, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:08:34,660 Epoch[90/120] train loss: 0.36098, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:08:38,842 Epoch[91/120] train loss: 0.36021, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:08:42,964 Epoch[92/120] train loss: 0.35949, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:08:47,124 Epoch[93/120] train loss: 0.35904, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:08:51,335 Epoch[94/120] train loss: 0.35902, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:08:55,595 Epoch[95/120] train loss: 0.35777, val loss: nan, lr: 0.0010000, time: 4.26\n",
      "2022-06-08 19:08:59,797 Epoch[96/120] train loss: 0.35861, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:09:03,988 Epoch[97/120] train loss: 0.35801, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:09:08,205 Epoch[98/120] train loss: 0.35876, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:09:12,347 Epoch[99/120] train loss: 0.35690, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:09:16,546 Epoch[100/120] train loss: 0.35831, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:09:20,773 Epoch[101/120] train loss: 0.35644, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:09:24,925 Epoch[102/120] train loss: 0.36126, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:09:29,097 Epoch[103/120] train loss: 0.35843, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:09:33,265 Epoch[104/120] train loss: 0.35689, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:09:37,426 Epoch[105/120] train loss: 0.35754, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:09:41,566 Epoch[106/120] train loss: 0.35598, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:09:45,758 Epoch[107/120] train loss: 0.35739, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:09:49,936 Epoch[108/120] train loss: 0.35578, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:09:54,095 Epoch[109/120] train loss: 0.35678, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:09:58,239 Epoch[110/120] train loss: 0.35945, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:10:02,415 Epoch[111/120] train loss: 0.35676, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:10:06,619 Epoch[112/120] train loss: 0.35808, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:10:10,783 Epoch[113/120] train loss: 0.35617, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:10:14,987 Epoch[114/120] train loss: 0.35521, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:10:19,197 Epoch[115/120] train loss: 0.35535, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:10:23,346 Epoch[116/120] train loss: 0.35598, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:10:27,515 Epoch[117/120] train loss: 0.35513, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:10:31,707 Epoch[118/120] train loss: 0.35568, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:10:35,909 Epoch[119/120] train loss: 0.35721, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:10:40,121 Epoch[120/120] train loss: 0.35541, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:10:40,121 => end training\n",
      "2022-06-08 19:10:40,122 => calculating train scores\n",
      "2022-06-08 19:10:42,427 => train score\n",
      "accuracy: 0.9991220801325839\n",
      "presision: 0.8984149855907781\n",
      "recall: 0.9577572964669739\n",
      "f1: 0.9271375464684015\n",
      "2022-06-08 19:10:42,428 => calculating test scores\n",
      "2022-06-08 19:12:03,709 => test score\n",
      "accuracy: 0.9975160645823209\n",
      "presision: 0.5136986301369864\n",
      "recall: 0.528169014084507\n",
      "f1: 0.5208333333333333\n",
      "2022-06-08 19:12:03,748 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:12:08,156 Epoch[1/120] train loss: 0.69407, val loss: nan, lr: 0.0010000, time: 4.41\n",
      "2022-06-08 19:12:12,329 Epoch[2/120] train loss: 0.58503, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:12:16,455 Epoch[3/120] train loss: 0.52334, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:12:20,627 Epoch[4/120] train loss: 0.47841, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:12:24,802 Epoch[5/120] train loss: 0.45569, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:12:28,936 Epoch[6/120] train loss: 0.45223, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:12:33,080 Epoch[7/120] train loss: 0.43030, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:12:37,238 Epoch[8/120] train loss: 0.42108, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:12:41,399 Epoch[9/120] train loss: 0.41451, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:12:45,566 Epoch[10/120] train loss: 0.40786, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:12:49,709 Epoch[11/120] train loss: 0.40233, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:12:53,921 Epoch[12/120] train loss: 0.40318, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:12:58,079 Epoch[13/120] train loss: 0.39893, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:13:02,262 Epoch[14/120] train loss: 0.39671, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:13:06,424 Epoch[15/120] train loss: 0.39732, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:13:10,628 Epoch[16/120] train loss: 0.39472, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:13:14,786 Epoch[17/120] train loss: 0.39727, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:13:18,970 Epoch[18/120] train loss: 0.39131, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:13:23,179 Epoch[19/120] train loss: 0.39326, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:13:27,375 Epoch[20/120] train loss: 0.39024, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:13:31,578 Epoch[21/120] train loss: 0.39130, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:13:35,749 Epoch[22/120] train loss: 0.38796, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:13:39,924 Epoch[23/120] train loss: 0.38711, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:13:44,108 Epoch[24/120] train loss: 0.39097, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:13:48,283 Epoch[25/120] train loss: 0.39368, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:13:52,434 Epoch[26/120] train loss: 0.38817, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:13:56,595 Epoch[27/120] train loss: 0.38592, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:14:00,789 Epoch[28/120] train loss: 0.38782, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:14:04,919 Epoch[29/120] train loss: 0.38483, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:14:09,076 Epoch[30/120] train loss: 0.38615, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:14:13,236 Epoch[31/120] train loss: 0.38588, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:14:17,404 Epoch[32/120] train loss: 0.38314, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:14:21,555 Epoch[33/120] train loss: 0.38240, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:14:25,737 Epoch[34/120] train loss: 0.38193, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:14:29,912 Epoch[35/120] train loss: 0.38333, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:14:34,094 Epoch[36/120] train loss: 0.38217, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:14:38,220 Epoch[37/120] train loss: 0.38229, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:14:42,371 Epoch[38/120] train loss: 0.38118, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:14:46,512 Epoch[39/120] train loss: 0.38120, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:14:50,679 Epoch[40/120] train loss: 0.38004, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:14:54,815 Epoch[41/120] train loss: 0.37965, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:14:59,001 Epoch[42/120] train loss: 0.37767, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:15:03,161 Epoch[43/120] train loss: 0.38019, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:15:07,363 Epoch[44/120] train loss: 0.37777, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:15:11,548 Epoch[45/120] train loss: 0.38074, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:15:15,756 Epoch[46/120] train loss: 0.37866, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:15:19,939 Epoch[47/120] train loss: 0.37825, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:15:24,126 Epoch[48/120] train loss: 0.37763, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:15:28,321 Epoch[49/120] train loss: 0.37926, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:15:32,508 Epoch[50/120] train loss: 0.37612, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:15:36,666 Epoch[51/120] train loss: 0.37656, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:15:40,817 Epoch[52/120] train loss: 0.37761, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:15:44,984 Epoch[53/120] train loss: 0.37602, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:15:49,130 Epoch[54/120] train loss: 0.37456, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:15:53,320 Epoch[55/120] train loss: 0.37474, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:15:57,501 Epoch[56/120] train loss: 0.37443, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:16:01,674 Epoch[57/120] train loss: 0.37407, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:16:05,837 Epoch[58/120] train loss: 0.37537, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:16:09,970 Epoch[59/120] train loss: 0.37441, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:16:14,100 Epoch[60/120] train loss: 0.37474, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:16:18,238 Epoch[61/120] train loss: 0.37336, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:16:22,379 Epoch[62/120] train loss: 0.37591, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:16:26,544 Epoch[63/120] train loss: 0.37410, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:16:30,693 Epoch[64/120] train loss: 0.37185, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:16:34,901 Epoch[65/120] train loss: 0.37458, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:16:39,081 Epoch[66/120] train loss: 0.37240, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:16:43,240 Epoch[67/120] train loss: 0.37208, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:16:47,423 Epoch[68/120] train loss: 0.37271, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:16:51,608 Epoch[69/120] train loss: 0.37473, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:16:55,758 Epoch[70/120] train loss: 0.37229, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:16:59,912 Epoch[71/120] train loss: 0.37177, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:17:04,066 Epoch[72/120] train loss: 0.37226, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:17:08,232 Epoch[73/120] train loss: 0.37158, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:17:12,412 Epoch[74/120] train loss: 0.37193, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:17:16,586 Epoch[75/120] train loss: 0.37169, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:17:20,710 Epoch[76/120] train loss: 0.37362, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:17:24,860 Epoch[77/120] train loss: 0.37066, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:17:29,047 Epoch[78/120] train loss: 0.37127, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:17:33,181 Epoch[79/120] train loss: 0.37146, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:17:37,356 Epoch[80/120] train loss: 0.37063, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:17:41,521 Epoch[81/120] train loss: 0.36968, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:17:45,700 Epoch[82/120] train loss: 0.37096, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:17:49,878 Epoch[83/120] train loss: 0.36956, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:17:54,023 Epoch[84/120] train loss: 0.37191, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:17:58,178 Epoch[85/120] train loss: 0.37015, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:18:02,302 Epoch[86/120] train loss: 0.37109, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:18:06,460 Epoch[87/120] train loss: 0.37008, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:18:10,606 Epoch[88/120] train loss: 0.37015, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:18:14,805 Epoch[89/120] train loss: 0.36945, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:18:18,916 Epoch[90/120] train loss: 0.36765, val loss: nan, lr: 0.0010000, time: 4.11\n",
      "2022-06-08 19:18:23,125 Epoch[91/120] train loss: 0.37181, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:18:27,289 Epoch[92/120] train loss: 0.36917, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:18:31,470 Epoch[93/120] train loss: 0.36981, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:18:35,646 Epoch[94/120] train loss: 0.36834, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:18:39,862 Epoch[95/120] train loss: 0.36848, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:18:44,047 Epoch[96/120] train loss: 0.36902, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:18:48,246 Epoch[97/120] train loss: 0.36888, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:18:52,394 Epoch[98/120] train loss: 0.36826, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:18:56,546 Epoch[99/120] train loss: 0.36871, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:19:00,727 Epoch[100/120] train loss: 0.36952, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:19:04,887 Epoch[101/120] train loss: 0.36749, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:19:09,040 Epoch[102/120] train loss: 0.36809, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:19:13,238 Epoch[103/120] train loss: 0.36861, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:19:17,391 Epoch[104/120] train loss: 0.36833, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:19:21,576 Epoch[105/120] train loss: 0.36828, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:19:25,759 Epoch[106/120] train loss: 0.36830, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:19:29,964 Epoch[107/120] train loss: 0.36794, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:19:34,130 Epoch[108/120] train loss: 0.36944, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:19:38,340 Epoch[109/120] train loss: 0.36717, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:19:42,547 Epoch[110/120] train loss: 0.36806, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:19:46,736 Epoch[111/120] train loss: 0.36885, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:19:50,897 Epoch[112/120] train loss: 0.36800, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:19:55,083 Epoch[113/120] train loss: 0.36822, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:19:59,232 Epoch[114/120] train loss: 0.36783, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:20:03,404 Epoch[115/120] train loss: 0.37016, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:20:07,576 Epoch[116/120] train loss: 0.36914, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:20:11,693 Epoch[117/120] train loss: 0.36794, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:20:15,874 Epoch[118/120] train loss: 0.36658, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:20:20,044 Epoch[119/120] train loss: 0.36800, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:20:24,224 Epoch[120/120] train loss: 0.36783, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:20:24,224 => end training\n",
      "2022-06-08 19:20:24,225 => calculating train scores\n",
      "2022-06-08 19:20:26,584 => train score\n",
      "accuracy: 0.9970750935029451\n",
      "presision: 0.7299787384833452\n",
      "recall: 0.7910906298003072\n",
      "f1: 0.759307040176926\n",
      "2022-06-08 19:20:26,585 => calculating test scores\n",
      "2022-06-08 19:21:47,828 => test score\n",
      "accuracy: 0.9956621127850676\n",
      "presision: 0.2631578947368421\n",
      "recall: 0.3873239436619718\n",
      "f1: 0.31339031339031337\n",
      "2022-06-08 19:21:47,861 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:21:52,181 Epoch[1/120] train loss: 0.75840, val loss: nan, lr: 0.0010000, time: 4.32\n",
      "2022-06-08 19:21:56,331 Epoch[2/120] train loss: 0.62375, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:22:00,491 Epoch[3/120] train loss: 0.55420, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:22:04,588 Epoch[4/120] train loss: 0.51311, val loss: nan, lr: 0.0010000, time: 4.10\n",
      "2022-06-08 19:22:08,731 Epoch[5/120] train loss: 0.47967, val loss: nan, lr: 0.0010000, time: 4.14\n",
      "2022-06-08 19:22:12,852 Epoch[6/120] train loss: 0.46478, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:22:16,968 Epoch[7/120] train loss: 0.45031, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:22:21,142 Epoch[8/120] train loss: 0.44299, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:22:25,358 Epoch[9/120] train loss: 0.43854, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:22:29,539 Epoch[10/120] train loss: 0.43444, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:22:33,703 Epoch[11/120] train loss: 0.42753, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:22:37,849 Epoch[12/120] train loss: 0.42369, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:22:41,978 Epoch[13/120] train loss: 0.42077, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:22:46,159 Epoch[14/120] train loss: 0.41939, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:22:50,356 Epoch[15/120] train loss: 0.42144, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:22:54,536 Epoch[16/120] train loss: 0.41844, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:22:58,682 Epoch[17/120] train loss: 0.43130, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:23:02,852 Epoch[18/120] train loss: 0.41885, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:23:06,976 Epoch[19/120] train loss: 0.41640, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:23:11,100 Epoch[20/120] train loss: 0.41327, val loss: nan, lr: 0.0010000, time: 4.12\n",
      "2022-06-08 19:23:15,283 Epoch[21/120] train loss: 0.41284, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:23:19,467 Epoch[22/120] train loss: 0.40805, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:23:23,615 Epoch[23/120] train loss: 0.40916, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:23:27,748 Epoch[24/120] train loss: 0.40896, val loss: nan, lr: 0.0010000, time: 4.13\n",
      "2022-06-08 19:23:31,937 Epoch[25/120] train loss: 0.40719, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:23:36,099 Epoch[26/120] train loss: 0.40386, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:23:40,260 Epoch[27/120] train loss: 0.40632, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:23:44,430 Epoch[28/120] train loss: 0.40488, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:23:48,604 Epoch[29/120] train loss: 0.40473, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:23:52,759 Epoch[30/120] train loss: 0.40412, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:23:56,921 Epoch[31/120] train loss: 0.40541, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:24:01,122 Epoch[32/120] train loss: 0.40403, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:24:05,290 Epoch[33/120] train loss: 0.40702, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:24:09,443 Epoch[34/120] train loss: 0.40676, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:24:13,621 Epoch[35/120] train loss: 0.40872, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:24:17,770 Epoch[36/120] train loss: 0.40463, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:24:21,940 Epoch[37/120] train loss: 0.40330, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:24:26,105 Epoch[38/120] train loss: 0.40366, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:24:30,294 Epoch[39/120] train loss: 0.40091, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:24:34,442 Epoch[40/120] train loss: 0.39950, val loss: nan, lr: 0.0010000, time: 4.15\n",
      "2022-06-08 19:24:38,628 Epoch[41/120] train loss: 0.40046, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:24:42,840 Epoch[42/120] train loss: 0.40130, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:24:47,009 Epoch[43/120] train loss: 0.39593, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:24:51,184 Epoch[44/120] train loss: 0.39976, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:24:55,639 Epoch[45/120] train loss: 0.39754, val loss: nan, lr: 0.0010000, time: 4.45\n",
      "2022-06-08 19:24:59,840 Epoch[46/120] train loss: 0.39659, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:25:04,023 Epoch[47/120] train loss: 0.39772, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:25:08,204 Epoch[48/120] train loss: 0.39306, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:25:12,449 Epoch[49/120] train loss: 0.43057, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:25:16,660 Epoch[50/120] train loss: 0.40945, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:25:20,885 Epoch[51/120] train loss: 0.40141, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:25:25,108 Epoch[52/120] train loss: 0.39836, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:25:29,295 Epoch[53/120] train loss: 0.40101, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:25:33,473 Epoch[54/120] train loss: 0.39658, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:25:37,667 Epoch[55/120] train loss: 0.39840, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:25:41,878 Epoch[56/120] train loss: 0.39459, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:25:46,082 Epoch[57/120] train loss: 0.39549, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:25:50,264 Epoch[58/120] train loss: 0.39617, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:25:54,509 Epoch[59/120] train loss: 0.39470, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:25:58,672 Epoch[60/120] train loss: 0.39525, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:26:02,833 Epoch[61/120] train loss: 0.39634, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:26:07,001 Epoch[62/120] train loss: 0.39333, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:26:11,195 Epoch[63/120] train loss: 0.40133, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:26:15,383 Epoch[64/120] train loss: 0.40544, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:26:19,596 Epoch[65/120] train loss: 0.39931, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:26:23,837 Epoch[66/120] train loss: 0.39559, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:26:28,059 Epoch[67/120] train loss: 0.39519, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:26:32,270 Epoch[68/120] train loss: 0.39687, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:26:36,467 Epoch[69/120] train loss: 0.39377, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:26:40,681 Epoch[70/120] train loss: 0.39412, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:26:44,899 Epoch[71/120] train loss: 0.39140, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:26:49,115 Epoch[72/120] train loss: 0.39437, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:26:53,319 Epoch[73/120] train loss: 0.38932, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:26:57,537 Epoch[74/120] train loss: 0.39017, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:27:01,748 Epoch[75/120] train loss: 0.39224, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:27:05,957 Epoch[76/120] train loss: 0.39101, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:27:10,161 Epoch[77/120] train loss: 0.38905, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:27:14,354 Epoch[78/120] train loss: 0.38892, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:27:18,544 Epoch[79/120] train loss: 0.38823, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:27:22,727 Epoch[80/120] train loss: 0.38831, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:27:26,966 Epoch[81/120] train loss: 0.38729, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:27:31,177 Epoch[82/120] train loss: 0.39221, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:27:35,355 Epoch[83/120] train loss: 0.38660, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:27:39,536 Epoch[84/120] train loss: 0.39044, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:27:43,754 Epoch[85/120] train loss: 0.38658, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:27:47,948 Epoch[86/120] train loss: 0.38649, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:27:52,135 Epoch[87/120] train loss: 0.38578, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:27:56,347 Epoch[88/120] train loss: 0.39001, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:28:00,551 Epoch[89/120] train loss: 0.39052, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:28:04,770 Epoch[90/120] train loss: 0.38602, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:28:08,970 Epoch[91/120] train loss: 0.39157, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:28:13,172 Epoch[92/120] train loss: 0.38591, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:28:17,372 Epoch[93/120] train loss: 0.38361, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:28:21,578 Epoch[94/120] train loss: 0.40025, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:28:25,814 Epoch[95/120] train loss: 0.39014, val loss: nan, lr: 0.0010000, time: 4.24\n",
      "2022-06-08 19:28:30,026 Epoch[96/120] train loss: 0.38379, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:28:34,186 Epoch[97/120] train loss: 0.39174, val loss: nan, lr: 0.0010000, time: 4.16\n",
      "2022-06-08 19:28:38,380 Epoch[98/120] train loss: 0.38975, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:28:42,635 Epoch[99/120] train loss: 0.39095, val loss: nan, lr: 0.0010000, time: 4.25\n",
      "2022-06-08 19:28:46,827 Epoch[100/120] train loss: 0.38417, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:28:51,054 Epoch[101/120] train loss: 0.38376, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:28:55,276 Epoch[102/120] train loss: 0.38765, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:28:59,454 Epoch[103/120] train loss: 0.38898, val loss: nan, lr: 0.0010000, time: 4.18\n",
      "2022-06-08 19:29:03,685 Epoch[104/120] train loss: 0.38454, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:29:07,899 Epoch[105/120] train loss: 0.39242, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:29:12,128 Epoch[106/120] train loss: 0.38443, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:29:16,354 Epoch[107/120] train loss: 0.38457, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:29:20,574 Epoch[108/120] train loss: 0.38497, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:29:24,769 Epoch[109/120] train loss: 0.38204, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:29:28,960 Epoch[110/120] train loss: 0.38270, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:29:33,154 Epoch[111/120] train loss: 0.38259, val loss: nan, lr: 0.0010000, time: 4.19\n",
      "2022-06-08 19:29:37,367 Epoch[112/120] train loss: 0.38433, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:29:41,585 Epoch[113/120] train loss: 0.38266, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:29:45,809 Epoch[114/120] train loss: 0.38707, val loss: nan, lr: 0.0010000, time: 4.22\n",
      "2022-06-08 19:29:50,044 Epoch[115/120] train loss: 0.39143, val loss: nan, lr: 0.0010000, time: 4.23\n",
      "2022-06-08 19:29:54,290 Epoch[116/120] train loss: 0.38828, val loss: nan, lr: 0.0010000, time: 4.25\n",
      "2022-06-08 19:29:58,463 Epoch[117/120] train loss: 0.38461, val loss: nan, lr: 0.0010000, time: 4.17\n",
      "2022-06-08 19:30:02,665 Epoch[118/120] train loss: 0.38171, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:30:06,872 Epoch[119/120] train loss: 0.38348, val loss: nan, lr: 0.0010000, time: 4.21\n",
      "2022-06-08 19:30:11,076 Epoch[120/120] train loss: 0.38219, val loss: nan, lr: 0.0010000, time: 4.20\n",
      "2022-06-08 19:30:11,077 => end training\n",
      "2022-06-08 19:30:11,078 => calculating train scores\n",
      "2022-06-08 19:30:13,512 => train score\n",
      "accuracy: 0.9971557187968915\n",
      "presision: 0.6772993088782563\n",
      "recall: 0.978494623655914\n",
      "f1: 0.800502670436695\n",
      "2022-06-08 19:30:13,513 => calculating test scores\n",
      "2022-06-08 19:31:35,858 => test score\n",
      "accuracy: 0.9951761254207391\n",
      "presision: 0.25\n",
      "recall: 0.44366197183098594\n",
      "f1: 0.31979695431472077\n",
      "2022-06-08 19:31:35,903 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:31:46,802 Epoch[1/120] train loss: 0.60822, val loss: nan, lr: 0.0010000, time: 10.90\n",
      "2022-06-08 19:31:57,311 Epoch[2/120] train loss: 0.54226, val loss: nan, lr: 0.0010000, time: 10.51\n",
      "2022-06-08 19:32:07,813 Epoch[3/120] train loss: 0.47888, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 19:32:18,371 Epoch[4/120] train loss: 0.44293, val loss: nan, lr: 0.0010000, time: 10.56\n",
      "2022-06-08 19:32:28,959 Epoch[5/120] train loss: 0.42259, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:32:39,509 Epoch[6/120] train loss: 0.40862, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:32:50,054 Epoch[7/120] train loss: 0.40012, val loss: nan, lr: 0.0010000, time: 10.54\n",
      "2022-06-08 19:33:00,675 Epoch[8/120] train loss: 0.39427, val loss: nan, lr: 0.0010000, time: 10.62\n",
      "2022-06-08 19:33:11,266 Epoch[9/120] train loss: 0.39117, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:33:21,851 Epoch[10/120] train loss: 0.38813, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:33:32,417 Epoch[11/120] train loss: 0.38674, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:33:42,984 Epoch[12/120] train loss: 0.38575, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:33:53,558 Epoch[13/120] train loss: 0.38425, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:34:04,150 Epoch[14/120] train loss: 0.38291, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:34:14,690 Epoch[15/120] train loss: 0.38198, val loss: nan, lr: 0.0010000, time: 10.54\n",
      "2022-06-08 19:34:25,229 Epoch[16/120] train loss: 0.38224, val loss: nan, lr: 0.0010000, time: 10.54\n",
      "2022-06-08 19:34:35,803 Epoch[17/120] train loss: 0.38184, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:34:46,353 Epoch[18/120] train loss: 0.38096, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:34:56,946 Epoch[19/120] train loss: 0.38105, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:35:07,531 Epoch[20/120] train loss: 0.37886, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:35:18,101 Epoch[21/120] train loss: 0.37931, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:35:28,698 Epoch[22/120] train loss: 0.37785, val loss: nan, lr: 0.0010000, time: 10.60\n",
      "2022-06-08 19:35:39,233 Epoch[23/120] train loss: 0.37672, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 19:35:49,789 Epoch[24/120] train loss: 0.37553, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:36:00,356 Epoch[25/120] train loss: 0.37497, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:36:10,916 Epoch[26/120] train loss: 0.37482, val loss: nan, lr: 0.0010000, time: 10.56\n",
      "2022-06-08 19:36:21,528 Epoch[27/120] train loss: 0.37518, val loss: nan, lr: 0.0010000, time: 10.61\n",
      "2022-06-08 19:36:32,109 Epoch[28/120] train loss: 0.37375, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:36:42,671 Epoch[29/120] train loss: 0.37221, val loss: nan, lr: 0.0010000, time: 10.56\n",
      "2022-06-08 19:36:53,247 Epoch[30/120] train loss: 0.37293, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:37:03,833 Epoch[31/120] train loss: 0.37220, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:37:14,379 Epoch[32/120] train loss: 0.37268, val loss: nan, lr: 0.0010000, time: 10.54\n",
      "2022-06-08 19:37:24,912 Epoch[33/120] train loss: 0.37177, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 19:37:35,467 Epoch[34/120] train loss: 0.37023, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:37:46,072 Epoch[35/120] train loss: 0.37025, val loss: nan, lr: 0.0010000, time: 10.60\n",
      "2022-06-08 19:37:56,649 Epoch[36/120] train loss: 0.37000, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:38:07,202 Epoch[37/120] train loss: 0.36937, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:38:17,871 Epoch[38/120] train loss: 0.37181, val loss: nan, lr: 0.0010000, time: 10.67\n",
      "2022-06-08 19:38:28,445 Epoch[39/120] train loss: 0.36903, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:38:39,043 Epoch[40/120] train loss: 0.36941, val loss: nan, lr: 0.0010000, time: 10.60\n",
      "2022-06-08 19:38:49,598 Epoch[41/120] train loss: 0.36956, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 19:39:00,222 Epoch[42/120] train loss: 0.36914, val loss: nan, lr: 0.0010000, time: 10.62\n",
      "2022-06-08 19:39:10,832 Epoch[43/120] train loss: 0.36762, val loss: nan, lr: 0.0010000, time: 10.61\n",
      "2022-06-08 19:39:21,404 Epoch[44/120] train loss: 0.36949, val loss: nan, lr: 0.0010000, time: 10.57\n",
      "2022-06-08 19:39:31,995 Epoch[45/120] train loss: 0.36755, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:39:42,578 Epoch[46/120] train loss: 0.36758, val loss: nan, lr: 0.0010000, time: 10.58\n",
      "2022-06-08 19:39:53,194 Epoch[47/120] train loss: 0.36831, val loss: nan, lr: 0.0010000, time: 10.61\n",
      "2022-06-08 19:40:03,830 Epoch[48/120] train loss: 0.36774, val loss: nan, lr: 0.0010000, time: 10.64\n",
      "2022-06-08 19:40:14,387 Epoch[49/120] train loss: 0.36638, val loss: nan, lr: 0.0010000, time: 10.56\n",
      "2022-06-08 19:40:24,947 Epoch[50/120] train loss: 0.36623, val loss: nan, lr: 0.0010000, time: 10.56\n",
      "2022-06-08 19:40:35,539 Epoch[51/120] train loss: 0.36581, val loss: nan, lr: 0.0010000, time: 10.59\n",
      "2022-06-08 19:40:46,358 Epoch[52/120] train loss: 0.36559, val loss: nan, lr: 0.0010000, time: 10.82\n",
      "2022-06-08 19:40:56,816 Epoch[53/120] train loss: 0.36582, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 19:41:07,284 Epoch[54/120] train loss: 0.36657, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 19:41:17,752 Epoch[55/120] train loss: 0.36689, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 19:41:28,238 Epoch[56/120] train loss: 0.36558, val loss: nan, lr: 0.0010000, time: 10.49\n",
      "2022-06-08 19:41:38,770 Epoch[57/120] train loss: 0.36495, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 19:41:49,204 Epoch[58/120] train loss: 0.36472, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:41:59,636 Epoch[59/120] train loss: 0.36452, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:42:10,044 Epoch[60/120] train loss: 0.36776, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:42:20,416 Epoch[61/120] train loss: 0.37405, val loss: nan, lr: 0.0010000, time: 10.37\n",
      "2022-06-08 19:42:30,784 Epoch[62/120] train loss: 0.36896, val loss: nan, lr: 0.0010000, time: 10.37\n",
      "2022-06-08 19:42:41,207 Epoch[63/120] train loss: 0.36709, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:42:51,617 Epoch[64/120] train loss: 0.36649, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:43:02,051 Epoch[65/120] train loss: 0.36675, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:43:12,456 Epoch[66/120] train loss: 0.36526, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:43:22,887 Epoch[67/120] train loss: 0.36512, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:43:33,290 Epoch[68/120] train loss: 0.36405, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:43:43,711 Epoch[69/120] train loss: 0.36453, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:43:54,047 Epoch[70/120] train loss: 0.36314, val loss: nan, lr: 0.0010000, time: 10.33\n",
      "2022-06-08 19:44:04,447 Epoch[71/120] train loss: 0.36409, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:44:14,882 Epoch[72/120] train loss: 0.36319, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:44:25,267 Epoch[73/120] train loss: 0.36446, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 19:44:35,658 Epoch[74/120] train loss: 0.36514, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 19:44:46,059 Epoch[75/120] train loss: 0.36436, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:44:56,473 Epoch[76/120] train loss: 0.36371, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:45:06,908 Epoch[77/120] train loss: 0.36286, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:45:17,307 Epoch[78/120] train loss: 0.36298, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:45:27,706 Epoch[79/120] train loss: 0.36161, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:45:38,106 Epoch[80/120] train loss: 0.36379, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:45:48,531 Epoch[81/120] train loss: 0.36211, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:45:58,933 Epoch[82/120] train loss: 0.36143, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:46:09,347 Epoch[83/120] train loss: 0.36103, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:46:19,738 Epoch[84/120] train loss: 0.36294, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 19:46:30,139 Epoch[85/120] train loss: 0.36280, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:46:40,560 Epoch[86/120] train loss: 0.36148, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:46:50,941 Epoch[87/120] train loss: 0.36120, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 19:47:01,334 Epoch[88/120] train loss: 0.36076, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 19:47:11,736 Epoch[89/120] train loss: 0.36269, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:47:22,148 Epoch[90/120] train loss: 0.36116, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:47:32,581 Epoch[91/120] train loss: 0.36036, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:47:43,014 Epoch[92/120] train loss: 0.35994, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:47:53,440 Epoch[93/120] train loss: 0.36029, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:48:03,853 Epoch[94/120] train loss: 0.35974, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:48:14,285 Epoch[95/120] train loss: 0.35854, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:48:24,727 Epoch[96/120] train loss: 0.35928, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:48:35,146 Epoch[97/120] train loss: 0.35975, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:48:45,561 Epoch[98/120] train loss: 0.35868, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:48:55,983 Epoch[99/120] train loss: 0.36042, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:49:06,421 Epoch[100/120] train loss: 0.35845, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:49:16,836 Epoch[101/120] train loss: 0.37219, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:49:27,257 Epoch[102/120] train loss: 0.36736, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:49:37,667 Epoch[103/120] train loss: 0.36484, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:49:48,106 Epoch[104/120] train loss: 0.36210, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:49:58,461 Epoch[105/120] train loss: 0.36110, val loss: nan, lr: 0.0010000, time: 10.35\n",
      "2022-06-08 19:50:08,871 Epoch[106/120] train loss: 0.36269, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:50:19,340 Epoch[107/120] train loss: 0.36040, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 19:50:29,707 Epoch[108/120] train loss: 0.36004, val loss: nan, lr: 0.0010000, time: 10.37\n",
      "2022-06-08 19:50:40,122 Epoch[109/120] train loss: 0.35911, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:50:50,537 Epoch[110/120] train loss: 0.35982, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:51:00,954 Epoch[111/120] train loss: 0.35916, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:51:11,366 Epoch[112/120] train loss: 0.35881, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:51:21,788 Epoch[113/120] train loss: 0.35914, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:51:32,204 Epoch[114/120] train loss: 0.35800, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:51:42,657 Epoch[115/120] train loss: 0.35961, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 19:51:53,116 Epoch[116/120] train loss: 0.35756, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 19:52:03,518 Epoch[117/120] train loss: 0.35788, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 19:52:13,954 Epoch[118/120] train loss: 0.35752, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:52:24,334 Epoch[119/120] train loss: 0.35793, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 19:52:34,743 Epoch[120/120] train loss: 0.36001, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:52:34,744 => end training\n",
      "2022-06-08 19:52:34,745 => calculating train scores\n",
      "2022-06-08 19:52:38,812 => train score\n",
      "accuracy: 0.9971109269669213\n",
      "presision: 0.8101983002832861\n",
      "recall: 0.6589861751152074\n",
      "f1: 0.7268106734434561\n",
      "2022-06-08 19:52:38,813 => calculating test scores\n",
      "2022-06-08 19:54:02,319 => test score\n",
      "accuracy: 0.9974980650503087\n",
      "presision: 0.5189873417721519\n",
      "recall: 0.2887323943661972\n",
      "f1: 0.37104072398190047\n",
      "2022-06-08 19:54:02,423 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 19:54:13,068 Epoch[1/120] train loss: 0.63154, val loss: nan, lr: 0.0010000, time: 10.64\n",
      "2022-06-08 19:54:23,491 Epoch[2/120] train loss: 0.53957, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:54:33,864 Epoch[3/120] train loss: 0.50503, val loss: nan, lr: 0.0010000, time: 10.37\n",
      "2022-06-08 19:54:44,247 Epoch[4/120] train loss: 0.46832, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 19:54:54,738 Epoch[5/120] train loss: 0.46451, val loss: nan, lr: 0.0010000, time: 10.49\n",
      "2022-06-08 19:55:05,150 Epoch[6/120] train loss: 0.44609, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:55:15,587 Epoch[7/120] train loss: 0.43483, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:55:26,010 Epoch[8/120] train loss: 0.42891, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:55:36,475 Epoch[9/120] train loss: 0.42486, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 19:55:46,944 Epoch[10/120] train loss: 0.42028, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 19:55:57,351 Epoch[11/120] train loss: 0.41403, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:56:07,778 Epoch[12/120] train loss: 0.41066, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:56:18,201 Epoch[13/120] train loss: 0.40786, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:56:28,616 Epoch[14/120] train loss: 0.40619, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:56:39,046 Epoch[15/120] train loss: 0.40522, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:56:49,398 Epoch[16/120] train loss: 0.40515, val loss: nan, lr: 0.0010000, time: 10.35\n",
      "2022-06-08 19:56:59,819 Epoch[17/120] train loss: 0.40356, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:57:10,263 Epoch[18/120] train loss: 0.40271, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:57:20,728 Epoch[19/120] train loss: 0.40302, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 19:57:31,151 Epoch[20/120] train loss: 0.40162, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:57:41,562 Epoch[21/120] train loss: 0.40185, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:57:51,952 Epoch[22/120] train loss: 0.40248, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 19:58:02,372 Epoch[23/120] train loss: 0.40235, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:58:12,803 Epoch[24/120] train loss: 0.40047, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:58:23,222 Epoch[25/120] train loss: 0.40053, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 19:58:33,674 Epoch[26/120] train loss: 0.40005, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 19:58:44,107 Epoch[27/120] train loss: 0.40074, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 19:58:54,562 Epoch[28/120] train loss: 0.39925, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 19:59:04,975 Epoch[29/120] train loss: 0.39798, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:59:15,421 Epoch[30/120] train loss: 0.39581, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 19:59:25,875 Epoch[31/120] train loss: 0.39610, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 19:59:36,262 Epoch[32/120] train loss: 0.39379, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 19:59:46,676 Epoch[33/120] train loss: 0.39303, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 19:59:57,143 Epoch[34/120] train loss: 0.39401, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:00:07,559 Epoch[35/120] train loss: 0.41165, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:00:17,987 Epoch[36/120] train loss: 0.41017, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:00:28,415 Epoch[37/120] train loss: 0.40745, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:00:38,802 Epoch[38/120] train loss: 0.40580, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:00:49,262 Epoch[39/120] train loss: 0.40417, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:00:59,712 Epoch[40/120] train loss: 0.40286, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:01:10,107 Epoch[41/120] train loss: 0.40236, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:01:20,559 Epoch[42/120] train loss: 0.40209, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:01:30,997 Epoch[43/120] train loss: 0.40212, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:01:41,394 Epoch[44/120] train loss: 0.40163, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:01:51,808 Epoch[45/120] train loss: 0.40120, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:02:02,298 Epoch[46/120] train loss: 0.40169, val loss: nan, lr: 0.0010000, time: 10.49\n",
      "2022-06-08 20:02:12,747 Epoch[47/120] train loss: 0.40019, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:02:23,163 Epoch[48/120] train loss: 0.39985, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:02:33,602 Epoch[49/120] train loss: 0.39974, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:02:44,059 Epoch[50/120] train loss: 0.40010, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:02:54,506 Epoch[51/120] train loss: 0.39986, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:03:04,916 Epoch[52/120] train loss: 0.39924, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:03:15,337 Epoch[53/120] train loss: 0.39791, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:03:25,742 Epoch[54/120] train loss: 0.39791, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:03:36,173 Epoch[55/120] train loss: 0.39680, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:03:46,636 Epoch[56/120] train loss: 0.39611, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:03:57,043 Epoch[57/120] train loss: 0.39432, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:04:07,442 Epoch[58/120] train loss: 0.39500, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:04:17,837 Epoch[59/120] train loss: 0.39322, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:04:28,250 Epoch[60/120] train loss: 0.39360, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:04:38,703 Epoch[61/120] train loss: 0.39332, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:04:49,125 Epoch[62/120] train loss: 0.39320, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:04:59,540 Epoch[63/120] train loss: 0.39310, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:05:09,985 Epoch[64/120] train loss: 0.39361, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:05:20,423 Epoch[65/120] train loss: 0.39416, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:05:30,861 Epoch[66/120] train loss: 0.39321, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:05:41,264 Epoch[67/120] train loss: 0.39180, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:05:51,654 Epoch[68/120] train loss: 0.39204, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:06:02,069 Epoch[69/120] train loss: 0.39106, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:06:12,462 Epoch[70/120] train loss: 0.39116, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:06:22,913 Epoch[71/120] train loss: 0.39055, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:06:33,396 Epoch[72/120] train loss: 0.39103, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:06:43,805 Epoch[73/120] train loss: 0.39051, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:06:54,246 Epoch[74/120] train loss: 0.39022, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:07:04,661 Epoch[75/120] train loss: 0.39108, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:07:15,083 Epoch[76/120] train loss: 0.39094, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:07:25,538 Epoch[77/120] train loss: 0.39034, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:07:36,003 Epoch[78/120] train loss: 0.39075, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:07:46,425 Epoch[79/120] train loss: 0.38973, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:07:56,852 Epoch[80/120] train loss: 0.38873, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:08:07,296 Epoch[81/120] train loss: 0.38825, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:08:17,734 Epoch[82/120] train loss: 0.38919, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:08:28,185 Epoch[83/120] train loss: 0.38763, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:08:38,640 Epoch[84/120] train loss: 0.38694, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:08:49,038 Epoch[85/120] train loss: 0.38568, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:08:59,476 Epoch[86/120] train loss: 0.38742, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:09:09,946 Epoch[87/120] train loss: 0.38590, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:09:20,392 Epoch[88/120] train loss: 0.38506, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:09:30,857 Epoch[89/120] train loss: 0.38544, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:09:41,282 Epoch[90/120] train loss: 0.38435, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:09:51,706 Epoch[91/120] train loss: 0.38472, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:10:02,167 Epoch[92/120] train loss: 0.38286, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:10:12,605 Epoch[93/120] train loss: 0.38231, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:10:23,019 Epoch[94/120] train loss: 0.38149, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:10:33,442 Epoch[95/120] train loss: 0.38216, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:10:43,886 Epoch[96/120] train loss: 0.38127, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:10:54,353 Epoch[97/120] train loss: 0.38065, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:11:04,769 Epoch[98/120] train loss: 0.38020, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:11:15,187 Epoch[99/120] train loss: 0.37891, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:11:25,640 Epoch[100/120] train loss: 0.38059, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:11:36,120 Epoch[101/120] train loss: 0.38037, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:11:46,519 Epoch[102/120] train loss: 0.37851, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:11:56,927 Epoch[103/120] train loss: 0.37829, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:12:07,302 Epoch[104/120] train loss: 0.37950, val loss: nan, lr: 0.0010000, time: 10.37\n",
      "2022-06-08 20:12:17,712 Epoch[105/120] train loss: 0.37775, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:12:28,166 Epoch[106/120] train loss: 0.37642, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:12:38,568 Epoch[107/120] train loss: 0.37496, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:12:49,015 Epoch[108/120] train loss: 0.37580, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:12:59,456 Epoch[109/120] train loss: 0.37623, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:13:09,839 Epoch[110/120] train loss: 0.37605, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 20:13:20,300 Epoch[111/120] train loss: 0.37342, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:13:30,736 Epoch[112/120] train loss: 0.37399, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:13:41,177 Epoch[113/120] train loss: 0.37351, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:13:51,605 Epoch[114/120] train loss: 0.37343, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:14:02,019 Epoch[115/120] train loss: 0.37432, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:14:12,496 Epoch[116/120] train loss: 0.37335, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:14:22,970 Epoch[117/120] train loss: 0.37626, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:14:33,417 Epoch[118/120] train loss: 0.37366, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:14:43,813 Epoch[119/120] train loss: 0.37329, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:14:54,211 Epoch[120/120] train loss: 0.37250, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:14:54,212 => end training\n",
      "2022-06-08 20:14:54,213 => calculating train scores\n",
      "2022-06-08 20:14:58,243 => train score\n",
      "accuracy: 0.9956641508588834\n",
      "presision: 0.5842583249243188\n",
      "recall: 0.8894009216589862\n",
      "f1: 0.7052375152253351\n",
      "2022-06-08 20:14:58,244 => calculating test scores\n",
      "2022-06-08 20:16:22,929 => test score\n",
      "accuracy: 0.9952301240167756\n",
      "presision: 0.30959752321981426\n",
      "recall: 0.704225352112676\n",
      "f1: 0.43010752688172044\n",
      "2022-06-08 20:16:23,012 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 20:16:33,687 Epoch[1/120] train loss: 0.81442, val loss: nan, lr: 0.0010000, time: 10.67\n",
      "2022-06-08 20:16:44,033 Epoch[2/120] train loss: 0.71074, val loss: nan, lr: 0.0010000, time: 10.35\n",
      "2022-06-08 20:16:54,459 Epoch[3/120] train loss: 0.62212, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:17:04,871 Epoch[4/120] train loss: 0.56716, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:17:15,329 Epoch[5/120] train loss: 0.51740, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:17:25,768 Epoch[6/120] train loss: 0.48955, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:17:36,158 Epoch[7/120] train loss: 0.47074, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:17:46,547 Epoch[8/120] train loss: 0.45624, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:17:57,255 Epoch[9/120] train loss: 0.45044, val loss: nan, lr: 0.0010000, time: 10.71\n",
      "2022-06-08 20:18:07,809 Epoch[10/120] train loss: 0.44074, val loss: nan, lr: 0.0010000, time: 10.55\n",
      "2022-06-08 20:18:18,288 Epoch[11/120] train loss: 0.43612, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:18:28,769 Epoch[12/120] train loss: 0.43466, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:18:39,281 Epoch[13/120] train loss: 0.43010, val loss: nan, lr: 0.0010000, time: 10.51\n",
      "2022-06-08 20:18:49,772 Epoch[14/120] train loss: 0.42709, val loss: nan, lr: 0.0010000, time: 10.49\n",
      "2022-06-08 20:19:00,225 Epoch[15/120] train loss: 0.43264, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:19:10,762 Epoch[16/120] train loss: 0.42261, val loss: nan, lr: 0.0010000, time: 10.54\n",
      "2022-06-08 20:19:21,241 Epoch[17/120] train loss: 0.43693, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:19:31,723 Epoch[18/120] train loss: 0.42542, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:19:42,254 Epoch[19/120] train loss: 0.42682, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 20:19:52,788 Epoch[20/120] train loss: 0.41779, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 20:20:03,285 Epoch[21/120] train loss: 0.41757, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:20:13,743 Epoch[22/120] train loss: 0.41580, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:20:24,191 Epoch[23/120] train loss: 0.41365, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:20:34,716 Epoch[24/120] train loss: 0.41180, val loss: nan, lr: 0.0010000, time: 10.52\n",
      "2022-06-08 20:20:45,211 Epoch[25/120] train loss: 0.41163, val loss: nan, lr: 0.0010000, time: 10.49\n",
      "2022-06-08 20:20:55,716 Epoch[26/120] train loss: 0.41778, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:21:06,237 Epoch[27/120] train loss: 0.41540, val loss: nan, lr: 0.0010000, time: 10.52\n",
      "2022-06-08 20:21:16,700 Epoch[28/120] train loss: 0.41363, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:21:27,170 Epoch[29/120] train loss: 0.41181, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:21:37,636 Epoch[30/120] train loss: 0.40842, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:21:48,118 Epoch[31/120] train loss: 0.42122, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:21:58,594 Epoch[32/120] train loss: 0.41410, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:22:09,113 Epoch[33/120] train loss: 0.41256, val loss: nan, lr: 0.0010000, time: 10.52\n",
      "2022-06-08 20:22:19,617 Epoch[34/120] train loss: 0.41277, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:22:30,070 Epoch[35/120] train loss: 0.40944, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:22:40,592 Epoch[36/120] train loss: 0.40624, val loss: nan, lr: 0.0010000, time: 10.52\n",
      "2022-06-08 20:22:51,060 Epoch[37/120] train loss: 0.40680, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:23:01,590 Epoch[38/120] train loss: 0.40476, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 20:23:12,195 Epoch[39/120] train loss: 0.40495, val loss: nan, lr: 0.0010000, time: 10.60\n",
      "2022-06-08 20:23:22,726 Epoch[40/120] train loss: 0.40190, val loss: nan, lr: 0.0010000, time: 10.53\n",
      "2022-06-08 20:23:33,227 Epoch[41/120] train loss: 0.40231, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:23:43,724 Epoch[42/120] train loss: 0.40077, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:23:54,205 Epoch[43/120] train loss: 0.40486, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:24:04,659 Epoch[44/120] train loss: 0.40101, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:24:15,087 Epoch[45/120] train loss: 0.40021, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:24:25,557 Epoch[46/120] train loss: 0.39923, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:24:36,081 Epoch[47/120] train loss: 0.40453, val loss: nan, lr: 0.0010000, time: 10.52\n",
      "2022-06-08 20:24:46,556 Epoch[48/120] train loss: 0.40205, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:24:56,960 Epoch[49/120] train loss: 0.40279, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:25:07,372 Epoch[50/120] train loss: 0.40140, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:25:17,804 Epoch[51/120] train loss: 0.39980, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:25:28,287 Epoch[52/120] train loss: 0.39956, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:25:38,783 Epoch[53/120] train loss: 0.39835, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:25:49,216 Epoch[54/120] train loss: 0.39758, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:25:59,641 Epoch[55/120] train loss: 0.39448, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:26:10,046 Epoch[56/120] train loss: 0.39796, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:26:20,487 Epoch[57/120] train loss: 0.40553, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:26:30,930 Epoch[58/120] train loss: 0.39730, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:26:41,331 Epoch[59/120] train loss: 0.39287, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:26:51,770 Epoch[60/120] train loss: 0.39164, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:27:02,228 Epoch[61/120] train loss: 0.39661, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:27:12,638 Epoch[62/120] train loss: 0.39420, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:27:23,042 Epoch[63/120] train loss: 0.39434, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:27:33,426 Epoch[64/120] train loss: 0.40375, val loss: nan, lr: 0.0010000, time: 10.38\n",
      "2022-06-08 20:27:43,899 Epoch[65/120] train loss: 0.39731, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:27:54,334 Epoch[66/120] train loss: 0.39691, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:28:04,802 Epoch[67/120] train loss: 0.39007, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:28:15,229 Epoch[68/120] train loss: 0.39084, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:28:25,713 Epoch[69/120] train loss: 0.40288, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:28:36,198 Epoch[70/120] train loss: 0.39657, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:28:46,658 Epoch[71/120] train loss: 0.39156, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:28:57,105 Epoch[72/120] train loss: 0.39241, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:29:07,533 Epoch[73/120] train loss: 0.38742, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:29:17,940 Epoch[74/120] train loss: 0.39348, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:29:28,412 Epoch[75/120] train loss: 0.39020, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:29:38,861 Epoch[76/120] train loss: 0.38778, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:29:49,293 Epoch[77/120] train loss: 0.38572, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:29:59,729 Epoch[78/120] train loss: 0.38533, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:30:10,141 Epoch[79/120] train loss: 0.38522, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:30:20,567 Epoch[80/120] train loss: 0.38727, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:30:31,015 Epoch[81/120] train loss: 0.38509, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:30:41,473 Epoch[82/120] train loss: 0.38872, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:30:51,881 Epoch[83/120] train loss: 0.38536, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:31:02,275 Epoch[84/120] train loss: 0.38283, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:31:12,691 Epoch[85/120] train loss: 0.38266, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:31:23,143 Epoch[86/120] train loss: 0.38462, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:31:33,549 Epoch[87/120] train loss: 0.38676, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:31:43,953 Epoch[88/120] train loss: 0.39160, val loss: nan, lr: 0.0010000, time: 10.40\n",
      "2022-06-08 20:31:54,405 Epoch[89/120] train loss: 0.38722, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:32:04,883 Epoch[90/120] train loss: 0.38367, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:32:15,310 Epoch[91/120] train loss: 0.38290, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:32:25,726 Epoch[92/120] train loss: 0.38790, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:32:36,169 Epoch[93/120] train loss: 0.38212, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:32:46,608 Epoch[94/120] train loss: 0.38196, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:32:57,033 Epoch[95/120] train loss: 0.38102, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:33:07,489 Epoch[96/120] train loss: 0.38690, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:33:17,950 Epoch[97/120] train loss: 0.39105, val loss: nan, lr: 0.0010000, time: 10.46\n",
      "2022-06-08 20:33:28,416 Epoch[98/120] train loss: 0.38536, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:33:38,824 Epoch[99/120] train loss: 0.38253, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:33:49,297 Epoch[100/120] train loss: 0.38896, val loss: nan, lr: 0.0010000, time: 10.47\n",
      "2022-06-08 20:33:59,719 Epoch[101/120] train loss: 0.38400, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:34:10,077 Epoch[102/120] train loss: 0.38150, val loss: nan, lr: 0.0010000, time: 10.36\n",
      "2022-06-08 20:34:20,499 Epoch[103/120] train loss: 0.38332, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:34:30,942 Epoch[104/120] train loss: 0.38412, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:34:41,447 Epoch[105/120] train loss: 0.38296, val loss: nan, lr: 0.0010000, time: 10.50\n",
      "2022-06-08 20:34:51,890 Epoch[106/120] train loss: 0.38650, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:35:02,341 Epoch[107/120] train loss: 0.38523, val loss: nan, lr: 0.0010000, time: 10.45\n",
      "2022-06-08 20:35:12,774 Epoch[108/120] train loss: 0.38202, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:35:23,205 Epoch[109/120] train loss: 0.38194, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:35:33,634 Epoch[110/120] train loss: 0.38158, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:35:44,079 Epoch[111/120] train loss: 0.38322, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:35:54,486 Epoch[112/120] train loss: 0.38019, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:36:04,907 Epoch[113/120] train loss: 0.38379, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:36:15,324 Epoch[114/120] train loss: 0.38150, val loss: nan, lr: 0.0010000, time: 10.42\n",
      "2022-06-08 20:36:25,714 Epoch[115/120] train loss: 0.37983, val loss: nan, lr: 0.0010000, time: 10.39\n",
      "2022-06-08 20:36:36,159 Epoch[116/120] train loss: 0.37928, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:36:46,565 Epoch[117/120] train loss: 0.37990, val loss: nan, lr: 0.0010000, time: 10.41\n",
      "2022-06-08 20:36:56,994 Epoch[118/120] train loss: 0.38174, val loss: nan, lr: 0.0010000, time: 10.43\n",
      "2022-06-08 20:37:07,475 Epoch[119/120] train loss: 0.39195, val loss: nan, lr: 0.0010000, time: 10.48\n",
      "2022-06-08 20:37:17,917 Epoch[120/120] train loss: 0.38297, val loss: nan, lr: 0.0010000, time: 10.44\n",
      "2022-06-08 20:37:17,918 => end training\n",
      "2022-06-08 20:37:17,918 => calculating train scores\n",
      "2022-06-08 20:37:21,963 => train score\n",
      "accuracy: 0.9983650982060872\n",
      "presision: 0.8193592365371506\n",
      "recall: 0.9231950844854071\n",
      "f1: 0.8681834597327555\n",
      "2022-06-08 20:37:21,964 => calculating test scores\n",
      "2022-06-08 20:38:45,528 => test score\n",
      "accuracy: 0.9968140828338463\n",
      "presision: 0.41706161137440756\n",
      "recall: 0.6197183098591549\n",
      "f1: 0.4985835694050991\n"
     ]
    }
   ],
   "source": [
    "max_acc = [[0, 0, 0, 0], None]\n",
    "max_pre = [[0, 0, 0, 0], None]\n",
    "max_rcl = [[0, 0, 0, 0], None]\n",
    "max_f1 = [[0, 0, 0, 0], None]\n",
    "max_models = [None for _ in range(4)]\n",
    "\n",
    "for n_rnns in params['n_rnns']:\n",
    "    for dim in params['rnn_hidden_dim']:\n",
    "        for weight in params['pos_weight']:\n",
    "            param = dict(n_rnns=n_rnns, rnn_hidden_dim=dim, weight=weight)\n",
    "            print(param)\n",
    "            \n",
    "            # update config\n",
    "            config = {}\n",
    "            for key, val in mdl_cfg.items():\n",
    "                config[key] = val\n",
    "            for key, val in param.items():\n",
    "                config[key] = val\n",
    "            pos_weight = param[\"weight\"]\n",
    "                \n",
    "            # init model, loss, optim\n",
    "            model = init_model(config, device)\n",
    "            criterion = init_loss([1, pos_weight], device)\n",
    "            optimizer, scheduler = init_optim(\n",
    "                model, train_cfg[\"optim\"][\"lr\"], train_cfg[\"optim\"][\"lr_rate\"]\n",
    "            )\n",
    "            \n",
    "            # training\n",
    "            model, epoch, history = train(\n",
    "                model, train_loader, val_loader,\n",
    "                criterion, optimizer, scheduler,\n",
    "                epoch_len, logger, device\n",
    "            )\n",
    "            \n",
    "            # test\n",
    "            score = test(model, test_loader, logger, device)\n",
    "            acc, pre, rcl, f1 = score\n",
    "            \n",
    "            # update max scores\n",
    "            if acc > max_acc[0][0]:\n",
    "                max_acc[0] = score\n",
    "                max_acc[1] = param\n",
    "                max_models[0] = model\n",
    "            if pre > max_pre[0][1]:\n",
    "                max_pre[0] = score\n",
    "                max_pre[1] = param\n",
    "                max_models[1] = model\n",
    "            if rcl > max_rcl[0][2]:\n",
    "                max_rcl[0] = score\n",
    "                max_rcl[1] = param\n",
    "                max_models[2] = model\n",
    "            if f1 > max_f1[0][3]:\n",
    "                max_f1[0] = score\n",
    "                max_f1[1] = param\n",
    "                max_models[3] = model\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48bd10c2-70ac-46bb-a799-ea597a2204ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=120\n",
      "max accuracy:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 4}\n",
      "accuracy: 0.998 precision: 0.514 recall: 0.528 f1_score: 0.521\n",
      "max precision:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 4}\n",
      "accuracy: 0.997 precision: 0.519 recall: 0.289 f1_score: 0.371\n",
      "max recall:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 8}\n",
      "accuracy: 0.995 precision: 0.310 recall: 0.704 f1_score: 0.430\n",
      "max f1:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 4}\n",
      "accuracy: 0.998 precision: 0.514 recall: 0.528 f1_score: 0.521\n"
     ]
    }
   ],
   "source": [
    "print(f\"epoch={epoch}\")\n",
    "print('max accuracy: ', max_acc[1])\n",
    "acc, pre, rcl, f1 = max_acc[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max precision: ', max_pre[1])\n",
    "acc, pre, rcl, f1 = max_pre[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max recall: ', max_rcl[1])\n",
    "acc, pre, rcl, f1 = max_rcl[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max f1: ', max_f1[1])\n",
    "acc, pre, rcl, f1 = max_f1[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "784c9760-81f6-4649-8df0-1d465e53a98f",
   "metadata": {},
   "source": [
    "epoch=50\n",
    "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 8}\n",
    "accuracy: 0.997 precision: 0.000 recall: 0.000 f1_score: 0.000\n",
    "\n",
    "max precision:  {'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 8}\n",
    "accuracy: 0.996 precision: 0.371 recall: 0.577 f1_score: 0.452\n",
    "\n",
    "max recall:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 8}\n",
    "accuracy: 0.977 precision: 0.080 recall: 0.746 f1_score: 0.145\n",
    "\n",
    "max f1:  {'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 8}\n",
    "accuracy: 0.996 precision: 0.371 recall: 0.577 f1_score: 0.452"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d191bd0c-f959-48a8-a728-ab999e0ec8dd",
   "metadata": {},
   "source": [
    "epoch=100\n",
    "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 4}\n",
    "accuracy: 0.997 precision: 0.464 recall: 0.493 f1_score: 0.478\n",
    "\n",
    "max precision:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 4}\n",
    "accuracy: 0.997 precision: 0.464 recall: 0.493 f1_score: 0.478\n",
    "\n",
    "max recall:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 16}\n",
    "accuracy: 0.970 precision: 0.065 recall: 0.803 f1_score: 0.120 <= max recall\n",
    "\n",
    "max f1:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 4}\n",
    "accuracy: 0.997 precision: 0.464 recall: 0.493 f1_score: 0.478"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d1c8008-8112-4674-9e0b-859a0d0ec1fc",
   "metadata": {},
   "source": [
    "epoch=120\n",
    "max accuracy:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 4}\n",
    "accuracy: 0.998 precision: 0.514 recall: 0.528 f1_score: 0.521\n",
    "\n",
    "max precision:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 4}\n",
    "accuracy: 0.997 precision: 0.519 recall: 0.289 f1_score: 0.371\n",
    "\n",
    "max recall:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 8}\n",
    "accuracy: 0.995 precision: 0.310 recall: 0.704 f1_score: 0.430\n",
    "\n",
    "max f1:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 4}\n",
    "accuracy: 0.998 precision: 0.514 recall: 0.528 f1_score: 0.521 <= max f1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dc558ae-3a42-41b6-9861-890d3a912a3e",
   "metadata": {},
   "source": [
    "epoch=150\n",
    "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 4}\n",
    "accuracy: 0.997 precision: 0.000 recall: 0.000 f1_score: 0.000\n",
    "\n",
    "max precision:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 8}\n",
    "accuracy: 0.997 precision: 0.425 recall: 0.556 f1_score: 0.482\n",
    "\n",
    "max recall:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 16}\n",
    "accuracy: 0.995 precision: 0.274 recall: 0.620 f1_score: 0.380\n",
    "\n",
    "max f1:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 8}\n",
    "accuracy: 0.997 precision: 0.425 recall: 0.556 f1_score: 0.482"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0f84ed3-8e4f-4bb3-8fa3-e661fe5ff595",
   "metadata": {},
   "source": [
    "epoch=200\n",
    "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 1}\n",
    "accuracy: 0.997 precision: 0.000 recall: 0.000 f1_score: 0.000\n",
    "\n",
    "max precision:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 16}\n",
    "accuracy: 0.997 precision: 0.491 recall: 0.373 f1_score: 0.424\n",
    "\n",
    "max recall:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 16}\n",
    "accuracy: 0.994 precision: 0.236 recall: 0.655 f1_score: 0.347\n",
    "\n",
    "max f1:  {'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 1}\n",
    "accuracy: 0.997 precision: 0.477 recall: 0.507 f1_score: 0.491"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0835b-4abf-46d4-ba43-1d4c26646dbb",
   "metadata": {},
   "source": [
    "## モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b54bf6e-f404-4857-b6cb-5b82b32d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max recall\n",
    "model = max_models[2]\n",
    "param = max_rcl[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9824f0c-aaf5-4d21-9350-2b5137621cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_recall_ep{epoch}.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc0d883-965c-43a5-9c60-bac39e468dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_recall_ep{epoch}.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147e18e5-12d3-41cb-a631-48554ecc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max f1\n",
    "model = max_models[3]\n",
    "param = max_f1[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfcc4a91-ce20-4aa0-a790-effed2c75aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_f1_ep{epoch}.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb143daa-abbe-419a-9fb9-8d58fc1ad234",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_f1_ep{epoch}.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a60fd-b6fe-4f37-9dea-75091bdeaf20",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3656ce-2abd-4508-9594-2a72718a379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "x_dict, y_dict = make_all_data(inds, train_cfg[\"dataset\"][\"setting\"], grp_cfg[\"passing\"][\"default\"], logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3629ce8-7854-4936-8136-7250fa3fe047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "np.random.seed(train_cfg[\"dataset\"][\"random_seed\"])\n",
    "random_keys = np.random.choice(\n",
    "    list(x_dict.keys()),\n",
    "    size=len(x_dict),\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "train_ratio = train_cfg[\"dataset\"][\"train_ratio\"] + train_cfg[\"dataset\"][\"val_ratio\"]\n",
    "train_len = int(len(x_dict) * train_ratio)\n",
    "\n",
    "train_keys = random_keys[:train_len]\n",
    "test_keys = random_keys[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc30e4-b08b-4287-8b07-f8effaaebe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x_lst, y_lst, pred, seq_len=30, path=None):\n",
    "    x_lst = [[0 for _ in range(x_lst.shape[1])]] + [[np.nan for _ in range(x_lst.shape[1])] for i in range(seq_len - 1)] + x_lst.tolist()\n",
    "    y_lst = [0] + [np.nan for i in range(seq_len - 1)] + y_lst\n",
    "    pred = [0] + [np.nan for i in range(seq_len - 1)] + pred.tolist()\n",
    "    \n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    ax = fig.add_axes((0.04, 0.17, 0.80, 0.81))\n",
    "    \n",
    "    ax.plot(pred, label='pred')\n",
    "    ax.plot(y_lst, linestyle=':', label='ground truth')\n",
    "    for i, feature in enumerate(np.array(x_lst).T):\n",
    "        ax.plot(feature, alpha=0.4, label=columns[i])\n",
    "\n",
    "    ax.set_ylim((-0.05, 1.05))\n",
    "    ax.set_xlabel('frame')\n",
    "    ax.legend(\n",
    "        bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0,\n",
    "        fontsize=20, handlelength=0.8, handletextpad=0.2\n",
    "    )\n",
    "    \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    if path is not None:\n",
    "        fig.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a72bc4-e0f7-42e4-946d-0727cdf210a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '02_06_1_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f9e0e-6b00-4dac-9641-1088126b63c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_all_train = []\n",
    "pred_all_train = []\n",
    "y_eve_train = []\n",
    "pred_eve_train = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in train_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "        \n",
    "        x, _ = create_sequence(x_lst, y_lst, **config)\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "        \n",
    "        if len(x) == 0:\n",
    "            continue\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[SEQ_LEN - 1:]\n",
    "        y_lst = y_lst[SEQ_LEN - 1:]\n",
    "            \n",
    "        y_all_train += y_lst\n",
    "        pred_all_train += pred.tolist()\n",
    "        y_eve_train.append(1 in y_lst)\n",
    "        pred_eve_train.append(1 in pred.tolist())\n",
    "        \n",
    "        if 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(\"data\", \"image\", \"passing\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, config[\"seq_len\"], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24e519-53ca-405c-a4ed-5bc31fc7d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_train, pred_all_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_train, pred_all_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_train, pred_all_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_train, pred_all_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388e620-e79f-4a62-9530-ba821dcb9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_train, pred_eve_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_train, pred_eve_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_train, pred_eve_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_train, pred_eve_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445c2a5-4a0f-49c4-9ef1-97983f804401",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '08_03_2_5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4075ac9-b3b6-460b-9cc2-dac368fed21f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_all_test = []\n",
    "pred_all_test = []\n",
    "y_eve_test = []\n",
    "pred_eve_test = []\n",
    "tn, fn = 0, 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in test_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "\n",
    "        x, _ = create_sequence(x_lst, y_lst, **config)\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "\n",
    "        if len(x) == 0:\n",
    "            tn += 1\n",
    "            continue\n",
    "            \n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[SEQ_LEN - 1:]\n",
    "        y_lst = y_lst[SEQ_LEN - 1:]\n",
    "        \n",
    "        y_all_test += y_lst\n",
    "        pred_all_test += pred.tolist()\n",
    "        y_eve_test.append(1 in y_lst)\n",
    "        pred_eve_test.append(1 in pred.tolist())\n",
    "        if 1 not in y_lst:\n",
    "            if 1 not in pred:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        \n",
    "        if 1 not in pred and 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(common.data_dir, \"image\", \"passing\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, config[\"seq_len\"], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125394d0-a266-4220-9cab-a86b943a9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_test, pred_all_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_test, pred_all_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_test, pred_all_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_test, pred_all_test)))\n",
    "\n",
    "cm = confusion_matrix(y_all_test, pred_all_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa66db1-dae9-433e-a64b-803a7d9f02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_test, pred_eve_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_test, pred_eve_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_test, pred_eve_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_test, pred_eve_test)))\n",
    "\n",
    "print('true negative:', tn)\n",
    "print('false negative:', fn)\n",
    "\n",
    "cm = confusion_matrix(y_eve_test, pred_eve_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30255301-5894-4c67-961f-6123eff18fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
