{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/research/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# back to project root\n",
    "%cd /mnt/c/research\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch import nn, optim\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from group.passing.dataset import make_data_loaders, make_all_data\n",
    "from group.passing.lstm_model import LSTMModel\n",
    "from utility.activity_loader import load_individuals\n",
    "from utility.logger import logger\n",
    "from tools.train_passing import init_model, init_loss, init_optim, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc298add-7d1e-4c1a-8a98-767bde6e9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams['xtick.direction'] = 'in'  # x axis in\n",
    "plt.rcParams['ytick.direction'] = 'in'  # y axis in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28083f0-6a29-4348-b2f0-6099f5289d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f8937b-15e0-489c-adfe-fc605429bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"config/passing/pass_train.yaml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    train_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"individual\"], \"r\") as f:\n",
    "    ind_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"group\"], \"r\") as f:\n",
    "    grp_cfg = yaml.safe_load(f)\n",
    "mdl_cfg_path = grp_cfg[\"passing\"][\"cfg_path\"]\n",
    "with open(mdl_cfg_path, \"r\") as f:\n",
    "    mdl_cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe7d65b-260f-4091-b271-5cf80ff3c575",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 16:18:32 [INFO]: => loading individuals from {'02_001': ['data/02/001/passing/01', 'data/02/001/passing/02', 'data/02/001/passing/03', 'data/02/001/passing/04', 'data/02/001/passing/05', 'data/02/001/passing/06', 'data/02/001/passing/07', 'data/02/001/passing/08', 'data/02/001/passing/09', 'data/02/001/passing/10', 'data/02/001/passing/11', 'data/02/001/passing/12', 'data/02/001/passing/13', 'data/02/001/passing/14', 'data/02/001/passing/15', 'data/02/001/passing/16', 'data/02/001/passing/17', 'data/02/001/passing/18', 'data/02/001/passing/19', 'data/02/001/passing/20', 'data/02/001/passing/21', 'data/02/001/passing/22', 'data/02/001/passing/23'], '08_001': ['data/08/001/passing/01', 'data/08/001/passing/02', 'data/08/001/passing/03', 'data/08/001/passing/04', 'data/08/001/passing/05', 'data/08/001/passing/06', 'data/08/001/passing/07', 'data/08/001/passing/08', 'data/08/001/passing/09', 'data/08/001/passing/10', 'data/08/001/passing/11', 'data/08/001/passing/12', 'data/08/001/passing/13', 'data/08/001/passing/14', 'data/08/001/passing/15', 'data/08/001/passing/16', 'data/08/001/passing/17', 'data/08/001/passing/18', 'data/08/001/passing/19', 'data/08/001/passing/20', 'data/08/001/passing/21', 'data/08/001/passing/22', 'data/08/001/passing/23', 'data/08/001/passing/24', 'data/08/001/passing/25', 'data/08/001/passing/26', 'data/08/001/passing/27', 'data/08/001/passing/28', 'data/08/001/passing/29', 'data/08/001/passing/30', 'data/08/001/passing/31', 'data/08/001/passing/32', 'data/08/001/passing/33', 'data/08/001/passing/34', 'data/08/001/passing/35', 'data/08/001/passing/36', 'data/08/001/passing/37', 'data/08/001/passing/38', 'data/08/001/passing/39', 'data/08/001/passing/40', 'data/08/001/passing/41'], '09_001': ['data/09/001/passing/01', 'data/09/001/passing/02', 'data/09/001/passing/03', 'data/09/001/passing/04', 'data/09/001/passing/05', 'data/09/001/passing/06', 'data/09/001/passing/07', 'data/09/001/passing/08', 'data/09/001/passing/09']}\n"
     ]
    }
   ],
   "source": [
    "data_dirs_all = {}\n",
    "for room_num, surgery_items in train_cfg[\"dataset\"][\"setting\"].items():\n",
    "    for surgery_num in surgery_items.keys():\n",
    "        dirs = sorted(glob(os.path.join(\"data\", room_num, surgery_num, \"passing\", \"*\")))\n",
    "        data_dirs_all[f\"{room_num}_{surgery_num}\"] = dirs\n",
    "\n",
    "logger.info(f\"=> loading individuals from {data_dirs_all}\")\n",
    "inds = {}\n",
    "for key_prefix, dirs in data_dirs_all.items():\n",
    "    for model_path in dirs:\n",
    "        num = model_path.split(\"/\")[-1]\n",
    "        json_path = os.path.join(model_path, \".json\", \"individual.json\")\n",
    "        tmp_inds, _ = load_individuals(json_path, ind_cfg)\n",
    "        for pid, ind in tmp_inds.items():\n",
    "            inds[f\"{key_prefix}_{num}_{pid}\"] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1653e85d-ed47-4ac8-9885-88b45f769be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = init_model(mdl_cfg, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd9b3a-c263-4b7b-980f-f9c1f4e9a2c8",
   "metadata": {},
   "source": [
    "# 深層学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b443de49-2306-476a-b2bb-295506a68dc5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 16:18:44 [INFO]: => createing time series 02_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  8.59it/s]\n",
      "2022-05-24 16:18:47 [INFO]: => createing time series 08_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 14.50it/s]\n",
      "2022-05-24 16:18:49 [INFO]: => createing time series 09_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  7.73it/s]\n",
      "2022-05-24 16:18:51 [INFO]: => extracting feature 02_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:10<00:00,  2.15it/s]\n",
      "2022-05-24 16:19:01 [INFO]: => extracting feature 08_001\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:09<00:00,  4.29it/s]\n",
      "2022-05-24 16:19:11 [INFO]: => extracting feature 09_001\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.39it/s]\n",
      "2022-05-24 16:19:17 [INFO]: => create dataset\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4052/4052 [00:00<00:00, 32186.28it/s]\n",
      "2022-05-24 16:19:17 [INFO]: => create dataset\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-24 16:19:17 [INFO]: => create dataset\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1013/1013 [00:00<00:00, 2476.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data loader\n",
    "dataset_cfg = train_cfg[\"dataset\"]\n",
    "passing_defs = grp_cfg[\"passing\"][\"default\"]\n",
    "train_loader, val_loader, test_loader = make_data_loaders(\n",
    "    inds, dataset_cfg, passing_defs, logger\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5821afa-ee6c-4452-92ca-88f99418a04f",
   "metadata": {},
   "source": [
    "# init optimizer\n",
    "criterion = init_loss(cfg[\"optim\"][\"pos_weight\"], detector.device)\n",
    "optimizer, scheduler = init_optim(\n",
    "    detector.model, cfg[\"optim\"][\"lr\"], cfg[\"optim\"][\"lr_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e449ab4-8dcb-493a-8106-c55f43d5b87f",
   "metadata": {
    "tags": []
   },
   "source": [
    "epoch_len = cfg[\"optim\"][\"epoch\"]\n",
    "detector.model, epoch, history = train(\n",
    "    detector.model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epoch_len,\n",
    "    logger,\n",
    ")\n",
    "acc, pre, rcl, f1 = test(detector.model, test_loader)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21117b82-420d-4c4d-9710-77dc143abee5",
   "metadata": {},
   "source": [
    "plt.plot(history['train'])\n",
    "plt.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Loss over training epochs')\n",
    "plt.legend(['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7b174-dd28-4aa8-8d26-9bc4427a0163",
   "metadata": {},
   "source": [
    "# グリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60011bb5-8a2a-497d-8f5e-a1fe6b56b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_rnns': [1, 2],\n",
    "    'rnn_hidden_dim': [128, 256, 512],\n",
    "    'pos_weight': [8, 16]\n",
    "}\n",
    "epoch_len = train_cfg[\"optim\"][\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d9a76f-fd7b-4bde-8e77-33dba46fd26a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 09:47:50 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 09:48:23 [INFO]: Epoch[1/50] train loss: 0.54059, val loss: nan, lr: 0.0010000, time: 32.72\n",
      "2022-05-24 09:48:55 [INFO]: Epoch[2/50] train loss: 0.48087, val loss: nan, lr: 0.0010000, time: 31.55\n",
      "2022-05-24 09:49:26 [INFO]: Epoch[3/50] train loss: 0.45065, val loss: nan, lr: 0.0010000, time: 31.19\n",
      "2022-05-24 09:49:57 [INFO]: Epoch[4/50] train loss: 0.43386, val loss: nan, lr: 0.0010000, time: 31.49\n",
      "2022-05-24 09:50:29 [INFO]: Epoch[5/50] train loss: 0.42408, val loss: nan, lr: 0.0010000, time: 31.38\n",
      "2022-05-24 09:51:00 [INFO]: Epoch[6/50] train loss: 0.41802, val loss: nan, lr: 0.0010000, time: 31.62\n",
      "2022-05-24 09:51:32 [INFO]: Epoch[7/50] train loss: 0.41342, val loss: nan, lr: 0.0010000, time: 31.48\n",
      "2022-05-24 09:52:03 [INFO]: Epoch[8/50] train loss: 0.41050, val loss: nan, lr: 0.0010000, time: 31.38\n",
      "2022-05-24 09:52:34 [INFO]: Epoch[9/50] train loss: 0.40790, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 09:53:05 [INFO]: Epoch[10/50] train loss: 0.40586, val loss: nan, lr: 0.0010000, time: 31.10\n",
      "2022-05-24 09:53:36 [INFO]: Epoch[11/50] train loss: 0.40496, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 09:54:07 [INFO]: Epoch[12/50] train loss: 0.40581, val loss: nan, lr: 0.0010000, time: 30.57\n",
      "2022-05-24 09:54:38 [INFO]: Epoch[13/50] train loss: 0.40224, val loss: nan, lr: 0.0010000, time: 31.30\n",
      "2022-05-24 09:55:09 [INFO]: Epoch[14/50] train loss: 0.40110, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 09:55:41 [INFO]: Epoch[15/50] train loss: 0.39935, val loss: nan, lr: 0.0010000, time: 31.89\n",
      "2022-05-24 09:56:13 [INFO]: Epoch[16/50] train loss: 0.39770, val loss: nan, lr: 0.0010000, time: 31.78\n",
      "2022-05-24 09:56:44 [INFO]: Epoch[17/50] train loss: 0.40022, val loss: nan, lr: 0.0010000, time: 31.31\n",
      "2022-05-24 09:57:16 [INFO]: Epoch[18/50] train loss: 0.39743, val loss: nan, lr: 0.0010000, time: 31.27\n",
      "2022-05-24 09:57:47 [INFO]: Epoch[19/50] train loss: 0.39612, val loss: nan, lr: 0.0010000, time: 31.58\n",
      "2022-05-24 09:58:18 [INFO]: Epoch[20/50] train loss: 0.39487, val loss: nan, lr: 0.0010000, time: 31.14\n",
      "2022-05-24 09:58:50 [INFO]: Epoch[21/50] train loss: 0.39396, val loss: nan, lr: 0.0010000, time: 31.42\n",
      "2022-05-24 09:59:21 [INFO]: Epoch[22/50] train loss: 0.39305, val loss: nan, lr: 0.0010000, time: 31.42\n",
      "2022-05-24 09:59:53 [INFO]: Epoch[23/50] train loss: 0.39147, val loss: nan, lr: 0.0010000, time: 31.75\n",
      "2022-05-24 10:00:25 [INFO]: Epoch[24/50] train loss: 0.39100, val loss: nan, lr: 0.0010000, time: 31.98\n",
      "2022-05-24 10:00:56 [INFO]: Epoch[25/50] train loss: 0.38883, val loss: nan, lr: 0.0010000, time: 31.49\n",
      "2022-05-24 10:01:27 [INFO]: Epoch[26/50] train loss: 0.38929, val loss: nan, lr: 0.0010000, time: 30.81\n",
      "2022-05-24 10:01:58 [INFO]: Epoch[27/50] train loss: 0.38980, val loss: nan, lr: 0.0010000, time: 31.19\n",
      "2022-05-24 10:02:29 [INFO]: Epoch[28/50] train loss: 0.38636, val loss: nan, lr: 0.0010000, time: 30.75\n",
      "2022-05-24 10:03:00 [INFO]: Epoch[29/50] train loss: 0.38498, val loss: nan, lr: 0.0010000, time: 31.17\n",
      "2022-05-24 10:03:32 [INFO]: Epoch[30/50] train loss: 0.38467, val loss: nan, lr: 0.0010000, time: 31.35\n",
      "2022-05-24 10:04:03 [INFO]: Epoch[31/50] train loss: 0.38407, val loss: nan, lr: 0.0010000, time: 31.03\n",
      "2022-05-24 10:04:34 [INFO]: Epoch[32/50] train loss: 0.38202, val loss: nan, lr: 0.0010000, time: 31.01\n",
      "2022-05-24 10:05:05 [INFO]: Epoch[33/50] train loss: 0.38377, val loss: nan, lr: 0.0010000, time: 31.39\n",
      "2022-05-24 10:05:36 [INFO]: Epoch[34/50] train loss: 0.38315, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 10:06:07 [INFO]: Epoch[35/50] train loss: 0.38108, val loss: nan, lr: 0.0010000, time: 30.55\n",
      "2022-05-24 10:06:38 [INFO]: Epoch[36/50] train loss: 0.38179, val loss: nan, lr: 0.0010000, time: 30.83\n",
      "2022-05-24 10:07:09 [INFO]: Epoch[37/50] train loss: 0.38020, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 10:07:41 [INFO]: Epoch[38/50] train loss: 0.38044, val loss: nan, lr: 0.0010000, time: 31.88\n",
      "2022-05-24 10:08:11 [INFO]: Epoch[39/50] train loss: 0.37773, val loss: nan, lr: 0.0010000, time: 30.92\n",
      "2022-05-24 10:08:43 [INFO]: Epoch[40/50] train loss: 0.37779, val loss: nan, lr: 0.0010000, time: 31.28\n",
      "2022-05-24 10:09:14 [INFO]: Epoch[41/50] train loss: 0.37811, val loss: nan, lr: 0.0010000, time: 31.31\n",
      "2022-05-24 10:09:45 [INFO]: Epoch[42/50] train loss: 0.37729, val loss: nan, lr: 0.0010000, time: 31.44\n",
      "2022-05-24 10:10:17 [INFO]: Epoch[43/50] train loss: 0.37623, val loss: nan, lr: 0.0010000, time: 31.50\n",
      "2022-05-24 10:10:48 [INFO]: Epoch[44/50] train loss: 0.37631, val loss: nan, lr: 0.0010000, time: 31.38\n",
      "2022-05-24 10:11:20 [INFO]: Epoch[45/50] train loss: 0.37832, val loss: nan, lr: 0.0010000, time: 31.41\n",
      "2022-05-24 10:11:51 [INFO]: Epoch[46/50] train loss: 0.37795, val loss: nan, lr: 0.0010000, time: 31.59\n",
      "2022-05-24 10:12:23 [INFO]: Epoch[47/50] train loss: 0.37719, val loss: nan, lr: 0.0010000, time: 31.39\n",
      "2022-05-24 10:12:54 [INFO]: Epoch[48/50] train loss: 0.37796, val loss: nan, lr: 0.0010000, time: 31.06\n",
      "2022-05-24 10:13:25 [INFO]: Epoch[49/50] train loss: 0.37644, val loss: nan, lr: 0.0010000, time: 31.10\n",
      "2022-05-24 10:13:56 [INFO]: Epoch[50/50] train loss: 0.37481, val loss: nan, lr: 0.0010000, time: 31.30\n",
      "2022-05-24 10:13:56 [INFO]: => end training\n",
      "2022-05-24 10:13:56 [INFO]: => calculating train scores\n",
      "2022-05-24 10:14:32 [INFO]: => train score\n",
      "accuracy: 0.9942536771504734\n",
      "presision: 0.565955766192733\n",
      "recall: 0.8911691542288557\n",
      "f1: 0.6922705314009663\n",
      "2022-05-24 10:14:32 [INFO]: => calculating test scores\n",
      "2022-05-24 10:15:22 [INFO]: => test score\n",
      "accuracy: 0.9902566165794952\n",
      "presision: 0.21875\n",
      "recall: 0.4163568773234201\n",
      "f1: 0.28681177976952626\n",
      "2022-05-24 10:15:22 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 10:15:53 [INFO]: Epoch[1/50] train loss: 0.75312, val loss: nan, lr: 0.0010000, time: 31.34\n",
      "2022-05-24 10:16:24 [INFO]: Epoch[2/50] train loss: 0.63875, val loss: nan, lr: 0.0010000, time: 30.70\n",
      "2022-05-24 10:16:55 [INFO]: Epoch[3/50] train loss: 0.55906, val loss: nan, lr: 0.0010000, time: 30.83\n",
      "2022-05-24 10:17:26 [INFO]: Epoch[4/50] train loss: 0.51497, val loss: nan, lr: 0.0010000, time: 31.49\n",
      "2022-05-24 10:17:58 [INFO]: Epoch[5/50] train loss: 0.49021, val loss: nan, lr: 0.0010000, time: 31.57\n",
      "2022-05-24 10:18:30 [INFO]: Epoch[6/50] train loss: 0.47466, val loss: nan, lr: 0.0010000, time: 31.50\n",
      "2022-05-24 10:19:01 [INFO]: Epoch[7/50] train loss: 0.46723, val loss: nan, lr: 0.0010000, time: 31.74\n",
      "2022-05-24 10:19:32 [INFO]: Epoch[8/50] train loss: 0.45731, val loss: nan, lr: 0.0010000, time: 31.12\n",
      "2022-05-24 10:20:04 [INFO]: Epoch[9/50] train loss: 0.45331, val loss: nan, lr: 0.0010000, time: 31.23\n",
      "2022-05-24 10:20:35 [INFO]: Epoch[10/50] train loss: 0.44753, val loss: nan, lr: 0.0010000, time: 31.40\n",
      "2022-05-24 10:21:07 [INFO]: Epoch[11/50] train loss: 0.44559, val loss: nan, lr: 0.0010000, time: 31.59\n",
      "2022-05-24 10:21:38 [INFO]: Epoch[12/50] train loss: 0.44037, val loss: nan, lr: 0.0010000, time: 31.51\n",
      "2022-05-24 10:22:10 [INFO]: Epoch[13/50] train loss: 0.44017, val loss: nan, lr: 0.0010000, time: 31.95\n",
      "2022-05-24 10:22:42 [INFO]: Epoch[14/50] train loss: 0.43710, val loss: nan, lr: 0.0010000, time: 31.77\n",
      "2022-05-24 10:23:13 [INFO]: Epoch[15/50] train loss: 0.43839, val loss: nan, lr: 0.0010000, time: 30.71\n",
      "2022-05-24 10:23:43 [INFO]: Epoch[16/50] train loss: 0.43186, val loss: nan, lr: 0.0010000, time: 30.81\n",
      "2022-05-24 10:24:15 [INFO]: Epoch[17/50] train loss: 0.42890, val loss: nan, lr: 0.0010000, time: 31.52\n",
      "2022-05-24 10:24:46 [INFO]: Epoch[18/50] train loss: 0.42887, val loss: nan, lr: 0.0010000, time: 31.25\n",
      "2022-05-24 10:25:17 [INFO]: Epoch[19/50] train loss: 0.42663, val loss: nan, lr: 0.0010000, time: 30.87\n",
      "2022-05-24 10:25:48 [INFO]: Epoch[20/50] train loss: 0.42643, val loss: nan, lr: 0.0010000, time: 30.53\n",
      "2022-05-24 10:26:19 [INFO]: Epoch[21/50] train loss: 0.41910, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 10:26:50 [INFO]: Epoch[22/50] train loss: 0.41874, val loss: nan, lr: 0.0010000, time: 31.44\n",
      "2022-05-24 10:27:21 [INFO]: Epoch[23/50] train loss: 0.42066, val loss: nan, lr: 0.0010000, time: 31.03\n",
      "2022-05-24 10:27:53 [INFO]: Epoch[24/50] train loss: 0.41854, val loss: nan, lr: 0.0010000, time: 31.93\n",
      "2022-05-24 10:28:25 [INFO]: Epoch[25/50] train loss: 0.41591, val loss: nan, lr: 0.0010000, time: 31.48\n",
      "2022-05-24 10:28:57 [INFO]: Epoch[26/50] train loss: 0.43514, val loss: nan, lr: 0.0010000, time: 32.08\n",
      "2022-05-24 10:29:28 [INFO]: Epoch[27/50] train loss: 0.42395, val loss: nan, lr: 0.0010000, time: 31.51\n",
      "2022-05-24 10:30:00 [INFO]: Epoch[28/50] train loss: 0.41695, val loss: nan, lr: 0.0010000, time: 31.65\n",
      "2022-05-24 10:30:31 [INFO]: Epoch[29/50] train loss: 0.42373, val loss: nan, lr: 0.0010000, time: 30.82\n",
      "2022-05-24 10:31:03 [INFO]: Epoch[30/50] train loss: 0.41539, val loss: nan, lr: 0.0010000, time: 31.94\n",
      "2022-05-24 10:31:35 [INFO]: Epoch[31/50] train loss: 0.41883, val loss: nan, lr: 0.0010000, time: 32.41\n",
      "2022-05-24 10:32:07 [INFO]: Epoch[32/50] train loss: 0.41196, val loss: nan, lr: 0.0010000, time: 31.71\n",
      "2022-05-24 10:32:39 [INFO]: Epoch[33/50] train loss: 0.41074, val loss: nan, lr: 0.0010000, time: 32.59\n",
      "2022-05-24 10:33:10 [INFO]: Epoch[34/50] train loss: 0.41056, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 10:33:42 [INFO]: Epoch[35/50] train loss: 0.40725, val loss: nan, lr: 0.0010000, time: 31.57\n",
      "2022-05-24 10:34:14 [INFO]: Epoch[36/50] train loss: 0.40962, val loss: nan, lr: 0.0010000, time: 31.74\n",
      "2022-05-24 10:34:45 [INFO]: Epoch[37/50] train loss: 0.40887, val loss: nan, lr: 0.0010000, time: 31.71\n",
      "2022-05-24 10:35:16 [INFO]: Epoch[38/50] train loss: 0.41143, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 10:35:47 [INFO]: Epoch[39/50] train loss: 0.41920, val loss: nan, lr: 0.0010000, time: 30.91\n",
      "2022-05-24 10:36:19 [INFO]: Epoch[40/50] train loss: 0.40902, val loss: nan, lr: 0.0010000, time: 31.27\n",
      "2022-05-24 10:36:50 [INFO]: Epoch[41/50] train loss: 0.41232, val loss: nan, lr: 0.0010000, time: 31.57\n",
      "2022-05-24 10:37:21 [INFO]: Epoch[42/50] train loss: 0.40473, val loss: nan, lr: 0.0010000, time: 31.25\n",
      "2022-05-24 10:37:52 [INFO]: Epoch[43/50] train loss: 0.40689, val loss: nan, lr: 0.0010000, time: 30.77\n",
      "2022-05-24 10:38:23 [INFO]: Epoch[44/50] train loss: 0.40305, val loss: nan, lr: 0.0010000, time: 31.27\n",
      "2022-05-24 10:38:55 [INFO]: Epoch[45/50] train loss: 0.40560, val loss: nan, lr: 0.0010000, time: 31.62\n",
      "2022-05-24 10:39:26 [INFO]: Epoch[46/50] train loss: 0.40518, val loss: nan, lr: 0.0010000, time: 30.83\n",
      "2022-05-24 10:39:58 [INFO]: Epoch[47/50] train loss: 0.40456, val loss: nan, lr: 0.0010000, time: 31.88\n",
      "2022-05-24 10:40:29 [INFO]: Epoch[48/50] train loss: 0.40240, val loss: nan, lr: 0.0010000, time: 31.34\n",
      "2022-05-24 10:41:00 [INFO]: Epoch[49/50] train loss: 0.40088, val loss: nan, lr: 0.0010000, time: 31.33\n",
      "2022-05-24 10:41:32 [INFO]: Epoch[50/50] train loss: 0.40548, val loss: nan, lr: 0.0010000, time: 31.67\n",
      "2022-05-24 10:41:32 [INFO]: => end training\n",
      "2022-05-24 10:41:32 [INFO]: => calculating train scores\n",
      "2022-05-24 10:42:09 [INFO]: => train score\n",
      "accuracy: 0.991168524223412\n",
      "presision: 0.44900932400932403\n",
      "recall: 0.9583333333333334\n",
      "f1: 0.6115079365079366\n",
      "2022-05-24 10:42:09 [INFO]: => calculating test scores\n",
      "2022-05-24 10:43:00 [INFO]: => test score\n",
      "accuracy: 0.9876152325642417\n",
      "presision: 0.2021709633649932\n",
      "recall: 0.5539033457249071\n",
      "f1: 0.2962226640159046\n",
      "2022-05-24 10:43:00 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 10:43:33 [INFO]: Epoch[1/50] train loss: 0.73985, val loss: nan, lr: 0.0010000, time: 32.59\n",
      "2022-05-24 10:44:04 [INFO]: Epoch[2/50] train loss: 0.61264, val loss: nan, lr: 0.0010000, time: 31.73\n",
      "2022-05-24 10:44:37 [INFO]: Epoch[3/50] train loss: 0.53920, val loss: nan, lr: 0.0010000, time: 32.82\n",
      "2022-05-24 10:45:10 [INFO]: Epoch[4/50] train loss: 0.49274, val loss: nan, lr: 0.0010000, time: 33.36\n",
      "2022-05-24 10:45:44 [INFO]: Epoch[5/50] train loss: 0.46298, val loss: nan, lr: 0.0010000, time: 33.46\n",
      "2022-05-24 10:46:16 [INFO]: Epoch[6/50] train loss: 0.44353, val loss: nan, lr: 0.0010000, time: 32.22\n",
      "2022-05-24 10:46:49 [INFO]: Epoch[7/50] train loss: 0.43229, val loss: nan, lr: 0.0010000, time: 33.02\n",
      "2022-05-24 10:47:21 [INFO]: Epoch[8/50] train loss: 0.42516, val loss: nan, lr: 0.0010000, time: 32.09\n",
      "2022-05-24 10:47:54 [INFO]: Epoch[9/50] train loss: 0.41946, val loss: nan, lr: 0.0010000, time: 32.54\n",
      "2022-05-24 10:48:27 [INFO]: Epoch[10/50] train loss: 0.41419, val loss: nan, lr: 0.0010000, time: 33.37\n",
      "2022-05-24 10:48:59 [INFO]: Epoch[11/50] train loss: 0.41164, val loss: nan, lr: 0.0010000, time: 32.02\n",
      "2022-05-24 10:49:32 [INFO]: Epoch[12/50] train loss: 0.40933, val loss: nan, lr: 0.0010000, time: 32.92\n",
      "2022-05-24 10:50:05 [INFO]: Epoch[13/50] train loss: 0.40621, val loss: nan, lr: 0.0010000, time: 32.51\n",
      "2022-05-24 10:50:37 [INFO]: Epoch[14/50] train loss: 0.40534, val loss: nan, lr: 0.0010000, time: 32.38\n",
      "2022-05-24 10:51:10 [INFO]: Epoch[15/50] train loss: 0.40275, val loss: nan, lr: 0.0010000, time: 33.22\n",
      "2022-05-24 10:51:43 [INFO]: Epoch[16/50] train loss: 0.40119, val loss: nan, lr: 0.0010000, time: 32.29\n",
      "2022-05-24 10:52:15 [INFO]: Epoch[17/50] train loss: 0.40181, val loss: nan, lr: 0.0010000, time: 32.88\n",
      "2022-05-24 10:52:49 [INFO]: Epoch[18/50] train loss: 0.39802, val loss: nan, lr: 0.0010000, time: 33.26\n",
      "2022-05-24 10:53:21 [INFO]: Epoch[19/50] train loss: 0.39713, val loss: nan, lr: 0.0010000, time: 32.41\n",
      "2022-05-24 10:53:53 [INFO]: Epoch[20/50] train loss: 0.39674, val loss: nan, lr: 0.0010000, time: 31.92\n",
      "2022-05-24 10:54:25 [INFO]: Epoch[21/50] train loss: 0.39492, val loss: nan, lr: 0.0010000, time: 31.62\n",
      "2022-05-24 10:54:56 [INFO]: Epoch[22/50] train loss: 0.39558, val loss: nan, lr: 0.0010000, time: 31.37\n",
      "2022-05-24 10:55:27 [INFO]: Epoch[23/50] train loss: 0.39225, val loss: nan, lr: 0.0010000, time: 31.06\n",
      "2022-05-24 10:55:58 [INFO]: Epoch[24/50] train loss: 0.39130, val loss: nan, lr: 0.0010000, time: 30.86\n",
      "2022-05-24 10:56:29 [INFO]: Epoch[25/50] train loss: 0.39094, val loss: nan, lr: 0.0010000, time: 31.41\n",
      "2022-05-24 10:57:01 [INFO]: Epoch[26/50] train loss: 0.39290, val loss: nan, lr: 0.0010000, time: 31.96\n",
      "2022-05-24 10:57:33 [INFO]: Epoch[27/50] train loss: 0.39045, val loss: nan, lr: 0.0010000, time: 31.59\n",
      "2022-05-24 10:58:05 [INFO]: Epoch[28/50] train loss: 0.38821, val loss: nan, lr: 0.0010000, time: 32.33\n",
      "2022-05-24 10:58:37 [INFO]: Epoch[29/50] train loss: 0.38782, val loss: nan, lr: 0.0010000, time: 31.85\n",
      "2022-05-24 10:59:10 [INFO]: Epoch[30/50] train loss: 0.38793, val loss: nan, lr: 0.0010000, time: 32.76\n",
      "2022-05-24 10:59:42 [INFO]: Epoch[31/50] train loss: 0.38920, val loss: nan, lr: 0.0010000, time: 32.45\n",
      "2022-05-24 11:00:14 [INFO]: Epoch[32/50] train loss: 0.38561, val loss: nan, lr: 0.0010000, time: 32.23\n",
      "2022-05-24 11:00:47 [INFO]: Epoch[33/50] train loss: 0.38760, val loss: nan, lr: 0.0010000, time: 32.25\n",
      "2022-05-24 11:01:19 [INFO]: Epoch[34/50] train loss: 0.38573, val loss: nan, lr: 0.0010000, time: 32.51\n",
      "2022-05-24 11:01:51 [INFO]: Epoch[35/50] train loss: 0.38460, val loss: nan, lr: 0.0010000, time: 31.90\n",
      "2022-05-24 11:02:22 [INFO]: Epoch[36/50] train loss: 0.38280, val loss: nan, lr: 0.0010000, time: 31.33\n",
      "2022-05-24 11:02:54 [INFO]: Epoch[37/50] train loss: 0.38314, val loss: nan, lr: 0.0010000, time: 31.47\n",
      "2022-05-24 11:03:25 [INFO]: Epoch[38/50] train loss: 0.38310, val loss: nan, lr: 0.0010000, time: 31.09\n",
      "2022-05-24 11:03:56 [INFO]: Epoch[39/50] train loss: 0.38278, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 11:04:28 [INFO]: Epoch[40/50] train loss: 0.38265, val loss: nan, lr: 0.0010000, time: 31.55\n",
      "2022-05-24 11:04:59 [INFO]: Epoch[41/50] train loss: 0.37986, val loss: nan, lr: 0.0010000, time: 31.40\n",
      "2022-05-24 11:05:31 [INFO]: Epoch[42/50] train loss: 0.38072, val loss: nan, lr: 0.0010000, time: 32.11\n",
      "2022-05-24 11:06:03 [INFO]: Epoch[43/50] train loss: 0.38048, val loss: nan, lr: 0.0010000, time: 31.74\n",
      "2022-05-24 11:06:35 [INFO]: Epoch[44/50] train loss: 0.38237, val loss: nan, lr: 0.0010000, time: 31.63\n",
      "2022-05-24 11:07:06 [INFO]: Epoch[45/50] train loss: 0.37911, val loss: nan, lr: 0.0010000, time: 31.71\n",
      "2022-05-24 11:07:38 [INFO]: Epoch[46/50] train loss: 0.37884, val loss: nan, lr: 0.0010000, time: 31.69\n",
      "2022-05-24 11:08:10 [INFO]: Epoch[47/50] train loss: 0.37857, val loss: nan, lr: 0.0010000, time: 31.80\n",
      "2022-05-24 11:08:41 [INFO]: Epoch[48/50] train loss: 0.37974, val loss: nan, lr: 0.0010000, time: 31.50\n",
      "2022-05-24 11:09:13 [INFO]: Epoch[49/50] train loss: 0.37740, val loss: nan, lr: 0.0010000, time: 31.38\n",
      "2022-05-24 11:09:46 [INFO]: Epoch[50/50] train loss: 0.37929, val loss: nan, lr: 0.0010000, time: 32.94\n",
      "2022-05-24 11:09:46 [INFO]: => end training\n",
      "2022-05-24 11:09:46 [INFO]: => calculating train scores\n",
      "2022-05-24 11:10:18 [INFO]: => train score\n",
      "accuracy: 0.9944746895677629\n",
      "presision: 0.5814547001276053\n",
      "recall: 0.8501243781094527\n",
      "f1: 0.6905784288961859\n",
      "2022-05-24 11:10:18 [INFO]: => calculating test scores\n",
      "2022-05-24 11:11:08 [INFO]: => test score\n",
      "accuracy: 0.9926880892822783\n",
      "presision: 0.3279445727482679\n",
      "recall: 0.5278810408921933\n",
      "f1: 0.4045584045584046\n",
      "2022-05-24 11:11:08 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:11:40 [INFO]: Epoch[1/50] train loss: 0.69209, val loss: nan, lr: 0.0010000, time: 32.10\n",
      "2022-05-24 11:12:12 [INFO]: Epoch[2/50] train loss: 0.59096, val loss: nan, lr: 0.0010000, time: 31.95\n",
      "2022-05-24 11:12:45 [INFO]: Epoch[3/50] train loss: 0.53709, val loss: nan, lr: 0.0010000, time: 32.69\n",
      "2022-05-24 11:13:18 [INFO]: Epoch[4/50] train loss: 0.51286, val loss: nan, lr: 0.0010000, time: 32.63\n",
      "2022-05-24 11:13:49 [INFO]: Epoch[5/50] train loss: 0.48657, val loss: nan, lr: 0.0010000, time: 31.77\n",
      "2022-05-24 11:14:21 [INFO]: Epoch[6/50] train loss: 0.47120, val loss: nan, lr: 0.0010000, time: 32.00\n",
      "2022-05-24 11:14:54 [INFO]: Epoch[7/50] train loss: 0.46124, val loss: nan, lr: 0.0010000, time: 32.37\n",
      "2022-05-24 11:15:26 [INFO]: Epoch[8/50] train loss: 0.45697, val loss: nan, lr: 0.0010000, time: 32.66\n",
      "2022-05-24 11:15:58 [INFO]: Epoch[9/50] train loss: 0.45645, val loss: nan, lr: 0.0010000, time: 31.74\n",
      "2022-05-24 11:16:30 [INFO]: Epoch[10/50] train loss: 0.44938, val loss: nan, lr: 0.0010000, time: 31.47\n",
      "2022-05-24 11:17:02 [INFO]: Epoch[11/50] train loss: 0.44794, val loss: nan, lr: 0.0010000, time: 32.65\n",
      "2022-05-24 11:17:35 [INFO]: Epoch[12/50] train loss: 0.44310, val loss: nan, lr: 0.0010000, time: 32.75\n",
      "2022-05-24 11:18:08 [INFO]: Epoch[13/50] train loss: 0.44386, val loss: nan, lr: 0.0010000, time: 33.02\n",
      "2022-05-24 11:18:41 [INFO]: Epoch[14/50] train loss: 0.44331, val loss: nan, lr: 0.0010000, time: 32.67\n",
      "2022-05-24 11:19:13 [INFO]: Epoch[15/50] train loss: 0.44253, val loss: nan, lr: 0.0010000, time: 32.35\n",
      "2022-05-24 11:19:46 [INFO]: Epoch[16/50] train loss: 0.43799, val loss: nan, lr: 0.0010000, time: 32.51\n",
      "2022-05-24 11:20:17 [INFO]: Epoch[17/50] train loss: 0.43664, val loss: nan, lr: 0.0010000, time: 31.81\n",
      "2022-05-24 11:20:50 [INFO]: Epoch[18/50] train loss: 0.43693, val loss: nan, lr: 0.0010000, time: 32.14\n",
      "2022-05-24 11:21:21 [INFO]: Epoch[19/50] train loss: 0.43666, val loss: nan, lr: 0.0010000, time: 31.66\n",
      "2022-05-24 11:21:53 [INFO]: Epoch[20/50] train loss: 0.43773, val loss: nan, lr: 0.0010000, time: 32.07\n",
      "2022-05-24 11:22:25 [INFO]: Epoch[21/50] train loss: 0.43443, val loss: nan, lr: 0.0010000, time: 31.96\n",
      "2022-05-24 11:22:58 [INFO]: Epoch[22/50] train loss: 0.42945, val loss: nan, lr: 0.0010000, time: 32.41\n",
      "2022-05-24 11:23:30 [INFO]: Epoch[23/50] train loss: 0.42954, val loss: nan, lr: 0.0010000, time: 32.48\n",
      "2022-05-24 11:24:02 [INFO]: Epoch[24/50] train loss: 0.43194, val loss: nan, lr: 0.0010000, time: 32.30\n",
      "2022-05-24 11:24:35 [INFO]: Epoch[25/50] train loss: 0.43064, val loss: nan, lr: 0.0010000, time: 32.09\n",
      "2022-05-24 11:25:07 [INFO]: Epoch[26/50] train loss: 0.42955, val loss: nan, lr: 0.0010000, time: 32.21\n",
      "2022-05-24 11:25:39 [INFO]: Epoch[27/50] train loss: 0.42385, val loss: nan, lr: 0.0010000, time: 32.35\n",
      "2022-05-24 11:26:12 [INFO]: Epoch[28/50] train loss: 0.42795, val loss: nan, lr: 0.0010000, time: 32.61\n",
      "2022-05-24 11:26:43 [INFO]: Epoch[29/50] train loss: 0.42371, val loss: nan, lr: 0.0010000, time: 31.73\n",
      "2022-05-24 11:27:16 [INFO]: Epoch[30/50] train loss: 0.42304, val loss: nan, lr: 0.0010000, time: 32.79\n",
      "2022-05-24 11:27:48 [INFO]: Epoch[31/50] train loss: 0.42263, val loss: nan, lr: 0.0010000, time: 32.01\n",
      "2022-05-24 11:28:20 [INFO]: Epoch[32/50] train loss: 0.41928, val loss: nan, lr: 0.0010000, time: 31.71\n",
      "2022-05-24 11:28:52 [INFO]: Epoch[33/50] train loss: 0.42199, val loss: nan, lr: 0.0010000, time: 32.40\n",
      "2022-05-24 11:29:24 [INFO]: Epoch[34/50] train loss: 0.42119, val loss: nan, lr: 0.0010000, time: 31.62\n",
      "2022-05-24 11:29:57 [INFO]: Epoch[35/50] train loss: 0.41644, val loss: nan, lr: 0.0010000, time: 32.55\n",
      "2022-05-24 11:30:29 [INFO]: Epoch[36/50] train loss: 0.41813, val loss: nan, lr: 0.0010000, time: 32.24\n",
      "2022-05-24 11:31:01 [INFO]: Epoch[37/50] train loss: 0.43428, val loss: nan, lr: 0.0010000, time: 32.52\n",
      "2022-05-24 11:31:33 [INFO]: Epoch[38/50] train loss: 0.42409, val loss: nan, lr: 0.0010000, time: 31.97\n",
      "2022-05-24 11:32:05 [INFO]: Epoch[39/50] train loss: 0.42474, val loss: nan, lr: 0.0010000, time: 32.13\n",
      "2022-05-24 11:32:37 [INFO]: Epoch[40/50] train loss: 0.41665, val loss: nan, lr: 0.0010000, time: 31.34\n",
      "2022-05-24 11:33:08 [INFO]: Epoch[41/50] train loss: 0.41793, val loss: nan, lr: 0.0010000, time: 31.75\n",
      "2022-05-24 11:33:40 [INFO]: Epoch[42/50] train loss: 0.41532, val loss: nan, lr: 0.0010000, time: 31.46\n",
      "2022-05-24 11:34:12 [INFO]: Epoch[43/50] train loss: 0.42621, val loss: nan, lr: 0.0010000, time: 32.23\n",
      "2022-05-24 11:34:44 [INFO]: Epoch[44/50] train loss: 0.41186, val loss: nan, lr: 0.0010000, time: 32.25\n",
      "2022-05-24 11:35:17 [INFO]: Epoch[45/50] train loss: 0.40841, val loss: nan, lr: 0.0010000, time: 32.43\n",
      "2022-05-24 11:35:49 [INFO]: Epoch[46/50] train loss: 0.40639, val loss: nan, lr: 0.0010000, time: 32.27\n",
      "2022-05-24 11:36:21 [INFO]: Epoch[47/50] train loss: 0.41146, val loss: nan, lr: 0.0010000, time: 32.24\n",
      "2022-05-24 11:36:53 [INFO]: Epoch[48/50] train loss: 0.40961, val loss: nan, lr: 0.0010000, time: 31.83\n",
      "2022-05-24 11:37:26 [INFO]: Epoch[49/50] train loss: 0.40847, val loss: nan, lr: 0.0010000, time: 32.79\n",
      "2022-05-24 11:37:58 [INFO]: Epoch[50/50] train loss: 0.40489, val loss: nan, lr: 0.0010000, time: 32.34\n",
      "2022-05-24 11:37:58 [INFO]: => end training\n",
      "2022-05-24 11:37:58 [INFO]: => calculating train scores\n",
      "2022-05-24 11:38:31 [INFO]: => train score\n",
      "accuracy: 0.9893778725976176\n",
      "presision: 0.39908132937044044\n",
      "recall: 0.9185323383084577\n",
      "f1: 0.5564136372198154\n",
      "2022-05-24 11:38:31 [INFO]: => calculating test scores\n",
      "2022-05-24 11:39:21 [INFO]: => test score\n",
      "accuracy: 0.9861108681582031\n",
      "presision: 0.17712177121771217\n",
      "recall: 0.5353159851301115\n",
      "f1: 0.266173752310536\n",
      "2022-05-24 11:39:21 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:39:53 [INFO]: Epoch[1/50] train loss: 0.86613, val loss: nan, lr: 0.0010000, time: 32.05\n",
      "2022-05-24 11:40:25 [INFO]: Epoch[2/50] train loss: 0.68848, val loss: nan, lr: 0.0010000, time: 31.76\n",
      "2022-05-24 11:40:56 [INFO]: Epoch[3/50] train loss: 0.62213, val loss: nan, lr: 0.0010000, time: 30.93\n",
      "2022-05-24 11:41:27 [INFO]: Epoch[4/50] train loss: 0.57756, val loss: nan, lr: 0.0010000, time: 31.14\n",
      "2022-05-24 11:41:59 [INFO]: Epoch[5/50] train loss: 0.55213, val loss: nan, lr: 0.0010000, time: 31.70\n",
      "2022-05-24 11:42:31 [INFO]: Epoch[6/50] train loss: 0.50243, val loss: nan, lr: 0.0010000, time: 32.52\n",
      "2022-05-24 11:43:04 [INFO]: Epoch[7/50] train loss: 0.47601, val loss: nan, lr: 0.0010000, time: 32.33\n",
      "2022-05-24 11:43:36 [INFO]: Epoch[8/50] train loss: 0.45829, val loss: nan, lr: 0.0010000, time: 32.49\n",
      "2022-05-24 11:44:11 [INFO]: Epoch[9/50] train loss: 0.44516, val loss: nan, lr: 0.0010000, time: 34.28\n",
      "2022-05-24 11:44:47 [INFO]: Epoch[10/50] train loss: 0.43462, val loss: nan, lr: 0.0010000, time: 36.07\n",
      "2022-05-24 11:45:21 [INFO]: Epoch[11/50] train loss: 0.42676, val loss: nan, lr: 0.0010000, time: 34.11\n",
      "2022-05-24 11:45:53 [INFO]: Epoch[12/50] train loss: 0.42160, val loss: nan, lr: 0.0010000, time: 31.81\n",
      "2022-05-24 11:46:24 [INFO]: Epoch[13/50] train loss: 0.41741, val loss: nan, lr: 0.0010000, time: 31.57\n",
      "2022-05-24 11:46:55 [INFO]: Epoch[14/50] train loss: 0.41505, val loss: nan, lr: 0.0010000, time: 31.32\n",
      "2022-05-24 11:47:28 [INFO]: Epoch[15/50] train loss: 0.41150, val loss: nan, lr: 0.0010000, time: 32.44\n",
      "2022-05-24 11:48:03 [INFO]: Epoch[16/50] train loss: 0.40950, val loss: nan, lr: 0.0010000, time: 35.13\n",
      "2022-05-24 11:48:34 [INFO]: Epoch[17/50] train loss: 0.40707, val loss: nan, lr: 0.0010000, time: 31.50\n",
      "2022-05-24 11:49:06 [INFO]: Epoch[18/50] train loss: 0.40588, val loss: nan, lr: 0.0010000, time: 31.06\n",
      "2022-05-24 11:49:40 [INFO]: Epoch[19/50] train loss: 0.40390, val loss: nan, lr: 0.0010000, time: 34.19\n",
      "2022-05-24 11:50:12 [INFO]: Epoch[20/50] train loss: 0.40243, val loss: nan, lr: 0.0010000, time: 32.57\n",
      "2022-05-24 11:50:48 [INFO]: Epoch[21/50] train loss: 0.40138, val loss: nan, lr: 0.0010000, time: 35.23\n",
      "2022-05-24 11:51:24 [INFO]: Epoch[22/50] train loss: 0.40014, val loss: nan, lr: 0.0010000, time: 36.10\n",
      "2022-05-24 11:51:59 [INFO]: Epoch[23/50] train loss: 0.39957, val loss: nan, lr: 0.0010000, time: 35.06\n",
      "2022-05-24 11:52:32 [INFO]: Epoch[24/50] train loss: 0.39769, val loss: nan, lr: 0.0010000, time: 33.25\n",
      "2022-05-24 11:53:07 [INFO]: Epoch[25/50] train loss: 0.39735, val loss: nan, lr: 0.0010000, time: 34.95\n",
      "2022-05-24 11:53:41 [INFO]: Epoch[26/50] train loss: 0.39700, val loss: nan, lr: 0.0010000, time: 33.87\n",
      "2022-05-24 11:54:15 [INFO]: Epoch[27/50] train loss: 0.39608, val loss: nan, lr: 0.0010000, time: 34.68\n",
      "2022-05-24 11:54:51 [INFO]: Epoch[28/50] train loss: 0.39476, val loss: nan, lr: 0.0010000, time: 35.90\n",
      "2022-05-24 11:55:27 [INFO]: Epoch[29/50] train loss: 0.39541, val loss: nan, lr: 0.0010000, time: 35.90\n",
      "2022-05-24 11:55:59 [INFO]: Epoch[30/50] train loss: 0.39482, val loss: nan, lr: 0.0010000, time: 32.18\n",
      "2022-05-24 11:56:33 [INFO]: Epoch[31/50] train loss: 0.39359, val loss: nan, lr: 0.0010000, time: 33.77\n",
      "2022-05-24 11:57:04 [INFO]: Epoch[32/50] train loss: 0.39292, val loss: nan, lr: 0.0010000, time: 31.23\n",
      "2022-05-24 11:57:36 [INFO]: Epoch[33/50] train loss: 0.39169, val loss: nan, lr: 0.0010000, time: 31.78\n",
      "2022-05-24 11:58:08 [INFO]: Epoch[34/50] train loss: 0.39178, val loss: nan, lr: 0.0010000, time: 32.17\n",
      "2022-05-24 11:58:41 [INFO]: Epoch[35/50] train loss: 0.39172, val loss: nan, lr: 0.0010000, time: 32.22\n",
      "2022-05-24 11:59:15 [INFO]: Epoch[36/50] train loss: 0.39132, val loss: nan, lr: 0.0010000, time: 34.12\n",
      "2022-05-24 11:59:52 [INFO]: Epoch[37/50] train loss: 0.39083, val loss: nan, lr: 0.0010000, time: 37.17\n",
      "2022-05-24 12:00:29 [INFO]: Epoch[38/50] train loss: 0.39039, val loss: nan, lr: 0.0010000, time: 37.13\n",
      "2022-05-24 12:01:01 [INFO]: Epoch[39/50] train loss: 0.38943, val loss: nan, lr: 0.0010000, time: 32.45\n",
      "2022-05-24 12:01:33 [INFO]: Epoch[40/50] train loss: 0.38897, val loss: nan, lr: 0.0010000, time: 31.63\n",
      "2022-05-24 12:02:06 [INFO]: Epoch[41/50] train loss: 0.38979, val loss: nan, lr: 0.0010000, time: 33.02\n",
      "2022-05-24 12:02:39 [INFO]: Epoch[42/50] train loss: 0.39049, val loss: nan, lr: 0.0010000, time: 32.41\n",
      "2022-05-24 12:03:10 [INFO]: Epoch[43/50] train loss: 0.38803, val loss: nan, lr: 0.0010000, time: 31.56\n",
      "2022-05-24 12:03:41 [INFO]: Epoch[44/50] train loss: 0.38763, val loss: nan, lr: 0.0010000, time: 31.32\n",
      "2022-05-24 12:04:13 [INFO]: Epoch[45/50] train loss: 0.38792, val loss: nan, lr: 0.0010000, time: 31.32\n",
      "2022-05-24 12:04:44 [INFO]: Epoch[46/50] train loss: 0.38725, val loss: nan, lr: 0.0010000, time: 31.70\n",
      "2022-05-24 12:05:15 [INFO]: Epoch[47/50] train loss: 0.38725, val loss: nan, lr: 0.0010000, time: 30.77\n",
      "2022-05-24 12:05:47 [INFO]: Epoch[48/50] train loss: 0.38734, val loss: nan, lr: 0.0010000, time: 31.42\n",
      "2022-05-24 12:06:18 [INFO]: Epoch[49/50] train loss: 0.38628, val loss: nan, lr: 0.0010000, time: 31.80\n",
      "2022-05-24 12:06:50 [INFO]: Epoch[50/50] train loss: 0.38587, val loss: nan, lr: 0.0010000, time: 31.61\n",
      "2022-05-24 12:06:50 [INFO]: => end training\n",
      "2022-05-24 12:06:50 [INFO]: => calculating train scores\n",
      "2022-05-24 12:07:22 [INFO]: => train score\n",
      "accuracy: 0.9927201216019341\n",
      "presision: 0.0\n",
      "recall: 0.0\n",
      "f1: 0.0\n",
      "2022-05-24 12:07:22 [INFO]: => calculating test scores\n",
      "/mnt/c/research/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "2022-05-24 12:08:21 [INFO]: => test score\n",
      "accuracy: 0.9952944880787867\n",
      "presision: 0.0\n",
      "recall: 0.0\n",
      "f1: 0.0\n",
      "2022-05-24 12:08:21 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:08:52 [INFO]: Epoch[1/50] train loss: 0.65922, val loss: nan, lr: 0.0010000, time: 31.47\n",
      "2022-05-24 12:09:24 [INFO]: Epoch[2/50] train loss: 0.57397, val loss: nan, lr: 0.0010000, time: 31.32\n",
      "2022-05-24 12:09:57 [INFO]: Epoch[3/50] train loss: 0.53114, val loss: nan, lr: 0.0010000, time: 33.25\n",
      "2022-05-24 12:10:30 [INFO]: Epoch[4/50] train loss: 0.50734, val loss: nan, lr: 0.0010000, time: 33.35\n",
      "2022-05-24 12:11:01 [INFO]: Epoch[5/50] train loss: 0.48240, val loss: nan, lr: 0.0010000, time: 30.76\n",
      "2022-05-24 12:11:32 [INFO]: Epoch[6/50] train loss: 0.47254, val loss: nan, lr: 0.0010000, time: 30.77\n",
      "2022-05-24 12:12:03 [INFO]: Epoch[7/50] train loss: 0.46297, val loss: nan, lr: 0.0010000, time: 31.25\n",
      "2022-05-24 12:12:33 [INFO]: Epoch[8/50] train loss: 0.45839, val loss: nan, lr: 0.0010000, time: 30.53\n",
      "2022-05-24 12:13:04 [INFO]: Epoch[9/50] train loss: 0.45507, val loss: nan, lr: 0.0010000, time: 30.89\n",
      "2022-05-24 12:13:35 [INFO]: Epoch[10/50] train loss: 0.45035, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 12:14:06 [INFO]: Epoch[11/50] train loss: 0.44703, val loss: nan, lr: 0.0010000, time: 30.69\n",
      "2022-05-24 12:14:37 [INFO]: Epoch[12/50] train loss: 0.44625, val loss: nan, lr: 0.0010000, time: 30.89\n",
      "2022-05-24 12:15:08 [INFO]: Epoch[13/50] train loss: 0.44663, val loss: nan, lr: 0.0010000, time: 31.14\n",
      "2022-05-24 12:15:40 [INFO]: Epoch[14/50] train loss: 0.44273, val loss: nan, lr: 0.0010000, time: 31.40\n",
      "2022-05-24 12:16:11 [INFO]: Epoch[15/50] train loss: 0.43922, val loss: nan, lr: 0.0010000, time: 31.10\n",
      "2022-05-24 12:16:42 [INFO]: Epoch[16/50] train loss: 0.43816, val loss: nan, lr: 0.0010000, time: 30.94\n",
      "2022-05-24 12:17:13 [INFO]: Epoch[17/50] train loss: 0.43565, val loss: nan, lr: 0.0010000, time: 30.88\n",
      "2022-05-24 12:17:43 [INFO]: Epoch[18/50] train loss: 0.43359, val loss: nan, lr: 0.0010000, time: 30.68\n",
      "2022-05-24 12:18:14 [INFO]: Epoch[19/50] train loss: 0.43867, val loss: nan, lr: 0.0010000, time: 31.14\n",
      "2022-05-24 12:18:45 [INFO]: Epoch[20/50] train loss: 0.43854, val loss: nan, lr: 0.0010000, time: 31.17\n",
      "2022-05-24 12:19:16 [INFO]: Epoch[21/50] train loss: 0.43478, val loss: nan, lr: 0.0010000, time: 30.74\n",
      "2022-05-24 12:19:47 [INFO]: Epoch[22/50] train loss: 0.42887, val loss: nan, lr: 0.0010000, time: 30.43\n",
      "2022-05-24 12:20:18 [INFO]: Epoch[23/50] train loss: 0.42622, val loss: nan, lr: 0.0010000, time: 31.17\n",
      "2022-05-24 12:20:49 [INFO]: Epoch[24/50] train loss: 0.43368, val loss: nan, lr: 0.0010000, time: 30.74\n",
      "2022-05-24 12:21:19 [INFO]: Epoch[25/50] train loss: 0.42639, val loss: nan, lr: 0.0010000, time: 30.62\n",
      "2022-05-24 12:21:50 [INFO]: Epoch[26/50] train loss: 0.42909, val loss: nan, lr: 0.0010000, time: 30.66\n",
      "2022-05-24 12:22:20 [INFO]: Epoch[27/50] train loss: 0.42241, val loss: nan, lr: 0.0010000, time: 30.46\n",
      "2022-05-24 12:22:52 [INFO]: Epoch[28/50] train loss: 0.42480, val loss: nan, lr: 0.0010000, time: 31.28\n",
      "2022-05-24 12:23:23 [INFO]: Epoch[29/50] train loss: 0.42092, val loss: nan, lr: 0.0010000, time: 31.33\n",
      "2022-05-24 12:23:54 [INFO]: Epoch[30/50] train loss: 0.41863, val loss: nan, lr: 0.0010000, time: 30.78\n",
      "2022-05-24 12:24:25 [INFO]: Epoch[31/50] train loss: 0.41813, val loss: nan, lr: 0.0010000, time: 31.06\n",
      "2022-05-24 12:24:55 [INFO]: Epoch[32/50] train loss: 0.41599, val loss: nan, lr: 0.0010000, time: 30.18\n",
      "2022-05-24 12:25:25 [INFO]: Epoch[33/50] train loss: 0.41779, val loss: nan, lr: 0.0010000, time: 30.45\n",
      "2022-05-24 12:25:56 [INFO]: Epoch[34/50] train loss: 0.41849, val loss: nan, lr: 0.0010000, time: 30.38\n",
      "2022-05-24 12:26:26 [INFO]: Epoch[35/50] train loss: 0.41680, val loss: nan, lr: 0.0010000, time: 30.67\n",
      "2022-05-24 12:26:57 [INFO]: Epoch[36/50] train loss: 0.41736, val loss: nan, lr: 0.0010000, time: 30.50\n",
      "2022-05-24 12:27:28 [INFO]: Epoch[37/50] train loss: 0.41455, val loss: nan, lr: 0.0010000, time: 30.80\n",
      "2022-05-24 12:27:59 [INFO]: Epoch[38/50] train loss: 0.41206, val loss: nan, lr: 0.0010000, time: 30.80\n",
      "2022-05-24 12:28:29 [INFO]: Epoch[39/50] train loss: 0.41610, val loss: nan, lr: 0.0010000, time: 30.84\n",
      "2022-05-24 12:29:01 [INFO]: Epoch[40/50] train loss: 0.40981, val loss: nan, lr: 0.0010000, time: 31.10\n",
      "2022-05-24 12:29:31 [INFO]: Epoch[41/50] train loss: 0.41021, val loss: nan, lr: 0.0010000, time: 30.56\n",
      "2022-05-24 12:30:02 [INFO]: Epoch[42/50] train loss: 0.40999, val loss: nan, lr: 0.0010000, time: 31.12\n",
      "2022-05-24 12:30:33 [INFO]: Epoch[43/50] train loss: 0.41207, val loss: nan, lr: 0.0010000, time: 31.09\n",
      "2022-05-24 12:31:04 [INFO]: Epoch[44/50] train loss: 0.41301, val loss: nan, lr: 0.0010000, time: 31.09\n",
      "2022-05-24 12:31:36 [INFO]: Epoch[45/50] train loss: 0.41561, val loss: nan, lr: 0.0010000, time: 31.27\n",
      "2022-05-24 12:32:07 [INFO]: Epoch[46/50] train loss: 0.40813, val loss: nan, lr: 0.0010000, time: 31.21\n",
      "2022-05-24 12:32:38 [INFO]: Epoch[47/50] train loss: 0.41003, val loss: nan, lr: 0.0010000, time: 30.86\n",
      "2022-05-24 12:33:09 [INFO]: Epoch[48/50] train loss: 0.40599, val loss: nan, lr: 0.0010000, time: 31.15\n",
      "2022-05-24 12:33:40 [INFO]: Epoch[49/50] train loss: 0.40456, val loss: nan, lr: 0.0010000, time: 31.27\n",
      "2022-05-24 12:34:11 [INFO]: Epoch[50/50] train loss: 0.40739, val loss: nan, lr: 0.0010000, time: 31.23\n",
      "2022-05-24 12:34:11 [INFO]: => end training\n",
      "2022-05-24 12:34:11 [INFO]: => calculating train scores\n",
      "2022-05-24 12:34:44 [INFO]: => train score\n",
      "accuracy: 0.9935410248661521\n",
      "presision: 0.5328358208955224\n",
      "recall: 0.8880597014925373\n",
      "f1: 0.6660447761194029\n",
      "2022-05-24 12:34:44 [INFO]: => calculating test scores\n",
      "2022-05-24 12:35:42 [INFO]: => test score\n",
      "accuracy: 0.9921982962198471\n",
      "presision: 0.30201342281879195\n",
      "recall: 0.5018587360594795\n",
      "f1: 0.37709497206703907\n",
      "2022-05-24 12:35:42 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:36:14 [INFO]: Epoch[1/50] train loss: 0.79652, val loss: nan, lr: 0.0010000, time: 32.27\n",
      "2022-05-24 12:36:46 [INFO]: Epoch[2/50] train loss: 0.66115, val loss: nan, lr: 0.0010000, time: 31.67\n",
      "2022-05-24 12:37:18 [INFO]: Epoch[3/50] train loss: 0.56710, val loss: nan, lr: 0.0010000, time: 32.86\n",
      "2022-05-24 12:37:52 [INFO]: Epoch[4/50] train loss: 0.50286, val loss: nan, lr: 0.0010000, time: 33.22\n",
      "2022-05-24 12:38:25 [INFO]: Epoch[5/50] train loss: 0.46636, val loss: nan, lr: 0.0010000, time: 33.07\n",
      "2022-05-24 12:38:57 [INFO]: Epoch[6/50] train loss: 0.44554, val loss: nan, lr: 0.0010000, time: 32.20\n",
      "2022-05-24 12:39:29 [INFO]: Epoch[7/50] train loss: 0.43122, val loss: nan, lr: 0.0010000, time: 31.92\n",
      "2022-05-24 12:40:01 [INFO]: Epoch[8/50] train loss: 0.42314, val loss: nan, lr: 0.0010000, time: 32.06\n",
      "2022-05-24 12:40:33 [INFO]: Epoch[9/50] train loss: 0.41944, val loss: nan, lr: 0.0010000, time: 32.00\n",
      "2022-05-24 12:41:06 [INFO]: Epoch[10/50] train loss: 0.41562, val loss: nan, lr: 0.0010000, time: 33.01\n",
      "2022-05-24 12:41:39 [INFO]: Epoch[11/50] train loss: 0.41277, val loss: nan, lr: 0.0010000, time: 32.92\n",
      "2022-05-24 12:42:12 [INFO]: Epoch[12/50] train loss: 0.41874, val loss: nan, lr: 0.0010000, time: 32.74\n",
      "2022-05-24 12:42:44 [INFO]: Epoch[13/50] train loss: 0.41337, val loss: nan, lr: 0.0010000, time: 32.59\n",
      "2022-05-24 12:43:16 [INFO]: Epoch[14/50] train loss: 0.41128, val loss: nan, lr: 0.0010000, time: 32.13\n",
      "2022-05-24 12:43:49 [INFO]: Epoch[15/50] train loss: 0.40851, val loss: nan, lr: 0.0010000, time: 32.28\n",
      "2022-05-24 12:44:22 [INFO]: Epoch[16/50] train loss: 0.40692, val loss: nan, lr: 0.0010000, time: 33.45\n",
      "2022-05-24 12:44:55 [INFO]: Epoch[17/50] train loss: 0.40770, val loss: nan, lr: 0.0010000, time: 33.16\n",
      "2022-05-24 12:45:28 [INFO]: Epoch[18/50] train loss: 0.40403, val loss: nan, lr: 0.0010000, time: 32.49\n",
      "2022-05-24 12:46:00 [INFO]: Epoch[19/50] train loss: 0.40211, val loss: nan, lr: 0.0010000, time: 32.70\n",
      "2022-05-24 12:46:33 [INFO]: Epoch[20/50] train loss: 0.40082, val loss: nan, lr: 0.0010000, time: 33.12\n",
      "2022-05-24 12:47:06 [INFO]: Epoch[21/50] train loss: 0.40051, val loss: nan, lr: 0.0010000, time: 33.04\n",
      "2022-05-24 12:47:39 [INFO]: Epoch[22/50] train loss: 0.39906, val loss: nan, lr: 0.0010000, time: 32.85\n",
      "2022-05-24 12:48:12 [INFO]: Epoch[23/50] train loss: 0.39887, val loss: nan, lr: 0.0010000, time: 32.66\n",
      "2022-05-24 12:48:45 [INFO]: Epoch[24/50] train loss: 0.39802, val loss: nan, lr: 0.0010000, time: 32.84\n",
      "2022-05-24 12:49:18 [INFO]: Epoch[25/50] train loss: 0.39846, val loss: nan, lr: 0.0010000, time: 33.01\n",
      "2022-05-24 12:49:51 [INFO]: Epoch[26/50] train loss: 0.39663, val loss: nan, lr: 0.0010000, time: 33.31\n",
      "2022-05-24 12:50:24 [INFO]: Epoch[27/50] train loss: 0.39539, val loss: nan, lr: 0.0010000, time: 32.70\n",
      "2022-05-24 12:50:56 [INFO]: Epoch[28/50] train loss: 0.39498, val loss: nan, lr: 0.0010000, time: 31.96\n",
      "2022-05-24 12:51:28 [INFO]: Epoch[29/50] train loss: 0.39821, val loss: nan, lr: 0.0010000, time: 32.09\n",
      "2022-05-24 12:52:02 [INFO]: Epoch[30/50] train loss: 0.39236, val loss: nan, lr: 0.0010000, time: 33.82\n",
      "2022-05-24 12:52:34 [INFO]: Epoch[31/50] train loss: 0.39561, val loss: nan, lr: 0.0010000, time: 32.28\n",
      "2022-05-24 12:53:07 [INFO]: Epoch[32/50] train loss: 0.39312, val loss: nan, lr: 0.0010000, time: 32.67\n",
      "2022-05-24 12:53:39 [INFO]: Epoch[33/50] train loss: 0.39088, val loss: nan, lr: 0.0010000, time: 31.88\n",
      "2022-05-24 12:54:14 [INFO]: Epoch[34/50] train loss: 0.38899, val loss: nan, lr: 0.0010000, time: 35.77\n",
      "2022-05-24 12:54:48 [INFO]: Epoch[35/50] train loss: 0.39015, val loss: nan, lr: 0.0010000, time: 33.51\n",
      "2022-05-24 12:55:21 [INFO]: Epoch[36/50] train loss: 0.38834, val loss: nan, lr: 0.0010000, time: 33.18\n",
      "2022-05-24 12:55:55 [INFO]: Epoch[37/50] train loss: 0.38542, val loss: nan, lr: 0.0010000, time: 33.61\n",
      "2022-05-24 12:56:29 [INFO]: Epoch[38/50] train loss: 0.39050, val loss: nan, lr: 0.0010000, time: 34.73\n",
      "2022-05-24 12:57:05 [INFO]: Epoch[39/50] train loss: 0.38776, val loss: nan, lr: 0.0010000, time: 35.45\n",
      "2022-05-24 12:57:43 [INFO]: Epoch[40/50] train loss: 0.38759, val loss: nan, lr: 0.0010000, time: 37.78\n",
      "2022-05-24 12:58:21 [INFO]: Epoch[41/50] train loss: 0.38497, val loss: nan, lr: 0.0010000, time: 38.06\n",
      "2022-05-24 12:58:59 [INFO]: Epoch[42/50] train loss: 0.39005, val loss: nan, lr: 0.0010000, time: 38.44\n",
      "2022-05-24 12:59:36 [INFO]: Epoch[43/50] train loss: 0.38427, val loss: nan, lr: 0.0010000, time: 36.65\n",
      "2022-05-24 13:00:13 [INFO]: Epoch[44/50] train loss: 0.38536, val loss: nan, lr: 0.0010000, time: 37.05\n",
      "2022-05-24 13:00:49 [INFO]: Epoch[45/50] train loss: 0.38726, val loss: nan, lr: 0.0010000, time: 35.88\n",
      "2022-05-24 13:01:21 [INFO]: Epoch[46/50] train loss: 0.38489, val loss: nan, lr: 0.0010000, time: 32.75\n",
      "2022-05-24 13:01:54 [INFO]: Epoch[47/50] train loss: 0.38326, val loss: nan, lr: 0.0010000, time: 32.82\n",
      "2022-05-24 13:02:28 [INFO]: Epoch[48/50] train loss: 0.38340, val loss: nan, lr: 0.0010000, time: 33.36\n",
      "2022-05-24 13:03:01 [INFO]: Epoch[49/50] train loss: 0.38208, val loss: nan, lr: 0.0010000, time: 32.88\n",
      "2022-05-24 13:03:34 [INFO]: Epoch[50/50] train loss: 0.38438, val loss: nan, lr: 0.0010000, time: 33.09\n",
      "2022-05-24 13:03:34 [INFO]: => end training\n",
      "2022-05-24 13:03:34 [INFO]: => calculating train scores\n",
      "2022-05-24 13:04:06 [INFO]: => train score\n",
      "accuracy: 0.9884487183535026\n",
      "presision: 0.36693660988550686\n",
      "recall: 0.8171641791044776\n",
      "f1: 0.5064559645403739\n",
      "2022-05-24 13:04:06 [INFO]: => calculating test scores\n",
      "2022-05-24 13:05:07 [INFO]: => test score\n",
      "accuracy: 0.9863732572987912\n",
      "presision: 0.16883116883116883\n",
      "recall: 0.483271375464684\n",
      "f1: 0.2502406159769009\n",
      "2022-05-24 13:05:07 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 128, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 13:05:40 [INFO]: Epoch[1/50] train loss: 0.72649, val loss: nan, lr: 0.0010000, time: 33.20\n",
      "2022-05-24 13:06:12 [INFO]: Epoch[2/50] train loss: 0.62019, val loss: nan, lr: 0.0010000, time: 32.28\n",
      "2022-05-24 13:06:45 [INFO]: Epoch[3/50] train loss: 0.55300, val loss: nan, lr: 0.0010000, time: 32.76\n",
      "2022-05-24 13:07:18 [INFO]: Epoch[4/50] train loss: 0.51431, val loss: nan, lr: 0.0010000, time: 32.82\n",
      "2022-05-24 13:07:51 [INFO]: Epoch[5/50] train loss: 0.48887, val loss: nan, lr: 0.0010000, time: 32.83\n",
      "2022-05-24 13:08:23 [INFO]: Epoch[6/50] train loss: 0.47132, val loss: nan, lr: 0.0010000, time: 31.84\n",
      "2022-05-24 13:08:55 [INFO]: Epoch[7/50] train loss: 0.46329, val loss: nan, lr: 0.0010000, time: 31.97\n",
      "2022-05-24 13:09:27 [INFO]: Epoch[8/50] train loss: 0.45646, val loss: nan, lr: 0.0010000, time: 32.37\n",
      "2022-05-24 13:09:59 [INFO]: Epoch[9/50] train loss: 0.45380, val loss: nan, lr: 0.0010000, time: 31.69\n",
      "2022-05-24 13:10:31 [INFO]: Epoch[10/50] train loss: 0.45082, val loss: nan, lr: 0.0010000, time: 31.99\n",
      "2022-05-24 13:11:03 [INFO]: Epoch[11/50] train loss: 0.44635, val loss: nan, lr: 0.0010000, time: 32.21\n",
      "2022-05-24 13:11:36 [INFO]: Epoch[12/50] train loss: 0.44598, val loss: nan, lr: 0.0010000, time: 32.76\n",
      "2022-05-24 13:12:09 [INFO]: Epoch[13/50] train loss: 0.44526, val loss: nan, lr: 0.0010000, time: 33.09\n",
      "2022-05-24 13:12:41 [INFO]: Epoch[14/50] train loss: 0.44038, val loss: nan, lr: 0.0010000, time: 32.08\n",
      "2022-05-24 13:13:14 [INFO]: Epoch[15/50] train loss: 0.43760, val loss: nan, lr: 0.0010000, time: 33.30\n",
      "2022-05-24 13:13:46 [INFO]: Epoch[16/50] train loss: 0.43924, val loss: nan, lr: 0.0010000, time: 31.96\n",
      "2022-05-24 13:14:18 [INFO]: Epoch[17/50] train loss: 0.43606, val loss: nan, lr: 0.0010000, time: 31.98\n",
      "2022-05-24 13:14:50 [INFO]: Epoch[18/50] train loss: 0.44064, val loss: nan, lr: 0.0010000, time: 32.38\n",
      "2022-05-24 13:15:23 [INFO]: Epoch[19/50] train loss: 0.43477, val loss: nan, lr: 0.0010000, time: 32.28\n",
      "2022-05-24 13:15:55 [INFO]: Epoch[20/50] train loss: 0.43709, val loss: nan, lr: 0.0010000, time: 32.39\n",
      "2022-05-24 13:16:27 [INFO]: Epoch[21/50] train loss: 0.43116, val loss: nan, lr: 0.0010000, time: 32.19\n",
      "2022-05-24 13:17:00 [INFO]: Epoch[22/50] train loss: 0.43140, val loss: nan, lr: 0.0010000, time: 32.66\n",
      "2022-05-24 13:17:33 [INFO]: Epoch[23/50] train loss: 0.43171, val loss: nan, lr: 0.0010000, time: 33.06\n",
      "2022-05-24 13:18:06 [INFO]: Epoch[24/50] train loss: 0.42742, val loss: nan, lr: 0.0010000, time: 32.58\n",
      "2022-05-24 13:18:38 [INFO]: Epoch[25/50] train loss: 0.42552, val loss: nan, lr: 0.0010000, time: 32.32\n",
      "2022-05-24 13:19:11 [INFO]: Epoch[26/50] train loss: 0.42602, val loss: nan, lr: 0.0010000, time: 33.19\n",
      "2022-05-24 13:19:44 [INFO]: Epoch[27/50] train loss: 0.42497, val loss: nan, lr: 0.0010000, time: 33.05\n",
      "2022-05-24 13:20:17 [INFO]: Epoch[28/50] train loss: 0.41787, val loss: nan, lr: 0.0010000, time: 32.62\n",
      "2022-05-24 13:20:49 [INFO]: Epoch[29/50] train loss: 0.42265, val loss: nan, lr: 0.0010000, time: 32.30\n",
      "2022-05-24 13:21:21 [INFO]: Epoch[30/50] train loss: 0.44782, val loss: nan, lr: 0.0010000, time: 31.89\n",
      "2022-05-24 13:21:54 [INFO]: Epoch[31/50] train loss: 0.42783, val loss: nan, lr: 0.0010000, time: 32.60\n",
      "2022-05-24 13:22:26 [INFO]: Epoch[32/50] train loss: 0.41694, val loss: nan, lr: 0.0010000, time: 32.60\n",
      "2022-05-24 13:22:59 [INFO]: Epoch[33/50] train loss: 0.42074, val loss: nan, lr: 0.0010000, time: 32.32\n",
      "2022-05-24 13:23:31 [INFO]: Epoch[34/50] train loss: 0.41366, val loss: nan, lr: 0.0010000, time: 32.02\n",
      "2022-05-24 13:24:03 [INFO]: Epoch[35/50] train loss: 0.41654, val loss: nan, lr: 0.0010000, time: 32.50\n",
      "2022-05-24 13:24:35 [INFO]: Epoch[36/50] train loss: 0.41181, val loss: nan, lr: 0.0010000, time: 32.45\n",
      "2022-05-24 13:25:07 [INFO]: Epoch[37/50] train loss: 0.41435, val loss: nan, lr: 0.0010000, time: 31.75\n",
      "2022-05-24 13:25:39 [INFO]: Epoch[38/50] train loss: 0.41076, val loss: nan, lr: 0.0010000, time: 31.97\n",
      "2022-05-24 13:26:12 [INFO]: Epoch[39/50] train loss: 0.41505, val loss: nan, lr: 0.0010000, time: 32.84\n",
      "2022-05-24 13:26:44 [INFO]: Epoch[40/50] train loss: 0.41180, val loss: nan, lr: 0.0010000, time: 31.83\n",
      "2022-05-24 13:27:16 [INFO]: Epoch[41/50] train loss: 0.41192, val loss: nan, lr: 0.0010000, time: 31.75\n",
      "2022-05-24 13:27:48 [INFO]: Epoch[42/50] train loss: 0.42622, val loss: nan, lr: 0.0010000, time: 31.95\n",
      "2022-05-24 13:28:20 [INFO]: Epoch[43/50] train loss: 0.41375, val loss: nan, lr: 0.0010000, time: 32.41\n",
      "2022-05-24 13:28:52 [INFO]: Epoch[44/50] train loss: 0.40599, val loss: nan, lr: 0.0010000, time: 32.31\n",
      "2022-05-24 13:29:24 [INFO]: Epoch[45/50] train loss: 0.41701, val loss: nan, lr: 0.0010000, time: 32.10\n",
      "2022-05-24 13:29:56 [INFO]: Epoch[46/50] train loss: 0.40779, val loss: nan, lr: 0.0010000, time: 31.47\n",
      "2022-05-24 13:30:27 [INFO]: Epoch[47/50] train loss: 0.40497, val loss: nan, lr: 0.0010000, time: 31.52\n",
      "2022-05-24 13:31:00 [INFO]: Epoch[48/50] train loss: 0.40920, val loss: nan, lr: 0.0010000, time: 32.20\n",
      "2022-05-24 13:31:32 [INFO]: Epoch[49/50] train loss: 0.40449, val loss: nan, lr: 0.0010000, time: 32.37\n",
      "2022-05-24 13:32:05 [INFO]: Epoch[50/50] train loss: 0.40660, val loss: nan, lr: 0.0010000, time: 32.80\n",
      "2022-05-24 13:32:05 [INFO]: => end training\n",
      "2022-05-24 13:32:05 [INFO]: => calculating train scores\n",
      "2022-05-24 13:32:38 [INFO]: => train score\n",
      "accuracy: 0.9912000974258819\n",
      "presision: 0.4505336025382175\n",
      "recall: 0.9713930348258707\n",
      "f1: 0.6155665024630542\n",
      "2022-05-24 13:32:38 [INFO]: => calculating test scores\n",
      "2022-05-24 13:33:39 [INFO]: => test score\n",
      "accuracy: 0.9889096856578096\n",
      "presision: 0.21528861154446177\n",
      "recall: 0.5130111524163569\n",
      "f1: 0.3032967032967033\n",
      "2022-05-24 13:33:39 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 13:34:10 [INFO]: Epoch[1/50] train loss: 0.66191, val loss: nan, lr: 0.0010000, time: 30.78\n",
      "2022-05-24 13:34:41 [INFO]: Epoch[2/50] train loss: 0.57322, val loss: nan, lr: 0.0010000, time: 31.21\n",
      "2022-05-24 13:35:12 [INFO]: Epoch[3/50] train loss: 0.51147, val loss: nan, lr: 0.0010000, time: 30.58\n",
      "2022-05-24 13:35:42 [INFO]: Epoch[4/50] train loss: 0.47315, val loss: nan, lr: 0.0010000, time: 30.59\n",
      "2022-05-24 13:36:14 [INFO]: Epoch[5/50] train loss: 0.44804, val loss: nan, lr: 0.0010000, time: 31.41\n",
      "2022-05-24 13:36:45 [INFO]: Epoch[6/50] train loss: 0.43478, val loss: nan, lr: 0.0010000, time: 31.34\n",
      "2022-05-24 13:37:16 [INFO]: Epoch[7/50] train loss: 0.42878, val loss: nan, lr: 0.0010000, time: 30.87\n",
      "2022-05-24 13:37:48 [INFO]: Epoch[8/50] train loss: 0.42008, val loss: nan, lr: 0.0010000, time: 32.62\n",
      "2022-05-24 13:38:19 [INFO]: Epoch[9/50] train loss: 0.41698, val loss: nan, lr: 0.0010000, time: 30.73\n",
      "2022-05-24 13:38:51 [INFO]: Epoch[10/50] train loss: 0.42007, val loss: nan, lr: 0.0010000, time: 31.52\n",
      "2022-05-24 13:39:22 [INFO]: Epoch[11/50] train loss: 0.41199, val loss: nan, lr: 0.0010000, time: 31.64\n",
      "2022-05-24 13:39:54 [INFO]: Epoch[12/50] train loss: 0.41417, val loss: nan, lr: 0.0010000, time: 31.46\n",
      "2022-05-24 13:40:24 [INFO]: Epoch[13/50] train loss: 0.42598, val loss: nan, lr: 0.0010000, time: 30.41\n",
      "2022-05-24 13:40:54 [INFO]: Epoch[14/50] train loss: 0.41976, val loss: nan, lr: 0.0010000, time: 30.26\n",
      "2022-05-24 13:41:25 [INFO]: Epoch[15/50] train loss: 0.41452, val loss: nan, lr: 0.0010000, time: 30.36\n",
      "2022-05-24 13:41:56 [INFO]: Epoch[16/50] train loss: 0.41205, val loss: nan, lr: 0.0010000, time: 30.90\n",
      "2022-05-24 13:42:27 [INFO]: Epoch[17/50] train loss: 0.40883, val loss: nan, lr: 0.0010000, time: 31.20\n",
      "2022-05-24 13:42:58 [INFO]: Epoch[18/50] train loss: 0.40750, val loss: nan, lr: 0.0010000, time: 31.39\n",
      "2022-05-24 13:43:29 [INFO]: Epoch[19/50] train loss: 0.40703, val loss: nan, lr: 0.0010000, time: 30.59\n",
      "2022-05-24 13:44:00 [INFO]: Epoch[20/50] train loss: 0.40528, val loss: nan, lr: 0.0010000, time: 30.80\n",
      "2022-05-24 13:44:31 [INFO]: Epoch[21/50] train loss: 0.40248, val loss: nan, lr: 0.0010000, time: 30.81\n",
      "2022-05-24 13:45:01 [INFO]: Epoch[22/50] train loss: 0.40161, val loss: nan, lr: 0.0010000, time: 30.60\n",
      "2022-05-24 13:45:32 [INFO]: Epoch[23/50] train loss: 0.40482, val loss: nan, lr: 0.0010000, time: 31.16\n",
      "2022-05-24 13:46:03 [INFO]: Epoch[24/50] train loss: 0.40097, val loss: nan, lr: 0.0010000, time: 30.54\n",
      "2022-05-24 13:46:33 [INFO]: Epoch[25/50] train loss: 0.39977, val loss: nan, lr: 0.0010000, time: 30.30\n",
      "2022-05-24 13:47:04 [INFO]: Epoch[26/50] train loss: 0.40573, val loss: nan, lr: 0.0010000, time: 31.18\n",
      "2022-05-24 13:47:35 [INFO]: Epoch[27/50] train loss: 0.40192, val loss: nan, lr: 0.0010000, time: 31.05\n",
      "2022-05-24 13:48:06 [INFO]: Epoch[28/50] train loss: 0.40092, val loss: nan, lr: 0.0010000, time: 30.59\n",
      "2022-05-24 13:48:36 [INFO]: Epoch[29/50] train loss: 0.39905, val loss: nan, lr: 0.0010000, time: 30.55\n",
      "2022-05-24 13:49:08 [INFO]: Epoch[30/50] train loss: 0.39921, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 13:49:38 [INFO]: Epoch[31/50] train loss: 0.39676, val loss: nan, lr: 0.0010000, time: 30.71\n",
      "2022-05-24 13:50:09 [INFO]: Epoch[32/50] train loss: 0.39657, val loss: nan, lr: 0.0010000, time: 30.78\n",
      "2022-05-24 13:50:41 [INFO]: Epoch[33/50] train loss: 0.39533, val loss: nan, lr: 0.0010000, time: 31.71\n",
      "2022-05-24 13:51:12 [INFO]: Epoch[34/50] train loss: 0.39659, val loss: nan, lr: 0.0010000, time: 31.48\n",
      "2022-05-24 13:51:44 [INFO]: Epoch[35/50] train loss: 0.39508, val loss: nan, lr: 0.0010000, time: 31.28\n",
      "2022-05-24 13:52:15 [INFO]: Epoch[36/50] train loss: 0.39546, val loss: nan, lr: 0.0010000, time: 31.13\n",
      "2022-05-24 13:52:45 [INFO]: Epoch[37/50] train loss: 0.39287, val loss: nan, lr: 0.0010000, time: 30.65\n",
      "2022-05-24 13:53:16 [INFO]: Epoch[38/50] train loss: 0.38955, val loss: nan, lr: 0.0010000, time: 30.92\n",
      "2022-05-24 13:53:47 [INFO]: Epoch[39/50] train loss: 0.38981, val loss: nan, lr: 0.0010000, time: 30.31\n",
      "2022-05-24 13:54:17 [INFO]: Epoch[40/50] train loss: 0.39408, val loss: nan, lr: 0.0010000, time: 30.59\n",
      "2022-05-24 13:54:49 [INFO]: Epoch[41/50] train loss: 0.39973, val loss: nan, lr: 0.0010000, time: 31.78\n",
      "2022-05-24 13:55:20 [INFO]: Epoch[42/50] train loss: 0.39032, val loss: nan, lr: 0.0010000, time: 30.98\n",
      "2022-05-24 13:55:51 [INFO]: Epoch[43/50] train loss: 0.39818, val loss: nan, lr: 0.0010000, time: 31.35\n",
      "2022-05-24 13:56:23 [INFO]: Epoch[44/50] train loss: 0.39369, val loss: nan, lr: 0.0010000, time: 32.00\n",
      "2022-05-24 13:56:55 [INFO]: Epoch[45/50] train loss: 0.38945, val loss: nan, lr: 0.0010000, time: 31.57\n",
      "2022-05-24 13:57:29 [INFO]: Epoch[46/50] train loss: 0.38748, val loss: nan, lr: 0.0010000, time: 34.03\n",
      "2022-05-24 13:58:02 [INFO]: Epoch[47/50] train loss: 0.38325, val loss: nan, lr: 0.0010000, time: 33.50\n",
      "2022-05-24 13:58:36 [INFO]: Epoch[48/50] train loss: 0.38405, val loss: nan, lr: 0.0010000, time: 33.81\n",
      "2022-05-24 13:59:10 [INFO]: Epoch[49/50] train loss: 0.38147, val loss: nan, lr: 0.0010000, time: 34.10\n",
      "2022-05-24 13:59:43 [INFO]: Epoch[50/50] train loss: 0.38656, val loss: nan, lr: 0.0010000, time: 33.13\n",
      "2022-05-24 13:59:43 [INFO]: => end training\n",
      "2022-05-24 13:59:43 [INFO]: => calculating train scores\n",
      "2022-05-24 14:00:18 [INFO]: => train score\n",
      "accuracy: 0.9939650078707484\n",
      "presision: 0.5556013179571664\n",
      "recall: 0.8389303482587065\n",
      "f1: 0.6684836471754212\n",
      "2022-05-24 14:00:18 [INFO]: => calculating test scores\n",
      "2022-05-24 14:01:21 [INFO]: => test score\n",
      "accuracy: 0.9919533996886315\n",
      "presision: 0.30145530145530147\n",
      "recall: 0.5390334572490706\n",
      "f1: 0.38666666666666666\n",
      "2022-05-24 14:01:21 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 14:01:52 [INFO]: Epoch[1/50] train loss: 0.86539, val loss: nan, lr: 0.0010000, time: 31.86\n",
      "2022-05-24 14:02:25 [INFO]: Epoch[2/50] train loss: 0.74932, val loss: nan, lr: 0.0010000, time: 32.64\n",
      "2022-05-24 14:02:57 [INFO]: Epoch[3/50] train loss: 0.68485, val loss: nan, lr: 0.0010000, time: 31.86\n",
      "2022-05-24 14:03:28 [INFO]: Epoch[4/50] train loss: 0.63199, val loss: nan, lr: 0.0010000, time: 30.62\n",
      "2022-05-24 14:03:59 [INFO]: Epoch[5/50] train loss: 0.59177, val loss: nan, lr: 0.0010000, time: 30.99\n",
      "2022-05-24 14:04:29 [INFO]: Epoch[6/50] train loss: 0.56587, val loss: nan, lr: 0.0010000, time: 30.62\n",
      "2022-05-24 14:05:00 [INFO]: Epoch[7/50] train loss: 0.53287, val loss: nan, lr: 0.0010000, time: 30.39\n",
      "2022-05-24 14:05:30 [INFO]: Epoch[8/50] train loss: 0.51065, val loss: nan, lr: 0.0010000, time: 30.32\n",
      "2022-05-24 14:06:00 [INFO]: Epoch[9/50] train loss: 0.49954, val loss: nan, lr: 0.0010000, time: 29.89\n",
      "2022-05-24 14:06:30 [INFO]: Epoch[10/50] train loss: 0.49270, val loss: nan, lr: 0.0010000, time: 30.75\n",
      "2022-05-24 14:07:02 [INFO]: Epoch[11/50] train loss: 0.48227, val loss: nan, lr: 0.0010000, time: 31.04\n",
      "2022-05-24 14:07:33 [INFO]: Epoch[12/50] train loss: 0.46863, val loss: nan, lr: 0.0010000, time: 31.03\n",
      "2022-05-24 14:08:03 [INFO]: Epoch[13/50] train loss: 0.46343, val loss: nan, lr: 0.0010000, time: 30.90\n",
      "2022-05-24 14:08:34 [INFO]: Epoch[14/50] train loss: 0.46034, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 14:09:05 [INFO]: Epoch[15/50] train loss: 0.45939, val loss: nan, lr: 0.0010000, time: 30.50\n",
      "2022-05-24 14:09:35 [INFO]: Epoch[16/50] train loss: 0.45372, val loss: nan, lr: 0.0010000, time: 30.26\n",
      "2022-05-24 14:10:06 [INFO]: Epoch[17/50] train loss: 0.45314, val loss: nan, lr: 0.0010000, time: 30.56\n",
      "2022-05-24 14:10:37 [INFO]: Epoch[18/50] train loss: 0.45303, val loss: nan, lr: 0.0010000, time: 30.84\n",
      "2022-05-24 14:11:07 [INFO]: Epoch[19/50] train loss: 0.45012, val loss: nan, lr: 0.0010000, time: 30.57\n",
      "2022-05-24 14:11:38 [INFO]: Epoch[20/50] train loss: 0.45265, val loss: nan, lr: 0.0010000, time: 30.89\n",
      "2022-05-24 14:12:09 [INFO]: Epoch[21/50] train loss: 0.44917, val loss: nan, lr: 0.0010000, time: 30.64\n",
      "2022-05-24 14:12:39 [INFO]: Epoch[22/50] train loss: 0.44803, val loss: nan, lr: 0.0010000, time: 30.45\n",
      "2022-05-24 14:13:10 [INFO]: Epoch[23/50] train loss: 0.44482, val loss: nan, lr: 0.0010000, time: 30.49\n",
      "2022-05-24 14:13:41 [INFO]: Epoch[24/50] train loss: 0.44626, val loss: nan, lr: 0.0010000, time: 31.22\n",
      "2022-05-24 14:14:12 [INFO]: Epoch[25/50] train loss: 0.44197, val loss: nan, lr: 0.0010000, time: 30.61\n",
      "2022-05-24 14:14:42 [INFO]: Epoch[26/50] train loss: 0.44049, val loss: nan, lr: 0.0010000, time: 30.48\n",
      "2022-05-24 14:15:12 [INFO]: Epoch[27/50] train loss: 0.43846, val loss: nan, lr: 0.0010000, time: 30.47\n",
      "2022-05-24 14:15:44 [INFO]: Epoch[28/50] train loss: 0.43731, val loss: nan, lr: 0.0010000, time: 31.44\n",
      "2022-05-24 14:16:15 [INFO]: Epoch[29/50] train loss: 0.43818, val loss: nan, lr: 0.0010000, time: 31.02\n",
      "2022-05-24 14:16:46 [INFO]: Epoch[30/50] train loss: 0.43533, val loss: nan, lr: 0.0010000, time: 30.83\n",
      "2022-05-24 14:17:16 [INFO]: Epoch[31/50] train loss: 0.43793, val loss: nan, lr: 0.0010000, time: 30.36\n",
      "2022-05-24 14:17:47 [INFO]: Epoch[32/50] train loss: 0.43753, val loss: nan, lr: 0.0010000, time: 30.88\n",
      "2022-05-24 14:18:17 [INFO]: Epoch[33/50] train loss: 0.43525, val loss: nan, lr: 0.0010000, time: 30.42\n",
      "2022-05-24 14:18:48 [INFO]: Epoch[34/50] train loss: 0.43545, val loss: nan, lr: 0.0010000, time: 30.90\n",
      "2022-05-24 14:19:20 [INFO]: Epoch[35/50] train loss: 0.43593, val loss: nan, lr: 0.0010000, time: 31.30\n",
      "2022-05-24 14:19:50 [INFO]: Epoch[36/50] train loss: 0.43010, val loss: nan, lr: 0.0010000, time: 30.70\n",
      "2022-05-24 14:20:21 [INFO]: Epoch[37/50] train loss: 0.42641, val loss: nan, lr: 0.0010000, time: 30.46\n",
      "2022-05-24 14:20:50 [INFO]: Epoch[38/50] train loss: 0.42865, val loss: nan, lr: 0.0010000, time: 29.60\n",
      "2022-05-24 14:21:21 [INFO]: Epoch[39/50] train loss: 0.43859, val loss: nan, lr: 0.0010000, time: 30.29\n",
      "2022-05-24 14:21:51 [INFO]: Epoch[40/50] train loss: 0.43323, val loss: nan, lr: 0.0010000, time: 30.54\n",
      "2022-05-24 14:22:22 [INFO]: Epoch[41/50] train loss: 0.42806, val loss: nan, lr: 0.0010000, time: 30.43\n",
      "2022-05-24 14:22:52 [INFO]: Epoch[42/50] train loss: 0.42780, val loss: nan, lr: 0.0010000, time: 30.53\n",
      "2022-05-24 14:23:22 [INFO]: Epoch[43/50] train loss: 0.42563, val loss: nan, lr: 0.0010000, time: 30.11\n",
      "2022-05-24 14:23:53 [INFO]: Epoch[44/50] train loss: 0.42542, val loss: nan, lr: 0.0010000, time: 30.49\n",
      "2022-05-24 14:24:24 [INFO]: Epoch[45/50] train loss: 0.42269, val loss: nan, lr: 0.0010000, time: 31.03\n",
      "2022-05-24 14:24:54 [INFO]: Epoch[46/50] train loss: 0.42174, val loss: nan, lr: 0.0010000, time: 30.62\n",
      "2022-05-24 14:25:25 [INFO]: Epoch[47/50] train loss: 0.41950, val loss: nan, lr: 0.0010000, time: 30.47\n",
      "2022-05-24 14:25:56 [INFO]: Epoch[48/50] train loss: 0.41945, val loss: nan, lr: 0.0010000, time: 31.35\n",
      "2022-05-24 14:26:27 [INFO]: Epoch[49/50] train loss: 0.41717, val loss: nan, lr: 0.0010000, time: 30.57\n",
      "2022-05-24 14:26:57 [INFO]: Epoch[50/50] train loss: 0.41717, val loss: nan, lr: 0.0010000, time: 30.05\n",
      "2022-05-24 14:26:57 [INFO]: => end training\n",
      "2022-05-24 14:26:57 [INFO]: => calculating train scores\n",
      "2022-05-24 14:27:29 [INFO]: => train score\n",
      "accuracy: 0.9773214197116014\n",
      "presision: 0.22294232015554116\n",
      "recall: 0.8557213930348259\n",
      "f1: 0.3537275064267352\n",
      "2022-05-24 14:27:29 [INFO]: => calculating test scores\n",
      "2022-05-24 14:28:29 [INFO]: => test score\n",
      "accuracy: 0.9806706666433432\n",
      "presision: 0.15625\n",
      "recall: 0.7063197026022305\n",
      "f1: 0.2558922558922559\n",
      "2022-05-24 14:28:29 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 14:29:06 [INFO]: Epoch[1/50] train loss: 0.77516, val loss: nan, lr: 0.0010000, time: 36.79\n",
      "2022-05-24 14:29:43 [INFO]: Epoch[2/50] train loss: 0.66046, val loss: nan, lr: 0.0010000, time: 36.45\n",
      "2022-05-24 14:30:19 [INFO]: Epoch[3/50] train loss: 0.56901, val loss: nan, lr: 0.0010000, time: 36.31\n",
      "2022-05-24 14:30:55 [INFO]: Epoch[4/50] train loss: 0.50977, val loss: nan, lr: 0.0010000, time: 36.16\n",
      "2022-05-24 14:31:32 [INFO]: Epoch[5/50] train loss: 0.47449, val loss: nan, lr: 0.0010000, time: 36.67\n",
      "2022-05-24 14:32:08 [INFO]: Epoch[6/50] train loss: 0.45279, val loss: nan, lr: 0.0010000, time: 36.56\n",
      "2022-05-24 14:32:45 [INFO]: Epoch[7/50] train loss: 0.44219, val loss: nan, lr: 0.0010000, time: 36.92\n",
      "2022-05-24 14:33:22 [INFO]: Epoch[8/50] train loss: 0.43499, val loss: nan, lr: 0.0010000, time: 37.13\n",
      "2022-05-24 14:34:00 [INFO]: Epoch[9/50] train loss: 0.43162, val loss: nan, lr: 0.0010000, time: 37.21\n",
      "2022-05-24 14:34:37 [INFO]: Epoch[10/50] train loss: 0.42844, val loss: nan, lr: 0.0010000, time: 37.16\n",
      "2022-05-24 14:35:14 [INFO]: Epoch[11/50] train loss: 0.42563, val loss: nan, lr: 0.0010000, time: 36.97\n",
      "2022-05-24 14:35:51 [INFO]: Epoch[12/50] train loss: 0.42566, val loss: nan, lr: 0.0010000, time: 37.11\n",
      "2022-05-24 14:36:27 [INFO]: Epoch[13/50] train loss: 0.42215, val loss: nan, lr: 0.0010000, time: 36.31\n",
      "2022-05-24 14:37:04 [INFO]: Epoch[14/50] train loss: 0.42207, val loss: nan, lr: 0.0010000, time: 36.85\n",
      "2022-05-24 14:37:41 [INFO]: Epoch[15/50] train loss: 0.42024, val loss: nan, lr: 0.0010000, time: 36.72\n",
      "2022-05-24 14:38:17 [INFO]: Epoch[16/50] train loss: 0.41727, val loss: nan, lr: 0.0010000, time: 36.47\n",
      "2022-05-24 14:38:54 [INFO]: Epoch[17/50] train loss: 0.41392, val loss: nan, lr: 0.0010000, time: 36.50\n",
      "2022-05-24 14:39:30 [INFO]: Epoch[18/50] train loss: 0.41216, val loss: nan, lr: 0.0010000, time: 36.50\n",
      "2022-05-24 14:40:07 [INFO]: Epoch[19/50] train loss: 0.41132, val loss: nan, lr: 0.0010000, time: 37.05\n",
      "2022-05-24 14:40:44 [INFO]: Epoch[20/50] train loss: 0.40982, val loss: nan, lr: 0.0010000, time: 36.77\n",
      "2022-05-24 14:41:20 [INFO]: Epoch[21/50] train loss: 0.41015, val loss: nan, lr: 0.0010000, time: 36.51\n",
      "2022-05-24 14:41:57 [INFO]: Epoch[22/50] train loss: 0.40769, val loss: nan, lr: 0.0010000, time: 36.91\n",
      "2022-05-24 14:42:34 [INFO]: Epoch[23/50] train loss: 0.40735, val loss: nan, lr: 0.0010000, time: 36.88\n",
      "2022-05-24 14:43:11 [INFO]: Epoch[24/50] train loss: 0.40457, val loss: nan, lr: 0.0010000, time: 36.59\n",
      "2022-05-24 14:43:47 [INFO]: Epoch[25/50] train loss: 0.40386, val loss: nan, lr: 0.0010000, time: 36.34\n",
      "2022-05-24 14:44:24 [INFO]: Epoch[26/50] train loss: 0.40429, val loss: nan, lr: 0.0010000, time: 36.34\n",
      "2022-05-24 14:45:00 [INFO]: Epoch[27/50] train loss: 0.40421, val loss: nan, lr: 0.0010000, time: 36.72\n",
      "2022-05-24 14:45:37 [INFO]: Epoch[28/50] train loss: 0.40407, val loss: nan, lr: 0.0010000, time: 36.74\n",
      "2022-05-24 14:46:13 [INFO]: Epoch[29/50] train loss: 0.40244, val loss: nan, lr: 0.0010000, time: 36.22\n",
      "2022-05-24 14:46:49 [INFO]: Epoch[30/50] train loss: 0.40267, val loss: nan, lr: 0.0010000, time: 36.10\n",
      "2022-05-24 14:47:26 [INFO]: Epoch[31/50] train loss: 0.40190, val loss: nan, lr: 0.0010000, time: 36.23\n",
      "2022-05-24 14:48:02 [INFO]: Epoch[32/50] train loss: 0.40066, val loss: nan, lr: 0.0010000, time: 36.39\n",
      "2022-05-24 14:48:38 [INFO]: Epoch[33/50] train loss: 0.40128, val loss: nan, lr: 0.0010000, time: 36.51\n",
      "2022-05-24 14:49:15 [INFO]: Epoch[34/50] train loss: 0.40062, val loss: nan, lr: 0.0010000, time: 36.94\n",
      "2022-05-24 14:49:52 [INFO]: Epoch[35/50] train loss: 0.40076, val loss: nan, lr: 0.0010000, time: 36.20\n",
      "2022-05-24 14:50:28 [INFO]: Epoch[36/50] train loss: 0.39997, val loss: nan, lr: 0.0010000, time: 36.88\n",
      "2022-05-24 14:51:05 [INFO]: Epoch[37/50] train loss: 0.39796, val loss: nan, lr: 0.0010000, time: 36.75\n",
      "2022-05-24 14:51:42 [INFO]: Epoch[38/50] train loss: 0.39878, val loss: nan, lr: 0.0010000, time: 36.53\n",
      "2022-05-24 14:52:18 [INFO]: Epoch[39/50] train loss: 0.40046, val loss: nan, lr: 0.0010000, time: 36.51\n",
      "2022-05-24 14:52:54 [INFO]: Epoch[40/50] train loss: 0.39792, val loss: nan, lr: 0.0010000, time: 36.02\n",
      "2022-05-24 14:53:30 [INFO]: Epoch[41/50] train loss: 0.39644, val loss: nan, lr: 0.0010000, time: 35.98\n",
      "2022-05-24 14:54:06 [INFO]: Epoch[42/50] train loss: 0.39543, val loss: nan, lr: 0.0010000, time: 36.14\n",
      "2022-05-24 14:54:43 [INFO]: Epoch[43/50] train loss: 0.39737, val loss: nan, lr: 0.0010000, time: 36.17\n",
      "2022-05-24 14:55:18 [INFO]: Epoch[44/50] train loss: 0.39503, val loss: nan, lr: 0.0010000, time: 35.78\n",
      "2022-05-24 14:55:54 [INFO]: Epoch[45/50] train loss: 0.39412, val loss: nan, lr: 0.0010000, time: 36.14\n",
      "2022-05-24 14:56:31 [INFO]: Epoch[46/50] train loss: 0.39226, val loss: nan, lr: 0.0010000, time: 36.11\n",
      "2022-05-24 14:57:07 [INFO]: Epoch[47/50] train loss: 0.39280, val loss: nan, lr: 0.0010000, time: 36.43\n",
      "2022-05-24 14:57:43 [INFO]: Epoch[48/50] train loss: 0.39150, val loss: nan, lr: 0.0010000, time: 36.15\n",
      "2022-05-24 14:58:20 [INFO]: Epoch[49/50] train loss: 0.39087, val loss: nan, lr: 0.0010000, time: 36.48\n",
      "2022-05-24 14:58:56 [INFO]: Epoch[50/50] train loss: 0.39132, val loss: nan, lr: 0.0010000, time: 36.33\n",
      "2022-05-24 14:58:56 [INFO]: => end training\n",
      "2022-05-24 14:58:56 [INFO]: => calculating train scores\n",
      "2022-05-24 14:59:27 [INFO]: => train score\n",
      "accuracy: 0.9900093366470161\n",
      "presision: 0.39913592555666333\n",
      "recall: 0.7468905472636815\n",
      "f1: 0.5202512453974443\n",
      "2022-05-24 14:59:27 [INFO]: => calculating test scores\n",
      "2022-05-24 15:00:52 [INFO]: => test score\n",
      "accuracy: 0.9893295082827506\n",
      "presision: 0.22807017543859648\n",
      "recall: 0.5315985130111525\n",
      "f1: 0.31919642857142855\n",
      "2022-05-24 15:00:53 [INFO]: => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 2, 'rnn_hidden_dim': 512, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 15:01:29 [INFO]: Epoch[1/50] train loss: 0.93263, val loss: nan, lr: 0.0010000, time: 36.46\n",
      "2022-05-24 15:02:05 [INFO]: Epoch[2/50] train loss: 0.82941, val loss: nan, lr: 0.0010000, time: 36.21\n",
      "2022-05-24 15:02:42 [INFO]: Epoch[3/50] train loss: 0.72291, val loss: nan, lr: 0.0010000, time: 36.70\n",
      "2022-05-24 15:03:18 [INFO]: Epoch[4/50] train loss: 0.63112, val loss: nan, lr: 0.0010000, time: 35.97\n",
      "2022-05-24 15:03:54 [INFO]: Epoch[5/50] train loss: 0.56852, val loss: nan, lr: 0.0010000, time: 35.68\n",
      "2022-05-24 15:04:30 [INFO]: Epoch[6/50] train loss: 0.52718, val loss: nan, lr: 0.0010000, time: 36.25\n",
      "2022-05-24 15:05:06 [INFO]: Epoch[7/50] train loss: 0.50087, val loss: nan, lr: 0.0010000, time: 36.15\n",
      "2022-05-24 15:05:42 [INFO]: Epoch[8/50] train loss: 0.48951, val loss: nan, lr: 0.0010000, time: 36.01\n",
      "2022-05-24 15:06:18 [INFO]: Epoch[9/50] train loss: 0.48298, val loss: nan, lr: 0.0010000, time: 36.47\n",
      "2022-05-24 15:06:55 [INFO]: Epoch[10/50] train loss: 0.47265, val loss: nan, lr: 0.0010000, time: 36.18\n",
      "2022-05-24 15:07:31 [INFO]: Epoch[11/50] train loss: 0.46826, val loss: nan, lr: 0.0010000, time: 36.34\n",
      "2022-05-24 15:08:07 [INFO]: Epoch[12/50] train loss: 0.46602, val loss: nan, lr: 0.0010000, time: 36.27\n",
      "2022-05-24 15:08:44 [INFO]: Epoch[13/50] train loss: 0.45951, val loss: nan, lr: 0.0010000, time: 36.30\n",
      "2022-05-24 15:09:20 [INFO]: Epoch[14/50] train loss: 0.45432, val loss: nan, lr: 0.0010000, time: 36.27\n",
      "2022-05-24 15:09:55 [INFO]: Epoch[15/50] train loss: 0.45109, val loss: nan, lr: 0.0010000, time: 35.40\n",
      "2022-05-24 15:10:31 [INFO]: Epoch[16/50] train loss: 0.45005, val loss: nan, lr: 0.0010000, time: 35.63\n",
      "2022-05-24 15:11:07 [INFO]: Epoch[17/50] train loss: 0.45387, val loss: nan, lr: 0.0010000, time: 35.65\n",
      "2022-05-24 15:11:42 [INFO]: Epoch[18/50] train loss: 0.45737, val loss: nan, lr: 0.0010000, time: 35.72\n",
      "2022-05-24 15:12:18 [INFO]: Epoch[19/50] train loss: 0.44933, val loss: nan, lr: 0.0010000, time: 35.61\n",
      "2022-05-24 15:12:54 [INFO]: Epoch[20/50] train loss: 0.44392, val loss: nan, lr: 0.0010000, time: 35.74\n",
      "2022-05-24 15:13:30 [INFO]: Epoch[21/50] train loss: 0.44245, val loss: nan, lr: 0.0010000, time: 35.96\n",
      "2022-05-24 15:14:05 [INFO]: Epoch[22/50] train loss: 0.44383, val loss: nan, lr: 0.0010000, time: 35.38\n",
      "2022-05-24 15:14:41 [INFO]: Epoch[23/50] train loss: 0.44078, val loss: nan, lr: 0.0010000, time: 36.33\n",
      "2022-05-24 15:15:17 [INFO]: Epoch[24/50] train loss: 0.43926, val loss: nan, lr: 0.0010000, time: 36.20\n",
      "2022-05-24 15:15:54 [INFO]: Epoch[25/50] train loss: 0.44160, val loss: nan, lr: 0.0010000, time: 36.55\n",
      "2022-05-24 15:16:30 [INFO]: Epoch[26/50] train loss: 0.44765, val loss: nan, lr: 0.0010000, time: 36.41\n",
      "2022-05-24 15:17:06 [INFO]: Epoch[27/50] train loss: 0.43954, val loss: nan, lr: 0.0010000, time: 35.65\n",
      "2022-05-24 15:17:42 [INFO]: Epoch[28/50] train loss: 0.43787, val loss: nan, lr: 0.0010000, time: 36.14\n",
      "2022-05-24 15:18:18 [INFO]: Epoch[29/50] train loss: 0.43664, val loss: nan, lr: 0.0010000, time: 36.23\n",
      "2022-05-24 15:18:54 [INFO]: Epoch[30/50] train loss: 0.43062, val loss: nan, lr: 0.0010000, time: 36.00\n",
      "2022-05-24 15:19:31 [INFO]: Epoch[31/50] train loss: 0.42974, val loss: nan, lr: 0.0010000, time: 36.13\n",
      "2022-05-24 15:20:07 [INFO]: Epoch[32/50] train loss: 0.42911, val loss: nan, lr: 0.0010000, time: 36.24\n",
      "2022-05-24 15:20:43 [INFO]: Epoch[33/50] train loss: 0.43201, val loss: nan, lr: 0.0010000, time: 36.55\n",
      "2022-05-24 15:21:20 [INFO]: Epoch[34/50] train loss: 0.43222, val loss: nan, lr: 0.0010000, time: 36.32\n",
      "2022-05-24 15:21:56 [INFO]: Epoch[35/50] train loss: 0.42772, val loss: nan, lr: 0.0010000, time: 36.11\n",
      "2022-05-24 15:22:32 [INFO]: Epoch[36/50] train loss: 0.43526, val loss: nan, lr: 0.0010000, time: 36.58\n",
      "2022-05-24 15:23:10 [INFO]: Epoch[37/50] train loss: 0.43204, val loss: nan, lr: 0.0010000, time: 37.32\n",
      "2022-05-24 15:23:47 [INFO]: Epoch[38/50] train loss: 0.42686, val loss: nan, lr: 0.0010000, time: 37.17\n",
      "2022-05-24 15:24:24 [INFO]: Epoch[39/50] train loss: 0.42654, val loss: nan, lr: 0.0010000, time: 37.05\n",
      "2022-05-24 15:25:01 [INFO]: Epoch[40/50] train loss: 0.42383, val loss: nan, lr: 0.0010000, time: 37.35\n",
      "2022-05-24 15:25:38 [INFO]: Epoch[41/50] train loss: 0.42402, val loss: nan, lr: 0.0010000, time: 36.65\n",
      "2022-05-24 15:26:15 [INFO]: Epoch[42/50] train loss: 0.42186, val loss: nan, lr: 0.0010000, time: 36.73\n",
      "2022-05-24 15:26:52 [INFO]: Epoch[43/50] train loss: 0.41830, val loss: nan, lr: 0.0010000, time: 36.86\n",
      "2022-05-24 15:27:28 [INFO]: Epoch[44/50] train loss: 0.41603, val loss: nan, lr: 0.0010000, time: 36.75\n",
      "2022-05-24 15:28:05 [INFO]: Epoch[45/50] train loss: 0.42139, val loss: nan, lr: 0.0010000, time: 36.86\n",
      "2022-05-24 15:28:42 [INFO]: Epoch[46/50] train loss: 0.42754, val loss: nan, lr: 0.0010000, time: 36.52\n",
      "2022-05-24 15:29:18 [INFO]: Epoch[47/50] train loss: 0.41640, val loss: nan, lr: 0.0010000, time: 36.48\n",
      "2022-05-24 15:29:54 [INFO]: Epoch[48/50] train loss: 0.41530, val loss: nan, lr: 0.0010000, time: 35.87\n",
      "2022-05-24 15:30:31 [INFO]: Epoch[49/50] train loss: 0.42548, val loss: nan, lr: 0.0010000, time: 36.69\n",
      "2022-05-24 15:31:08 [INFO]: Epoch[50/50] train loss: 0.42117, val loss: nan, lr: 0.0010000, time: 36.87\n",
      "2022-05-24 15:31:08 [INFO]: => end training\n",
      "2022-05-24 15:31:08 [INFO]: => calculating train scores\n",
      "2022-05-24 15:31:39 [INFO]: => train score\n",
      "accuracy: 0.9930674268291033\n",
      "presision: 0.5147119767923747\n",
      "recall: 0.7723880597014925\n",
      "f1: 0.6177567769211638\n",
      "2022-05-24 15:31:39 [INFO]: => calculating test scores\n",
      "2022-05-24 15:33:05 [INFO]: => test score\n",
      "accuracy: 0.9916560253292984\n",
      "presision: 0.27682403433476394\n",
      "recall: 0.4795539033457249\n",
      "f1: 0.3510204081632653\n"
     ]
    }
   ],
   "source": [
    "max_acc = [[0, 0, 0, 0], None]\n",
    "max_pre = [[0, 0, 0, 0], None]\n",
    "max_rcl = [[0, 0, 0, 0], None]\n",
    "max_f1 = [[0, 0, 0, 0], None]\n",
    "max_models = [None for _ in range(4)]\n",
    "\n",
    "for n_rnns in params['n_rnns']:\n",
    "    for dim in params['rnn_hidden_dim']:\n",
    "        for weight in params['pos_weight']:\n",
    "            param = dict(n_rnns=n_rnns, rnn_hidden_dim=dim, weight=weight)\n",
    "            print(param)\n",
    "            \n",
    "            # update config\n",
    "            config = {}\n",
    "            for key, val in mdl_cfg.items():\n",
    "                config[key] = val\n",
    "            for key, val in param.items():\n",
    "                config[key] = val\n",
    "            pos_weight = param[\"weight\"]\n",
    "                \n",
    "            # init model, loss, optim\n",
    "            model = init_model(config, device)\n",
    "            criterion = init_loss([1, pos_weight], device)\n",
    "            optimizer, scheduler = init_optim(\n",
    "                model, train_cfg[\"optim\"][\"lr\"], train_cfg[\"optim\"][\"lr_rate\"]\n",
    "            )\n",
    "            \n",
    "            # training\n",
    "            model, epoch, history = train(\n",
    "                model, train_loader, val_loader,\n",
    "                criterion, optimizer, scheduler,\n",
    "                epoch_len, logger, device\n",
    "            )\n",
    "            \n",
    "            # test\n",
    "            score = test(model, test_loader, logger, device)\n",
    "            acc, pre, rcl, f1 = score\n",
    "            \n",
    "            # update max scores\n",
    "            if acc > max_acc[0][0]:\n",
    "                max_acc[0] = score\n",
    "                max_acc[1] = param\n",
    "                max_models[0] = model\n",
    "            if pre > max_pre[0][1]:\n",
    "                max_pre[0] = score\n",
    "                max_pre[1] = param\n",
    "                max_models[1] = model\n",
    "            if rcl > max_rcl[0][2]:\n",
    "                max_rcl[0] = score\n",
    "                max_rcl[1] = param\n",
    "                max_models[2] = model\n",
    "            if f1 > max_f1[0][3]:\n",
    "                max_f1[0] = score\n",
    "                max_f1[1] = param\n",
    "                max_models[3] = model\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48bd10c2-70ac-46bb-a799-ea597a2204ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 8}\n",
      "accuracy: 0.995 precision: 0.000 recall: 0.000 f1_score: 0.000\n",
      "\n",
      "\n",
      "max precision:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n",
      "accuracy: 0.993 precision: 0.328 recall: 0.528 f1_score: 0.405\n",
      "\n",
      "\n",
      "max recall:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 16}\n",
      "accuracy: 0.981 precision: 0.156 recall: 0.706 f1_score: 0.256\n",
      "\n",
      "\n",
      "max f1:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n",
      "accuracy: 0.993 precision: 0.328 recall: 0.528 f1_score: 0.405\n"
     ]
    }
   ],
   "source": [
    "print('max accuracy: ', max_acc[1])\n",
    "acc, pre, rcl, f1 = max_acc[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "print('\\n')\n",
    "\n",
    "print('max precision: ', max_pre[1])\n",
    "acc, pre, rcl, f1 = max_pre[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "print('\\n')\n",
    "\n",
    "print('max recall: ', max_rcl[1])\n",
    "acc, pre, rcl, f1 = max_rcl[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "print('\\n')\n",
    "\n",
    "print('max f1: ', max_f1[1])\n",
    "acc, pre, rcl, f1 = max_f1[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "784c9760-81f6-4649-8df0-1d465e53a98f",
   "metadata": {},
   "source": [
    "epoch=50\n",
    "max accuracy:  {'n_rnns': 1, 'rnn_hidden_dim': 512, 'weight': 8}\n",
    "accuracy: 0.995 precision: 0.000 recall: 0.000 f1_score: 0.000\n",
    "\n",
    "\n",
    "max precision:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n",
    "accuracy: 0.993 precision: 0.328 recall: 0.528 f1_score: 0.405\n",
    "\n",
    "\n",
    "max recall:  {'n_rnns': 2, 'rnn_hidden_dim': 256, 'weight': 16}\n",
    "accuracy: 0.981 precision: 0.156 recall: 0.706 f1_score: 0.256\n",
    "\n",
    "\n",
    "max f1:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 8}\n",
    "accuracy: 0.993 precision: 0.328 recall: 0.528 f1_score: 0.405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0835b-4abf-46d4-ba43-1d4c26646dbb",
   "metadata": {},
   "source": [
    "## モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b54bf6e-f404-4857-b6cb-5b82b32d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max recall\n",
    "model = max_models[2]\n",
    "param = max_rcl[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9824f0c-aaf5-4d21-9350-2b5137621cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_recall.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc0d883-965c-43a5-9c60-bac39e468dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_recall.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147e18e5-12d3-41cb-a631-48554ecc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max f1\n",
    "model = max_models[3]\n",
    "param = max_f1[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfcc4a91-ce20-4aa0-a790-effed2c75aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_f1.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb143daa-abbe-419a-9fb9-8d58fc1ad234",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_f1.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a60fd-b6fe-4f37-9dea-75091bdeaf20",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3656ce-2abd-4508-9594-2a72718a379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 15:33:05 [INFO]: => createing time series 02_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  8.17it/s]\n",
      "2022-05-24 15:33:08 [INFO]: => createing time series 08_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 15.58it/s]\n",
      "2022-05-24 15:33:10 [INFO]: => createing time series 09_001\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  8.76it/s]\n",
      "2022-05-24 15:33:12 [INFO]: => extracting feature 02_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:10<00:00,  2.17it/s]\n",
      "2022-05-24 15:33:22 [INFO]: => extracting feature 08_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:09<00:00,  4.26it/s]\n",
      "2022-05-24 15:33:32 [INFO]: => extracting feature 09_001\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "x_dict, y_dict = make_all_data(inds, train_cfg[\"dataset\"][\"setting\"], grp_cfg[\"passing\"][\"default\"], logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3629ce8-7854-4936-8136-7250fa3fe047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "np.random.seed(train_cfg[\"dataset\"][\"random_seed\"])\n",
    "random_keys = np.random.choice(\n",
    "    list(x_dict.keys()),\n",
    "    size=len(x_dict),\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "train_ratio = train_cfg[\"dataset\"][\"train_ratio\"] + train_cfg[\"dataset\"][\"val_ratio\"]\n",
    "train_len = int(len(x_dict) * train_ratio)\n",
    "\n",
    "train_keys = random_keys[:train_len]\n",
    "test_keys = random_keys[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1acc30e4-b08b-4287-8b07-f8effaaebe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x_lst, y_lst, pred, seq_len=30, path=None):\n",
    "    x_lst = [[0 for _ in range(x_lst.shape[1])]] + [[np.nan for _ in range(x_lst.shape[1])] for i in range(seq_len - 1)] + x_lst.tolist()\n",
    "    y_lst = [0] + [np.nan for i in range(seq_len - 1)] + y_lst\n",
    "    pred = [0] + [np.nan for i in range(seq_len - 1)] + pred.tolist()\n",
    "    \n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    ax = fig.add_axes((0.04, 0.17, 0.80, 0.81))\n",
    "    \n",
    "    ax.plot(pred, label='pred')\n",
    "    ax.plot(y_lst, linestyle=':', label='ground truth')\n",
    "    for i, feature in enumerate(np.array(x_lst).T):\n",
    "        ax.plot(feature, alpha=0.4, label=columns[i])\n",
    "\n",
    "    ax.set_ylim((-0.05, 1.05))\n",
    "    ax.set_xlabel('frame')\n",
    "    ax.legend(\n",
    "        bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0,\n",
    "        fontsize=20, handlelength=0.8, handletextpad=0.2\n",
    "    )\n",
    "    \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    if path is not None:\n",
    "        fig.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a72bc4-e0f7-42e4-946d-0727cdf210a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '02_06_1_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d7f9e0e-6b00-4dac-9641-1088126b63c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 9\u001b[0m         x_lst \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     10\u001b[0m         y_lst \u001b[38;5;241m=\u001b[39m y_dict[key]\n\u001b[1;32m     12\u001b[0m         x, _ \u001b[38;5;241m=\u001b[39m create_sequence(x_lst, y_lst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'weight'"
     ]
    }
   ],
   "source": [
    "y_all_train = []\n",
    "pred_all_train = []\n",
    "y_eve_train = []\n",
    "pred_eve_train = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in train_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "        \n",
    "        x, _ = create_sequence(x_lst, y_lst, **config)\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "        \n",
    "        if len(x) == 0:\n",
    "            continue\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[SEQ_LEN - 1:]\n",
    "        y_lst = y_lst[SEQ_LEN - 1:]\n",
    "            \n",
    "        y_all_train += y_lst\n",
    "        pred_all_train += pred.tolist()\n",
    "        y_eve_train.append(1 in y_lst)\n",
    "        pred_eve_train.append(1 in pred.tolist())\n",
    "        \n",
    "        if 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(\"data\", \"image\", \"passing\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, config[\"seq_len\"], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24e519-53ca-405c-a4ed-5bc31fc7d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_train, pred_all_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_train, pred_all_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_train, pred_all_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_train, pred_all_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388e620-e79f-4a62-9530-ba821dcb9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_train, pred_eve_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_train, pred_eve_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_train, pred_eve_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_train, pred_eve_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445c2a5-4a0f-49c4-9ef1-97983f804401",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '08_03_2_5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4075ac9-b3b6-460b-9cc2-dac368fed21f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_all_test = []\n",
    "pred_all_test = []\n",
    "y_eve_test = []\n",
    "pred_eve_test = []\n",
    "tn, fn = 0, 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in test_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "\n",
    "        x, _ = create_sequence(x_lst, y_lst, **config)\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "\n",
    "        if len(x) == 0:\n",
    "            tn += 1\n",
    "            continue\n",
    "            \n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[SEQ_LEN - 1:]\n",
    "        y_lst = y_lst[SEQ_LEN - 1:]\n",
    "        \n",
    "        y_all_test += y_lst\n",
    "        pred_all_test += pred.tolist()\n",
    "        y_eve_test.append(1 in y_lst)\n",
    "        pred_eve_test.append(1 in pred.tolist())\n",
    "        if 1 not in y_lst:\n",
    "            if 1 not in pred:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        \n",
    "        if 1 not in pred and 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(common.data_dir, \"image\", \"passing\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, config[\"seq_len\"], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125394d0-a266-4220-9cab-a86b943a9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_test, pred_all_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_test, pred_all_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_test, pred_all_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_test, pred_all_test)))\n",
    "\n",
    "cm = confusion_matrix(y_all_test, pred_all_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa66db1-dae9-433e-a64b-803a7d9f02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_test, pred_eve_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_test, pred_eve_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_test, pred_eve_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_test, pred_eve_test)))\n",
    "\n",
    "print('true negative:', tn)\n",
    "print('false negative:', fn)\n",
    "\n",
    "cm = confusion_matrix(y_eve_test, pred_eve_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30255301-5894-4c67-961f-6123eff18fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
