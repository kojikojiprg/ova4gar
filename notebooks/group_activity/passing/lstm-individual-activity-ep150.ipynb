{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid6/home/yokoyama/research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yokoyama/research/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# back to project root\n",
    "%cd ~/research\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch import nn, optim\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from group.passing.dataset import make_data_loaders, make_all_data\n",
    "from group.passing.lstm_model import LSTMModel\n",
    "from utility.activity_loader import load_individuals\n",
    "from utility.logger import logger\n",
    "from tools.train_passing import init_model, init_loss, init_optim, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc298add-7d1e-4c1a-8a98-767bde6e9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams['xtick.direction'] = 'in'  # x axis in\n",
    "plt.rcParams['ytick.direction'] = 'in'  # y axis in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28083f0-6a29-4348-b2f0-6099f5289d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f8937b-15e0-489c-adfe-fc605429bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"config/passing/pass_train.yaml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    train_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"individual\"], \"r\") as f:\n",
    "    ind_cfg = yaml.safe_load(f)\n",
    "with open(train_cfg[\"config_path\"][\"group\"], \"r\") as f:\n",
    "    grp_cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe7d65b-260f-4091-b271-5cf80ff3c575",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:07<00:00, 11.25s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dirs_all = {}\n",
    "for room_num, surgery_items in train_cfg[\"dataset\"][\"setting\"].items():\n",
    "    for surgery_num in surgery_items.keys():\n",
    "        dirs = sorted(glob(os.path.join(\"data\", room_num, surgery_num, \"passing\", \"*\")))\n",
    "        data_dirs_all[f\"{room_num}_{surgery_num}\"] = dirs\n",
    "\n",
    "inds = {}\n",
    "for key_prefix, dirs in tqdm(data_dirs_all.items()):\n",
    "    for model_path in dirs:\n",
    "        num = model_path.split(\"/\")[-1]\n",
    "        json_path = os.path.join(model_path, \".json\", \"individual.json\")\n",
    "        tmp_inds = load_individuals(json_path, ind_cfg)\n",
    "        for pid, ind in tmp_inds.items():\n",
    "            inds[f\"{key_prefix}_{num}_{pid}\"] = ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7b174-dd28-4aa8-8d26-9bc4427a0163",
   "metadata": {},
   "source": [
    "# グリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b443de49-2306-476a-b2bb-295506a68dc5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 20:29:24,408 => createing time series 02_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.04it/s]\n",
      "2022-08-13 20:29:28,618 => createing time series 07_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:29<00:00,  1.63it/s]\n",
      "2022-08-13 20:29:57,998 => createing time series 08_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.50it/s]\n",
      "2022-08-13 20:30:03,847 => createing time series 08_002\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:32<00:00,  1.40it/s]\n",
      "2022-08-13 20:30:35,938 => createing time series 09_001\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.09it/s]\n",
      "2022-08-13 20:30:38,854 => createing time series 09_002\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:13<00:00,  1.18it/s]\n",
      "2022-08-13 20:30:52,423 => extracting feature 02_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:16<00:00,  1.06it/s]\n",
      "2022-08-13 20:31:08,436 => extracting feature 07_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:54<00:00,  1.14s/it]\n",
      "2022-08-13 20:32:03,128 => extracting feature 08_001\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:12<00:00,  3.05it/s]\n",
      "2022-08-13 20:32:15,577 => extracting feature 08_002\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [01:02<00:00,  1.39s/it]\n",
      "2022-08-13 20:33:18,074 => extracting feature 09_001\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:09<00:00,  1.07s/it]\n",
      "2022-08-13 20:33:27,750 => extracting feature 09_002\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:42<00:00,  2.64s/it]\n",
      "2022-08-13 20:34:10,154 => create train loader\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10075/10075 [00:04<00:00, 2106.54it/s]\n",
      "2022-08-13 20:34:14,948 => skip creating val loader\n",
      "2022-08-13 20:34:14,948 => create test loader\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2520/2520 [00:00<00:00, 8127.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data loader\n",
    "dataset_cfg = train_cfg[\"dataset\"]\n",
    "passing_defs = grp_cfg[\"passing\"][\"default\"]\n",
    "train_loader, val_loader, test_loader = make_data_loaders(\n",
    "    inds, dataset_cfg, passing_defs, logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60011bb5-8a2a-497d-8f5e-a1fe6b56b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "mdl_cfg = {\n",
    "    \"dropouts\": [0.1, 0],\n",
    "    \"hidden_dims\": [128, 64],\n",
    "    \"n_classes\": 2,\n",
    "    \"n_linears\": 2,\n",
    "    \"rnn_dropout\": 0.1,\n",
    "    \"size\": 4,\n",
    "}\n",
    "\n",
    "# grid search parameters\n",
    "params = {\n",
    "    'n_rnns': [1, 2, 3],\n",
    "    'rnn_hidden_dim': [128, 256],\n",
    "    'pos_weight': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# epoch\n",
    "# epoch_len = train_cfg[\"optim\"][\"epoch\"]\n",
    "epoch_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9a76f-fd7b-4bde-8e77-33dba46fd26a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 20:34:18,813 => start training\n",
      "2022-08-13 20:34:34,688 Epoch[1/150] train loss: 0.44169, val loss: nan, lr: 0.0010000, time: 15.87\n",
      "2022-08-13 20:34:51,725 Epoch[2/150] train loss: 0.39675, val loss: nan, lr: 0.0010000, time: 17.04\n",
      "2022-08-13 20:35:08,566 Epoch[3/150] train loss: 0.39408, val loss: nan, lr: 0.0010000, time: 16.84\n",
      "2022-08-13 20:35:25,968 Epoch[4/150] train loss: 0.39292, val loss: nan, lr: 0.0010000, time: 17.40\n",
      "2022-08-13 20:35:42,884 Epoch[5/150] train loss: 0.39169, val loss: nan, lr: 0.0010000, time: 16.91\n",
      "2022-08-13 20:35:59,812 Epoch[6/150] train loss: 0.39053, val loss: nan, lr: 0.0010000, time: 16.93\n",
      "2022-08-13 20:36:16,743 Epoch[7/150] train loss: 0.38935, val loss: nan, lr: 0.0010000, time: 16.93\n",
      "2022-08-13 20:36:34,796 Epoch[8/150] train loss: 0.38854, val loss: nan, lr: 0.0010000, time: 18.05\n",
      "2022-08-13 20:36:52,401 Epoch[9/150] train loss: 0.38782, val loss: nan, lr: 0.0010000, time: 17.60\n",
      "2022-08-13 20:37:08,964 Epoch[10/150] train loss: 0.38691, val loss: nan, lr: 0.0010000, time: 16.56\n",
      "2022-08-13 20:37:25,784 Epoch[11/150] train loss: 0.38612, val loss: nan, lr: 0.0010000, time: 16.82\n",
      "2022-08-13 20:37:42,588 Epoch[12/150] train loss: 0.38524, val loss: nan, lr: 0.0010000, time: 16.80\n",
      "2022-08-13 20:37:59,075 Epoch[13/150] train loss: 0.38439, val loss: nan, lr: 0.0010000, time: 16.49\n",
      "2022-08-13 20:38:15,868 Epoch[14/150] train loss: 0.38409, val loss: nan, lr: 0.0010000, time: 16.79\n",
      "2022-08-13 20:38:32,789 Epoch[15/150] train loss: 0.38306, val loss: nan, lr: 0.0010000, time: 16.92\n",
      "2022-08-13 20:38:49,381 Epoch[16/150] train loss: 0.38200, val loss: nan, lr: 0.0010000, time: 16.59\n",
      "2022-08-13 20:39:06,015 Epoch[17/150] train loss: 0.38087, val loss: nan, lr: 0.0010000, time: 16.63\n",
      "2022-08-13 20:39:23,663 Epoch[18/150] train loss: 0.38076, val loss: nan, lr: 0.0010000, time: 17.65\n",
      "2022-08-13 20:39:40,661 Epoch[19/150] train loss: 0.37998, val loss: nan, lr: 0.0010000, time: 17.00\n",
      "2022-08-13 20:39:58,041 Epoch[20/150] train loss: 0.37879, val loss: nan, lr: 0.0010000, time: 17.38\n",
      "2022-08-13 20:40:14,107 Epoch[21/150] train loss: 0.37845, val loss: nan, lr: 0.0010000, time: 16.06\n",
      "2022-08-13 20:40:32,393 Epoch[22/150] train loss: 0.37724, val loss: nan, lr: 0.0010000, time: 18.28\n",
      "2022-08-13 20:40:47,004 Epoch[23/150] train loss: 0.37681, val loss: nan, lr: 0.0010000, time: 14.61\n",
      "2022-08-13 20:41:03,139 Epoch[24/150] train loss: 0.37652, val loss: nan, lr: 0.0010000, time: 16.13\n",
      "2022-08-13 20:41:20,321 Epoch[25/150] train loss: 0.37552, val loss: nan, lr: 0.0010000, time: 17.18\n",
      "2022-08-13 20:41:37,234 Epoch[26/150] train loss: 0.37485, val loss: nan, lr: 0.0010000, time: 16.91\n",
      "2022-08-13 20:41:53,992 Epoch[27/150] train loss: 0.37458, val loss: nan, lr: 0.0010000, time: 16.76\n",
      "2022-08-13 20:42:10,677 Epoch[28/150] train loss: 0.37449, val loss: nan, lr: 0.0010000, time: 16.68\n",
      "2022-08-13 20:42:27,060 Epoch[29/150] train loss: 0.37354, val loss: nan, lr: 0.0010000, time: 16.38\n",
      "2022-08-13 20:42:43,584 Epoch[30/150] train loss: 0.37313, val loss: nan, lr: 0.0010000, time: 16.52\n",
      "2022-08-13 20:43:00,505 Epoch[31/150] train loss: 0.37249, val loss: nan, lr: 0.0010000, time: 16.92\n",
      "2022-08-13 20:43:18,105 Epoch[32/150] train loss: 0.37207, val loss: nan, lr: 0.0010000, time: 17.60\n",
      "2022-08-13 20:43:35,610 Epoch[33/150] train loss: 0.37257, val loss: nan, lr: 0.0010000, time: 17.50\n",
      "2022-08-13 20:43:52,585 Epoch[34/150] train loss: 0.37172, val loss: nan, lr: 0.0010000, time: 16.97\n",
      "2022-08-13 20:44:09,932 Epoch[35/150] train loss: 0.37117, val loss: nan, lr: 0.0010000, time: 17.35\n",
      "2022-08-13 20:44:26,877 Epoch[36/150] train loss: 0.37081, val loss: nan, lr: 0.0010000, time: 16.94\n",
      "2022-08-13 20:44:43,754 Epoch[37/150] train loss: 0.36963, val loss: nan, lr: 0.0010000, time: 16.88\n",
      "2022-08-13 20:45:00,246 Epoch[38/150] train loss: 0.37033, val loss: nan, lr: 0.0010000, time: 16.49\n",
      "2022-08-13 20:45:16,859 Epoch[39/150] train loss: 0.36900, val loss: nan, lr: 0.0010000, time: 16.61\n",
      "2022-08-13 20:45:34,209 Epoch[40/150] train loss: 0.37060, val loss: nan, lr: 0.0010000, time: 17.35\n",
      "2022-08-13 20:45:51,723 Epoch[41/150] train loss: 0.36951, val loss: nan, lr: 0.0010000, time: 17.51\n",
      "2022-08-13 20:46:09,045 Epoch[42/150] train loss: 0.36877, val loss: nan, lr: 0.0010000, time: 17.32\n",
      "2022-08-13 20:46:26,269 Epoch[43/150] train loss: 0.36844, val loss: nan, lr: 0.0010000, time: 17.22\n",
      "2022-08-13 20:46:42,902 Epoch[44/150] train loss: 0.36828, val loss: nan, lr: 0.0010000, time: 16.63\n",
      "2022-08-13 20:46:59,843 Epoch[45/150] train loss: 0.36763, val loss: nan, lr: 0.0010000, time: 16.94\n",
      "2022-08-13 20:47:16,669 Epoch[46/150] train loss: 0.36787, val loss: nan, lr: 0.0010000, time: 16.83\n",
      "2022-08-13 20:47:34,808 Epoch[47/150] train loss: 0.36788, val loss: nan, lr: 0.0010000, time: 18.14\n",
      "2022-08-13 20:47:52,137 Epoch[48/150] train loss: 0.36763, val loss: nan, lr: 0.0010000, time: 17.33\n",
      "2022-08-13 20:48:08,708 Epoch[49/150] train loss: 0.36743, val loss: nan, lr: 0.0010000, time: 16.57\n",
      "2022-08-13 20:48:25,528 Epoch[50/150] train loss: 0.36643, val loss: nan, lr: 0.0010000, time: 16.82\n",
      "2022-08-13 20:48:40,068 Epoch[51/150] train loss: 0.36658, val loss: nan, lr: 0.0010000, time: 14.54\n",
      "2022-08-13 20:48:56,980 Epoch[52/150] train loss: 0.36698, val loss: nan, lr: 0.0010000, time: 16.91\n",
      "2022-08-13 20:49:13,786 Epoch[53/150] train loss: 0.36578, val loss: nan, lr: 0.0010000, time: 16.80\n",
      "2022-08-13 20:49:31,556 Epoch[54/150] train loss: 0.36632, val loss: nan, lr: 0.0010000, time: 17.77\n",
      "2022-08-13 20:49:49,310 Epoch[55/150] train loss: 0.36558, val loss: nan, lr: 0.0010000, time: 17.75\n",
      "2022-08-13 20:50:07,316 Epoch[56/150] train loss: 0.36534, val loss: nan, lr: 0.0010000, time: 18.00\n",
      "2022-08-13 20:50:24,799 Epoch[57/150] train loss: 0.36647, val loss: nan, lr: 0.0010000, time: 17.48\n",
      "2022-08-13 20:50:41,603 Epoch[58/150] train loss: 0.36544, val loss: nan, lr: 0.0010000, time: 16.80\n",
      "2022-08-13 20:50:58,575 Epoch[59/150] train loss: 0.36481, val loss: nan, lr: 0.0010000, time: 16.97\n",
      "2022-08-13 20:51:15,628 Epoch[60/150] train loss: 0.36531, val loss: nan, lr: 0.0010000, time: 17.05\n",
      "2022-08-13 20:51:32,151 Epoch[61/150] train loss: 0.36522, val loss: nan, lr: 0.0010000, time: 16.52\n",
      "2022-08-13 20:51:49,660 Epoch[62/150] train loss: 0.36521, val loss: nan, lr: 0.0010000, time: 17.51\n",
      "2022-08-13 20:52:06,362 Epoch[63/150] train loss: 0.36475, val loss: nan, lr: 0.0010000, time: 16.70\n",
      "2022-08-13 20:52:23,592 Epoch[64/150] train loss: 0.36520, val loss: nan, lr: 0.0010000, time: 17.23\n",
      "2022-08-13 20:52:40,130 Epoch[65/150] train loss: 0.36464, val loss: nan, lr: 0.0010000, time: 16.54\n",
      "2022-08-13 20:52:56,509 Epoch[66/150] train loss: 0.36400, val loss: nan, lr: 0.0010000, time: 16.38\n",
      "2022-08-13 20:53:12,889 Epoch[67/150] train loss: 0.36370, val loss: nan, lr: 0.0010000, time: 16.38\n",
      "2022-08-13 20:53:29,783 Epoch[68/150] train loss: 0.36460, val loss: nan, lr: 0.0010000, time: 16.89\n",
      "2022-08-13 20:53:47,711 Epoch[69/150] train loss: 0.36514, val loss: nan, lr: 0.0010000, time: 17.93\n",
      "2022-08-13 20:54:05,032 Epoch[70/150] train loss: 0.36416, val loss: nan, lr: 0.0010000, time: 17.32\n",
      "2022-08-13 20:54:21,320 Epoch[71/150] train loss: 0.36420, val loss: nan, lr: 0.0010000, time: 16.29\n",
      "2022-08-13 20:54:38,679 Epoch[72/150] train loss: 0.36328, val loss: nan, lr: 0.0010000, time: 17.36\n",
      "2022-08-13 20:54:55,337 Epoch[73/150] train loss: 0.36377, val loss: nan, lr: 0.0010000, time: 16.66\n",
      "2022-08-13 20:55:11,616 Epoch[74/150] train loss: 0.36309, val loss: nan, lr: 0.0010000, time: 16.28\n",
      "2022-08-13 20:55:28,547 Epoch[75/150] train loss: 0.36331, val loss: nan, lr: 0.0010000, time: 16.93\n",
      "2022-08-13 20:55:45,498 Epoch[76/150] train loss: 0.36386, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 20:56:02,309 Epoch[77/150] train loss: 0.36347, val loss: nan, lr: 0.0010000, time: 16.81\n",
      "2022-08-13 20:56:19,725 Epoch[78/150] train loss: 0.36383, val loss: nan, lr: 0.0010000, time: 17.41\n",
      "2022-08-13 20:56:37,491 Epoch[79/150] train loss: 0.36291, val loss: nan, lr: 0.0010000, time: 17.76\n",
      "2022-08-13 20:56:54,597 Epoch[80/150] train loss: 0.36397, val loss: nan, lr: 0.0010000, time: 17.10\n",
      "2022-08-13 20:57:11,639 Epoch[81/150] train loss: 0.36265, val loss: nan, lr: 0.0010000, time: 17.04\n",
      "2022-08-13 20:57:28,880 Epoch[82/150] train loss: 0.36365, val loss: nan, lr: 0.0010000, time: 17.24\n",
      "2022-08-13 20:57:47,049 Epoch[83/150] train loss: 0.36257, val loss: nan, lr: 0.0010000, time: 18.17\n",
      "2022-08-13 20:58:04,323 Epoch[84/150] train loss: 0.36329, val loss: nan, lr: 0.0010000, time: 17.27\n",
      "2022-08-13 20:58:21,445 Epoch[85/150] train loss: 0.36297, val loss: nan, lr: 0.0010000, time: 17.12\n",
      "2022-08-13 20:58:39,147 Epoch[86/150] train loss: 0.36328, val loss: nan, lr: 0.0010000, time: 17.70\n",
      "2022-08-13 20:58:56,206 Epoch[87/150] train loss: 0.36264, val loss: nan, lr: 0.0010000, time: 17.06\n",
      "2022-08-13 20:59:13,806 Epoch[88/150] train loss: 0.36253, val loss: nan, lr: 0.0010000, time: 17.60\n",
      "2022-08-13 20:59:31,167 Epoch[89/150] train loss: 0.36189, val loss: nan, lr: 0.0010000, time: 17.36\n",
      "2022-08-13 20:59:48,388 Epoch[90/150] train loss: 0.36318, val loss: nan, lr: 0.0010000, time: 17.22\n",
      "2022-08-13 21:00:06,417 Epoch[91/150] train loss: 0.36273, val loss: nan, lr: 0.0010000, time: 18.03\n",
      "2022-08-13 21:00:24,251 Epoch[92/150] train loss: 0.36306, val loss: nan, lr: 0.0010000, time: 17.83\n",
      "2022-08-13 21:00:41,627 Epoch[93/150] train loss: 0.36212, val loss: nan, lr: 0.0010000, time: 17.37\n",
      "2022-08-13 21:00:58,682 Epoch[94/150] train loss: 0.36241, val loss: nan, lr: 0.0010000, time: 17.05\n",
      "2022-08-13 21:01:16,232 Epoch[95/150] train loss: 0.36232, val loss: nan, lr: 0.0010000, time: 17.55\n",
      "2022-08-13 21:01:33,516 Epoch[96/150] train loss: 0.36183, val loss: nan, lr: 0.0010000, time: 17.28\n",
      "2022-08-13 21:01:50,510 Epoch[97/150] train loss: 0.36331, val loss: nan, lr: 0.0010000, time: 16.99\n",
      "2022-08-13 21:02:06,955 Epoch[98/150] train loss: 0.36217, val loss: nan, lr: 0.0010000, time: 16.44\n",
      "2022-08-13 21:02:23,837 Epoch[99/150] train loss: 0.36252, val loss: nan, lr: 0.0010000, time: 16.88\n",
      "2022-08-13 21:02:41,576 Epoch[100/150] train loss: 0.36198, val loss: nan, lr: 0.0010000, time: 17.74\n",
      "2022-08-13 21:02:58,751 Epoch[101/150] train loss: 0.36169, val loss: nan, lr: 0.0010000, time: 17.17\n",
      "2022-08-13 21:03:16,046 Epoch[102/150] train loss: 0.36250, val loss: nan, lr: 0.0010000, time: 17.29\n",
      "2022-08-13 21:03:31,259 Epoch[103/150] train loss: 0.36175, val loss: nan, lr: 0.0010000, time: 15.21\n",
      "2022-08-13 21:03:48,236 Epoch[104/150] train loss: 0.36183, val loss: nan, lr: 0.0010000, time: 16.98\n",
      "2022-08-13 21:04:05,014 Epoch[105/150] train loss: 0.36237, val loss: nan, lr: 0.0010000, time: 16.78\n",
      "2022-08-13 21:04:21,295 Epoch[106/150] train loss: 0.36245, val loss: nan, lr: 0.0010000, time: 16.28\n",
      "2022-08-13 21:04:37,757 Epoch[107/150] train loss: 0.36177, val loss: nan, lr: 0.0010000, time: 16.46\n",
      "2022-08-13 21:04:55,197 Epoch[108/150] train loss: 0.36203, val loss: nan, lr: 0.0010000, time: 17.44\n",
      "2022-08-13 21:05:11,506 Epoch[109/150] train loss: 0.36119, val loss: nan, lr: 0.0010000, time: 16.31\n",
      "2022-08-13 21:05:27,370 Epoch[110/150] train loss: 0.36226, val loss: nan, lr: 0.0010000, time: 15.86\n",
      "2022-08-13 21:05:44,896 Epoch[111/150] train loss: 0.36155, val loss: nan, lr: 0.0010000, time: 17.52\n",
      "2022-08-13 21:06:01,503 Epoch[112/150] train loss: 0.36152, val loss: nan, lr: 0.0010000, time: 16.61\n",
      "2022-08-13 21:06:18,523 Epoch[113/150] train loss: 0.36169, val loss: nan, lr: 0.0010000, time: 17.02\n",
      "2022-08-13 21:06:35,340 Epoch[114/150] train loss: 0.36125, val loss: nan, lr: 0.0010000, time: 16.81\n",
      "2022-08-13 21:06:53,233 Epoch[115/150] train loss: 0.36158, val loss: nan, lr: 0.0010000, time: 17.89\n",
      "2022-08-13 21:07:09,791 Epoch[116/150] train loss: 0.36187, val loss: nan, lr: 0.0010000, time: 16.56\n",
      "2022-08-13 21:07:26,189 Epoch[117/150] train loss: 0.36193, val loss: nan, lr: 0.0010000, time: 16.40\n",
      "2022-08-13 21:07:43,879 Epoch[118/150] train loss: 0.36098, val loss: nan, lr: 0.0010000, time: 17.69\n",
      "2022-08-13 21:08:01,120 Epoch[119/150] train loss: 0.36082, val loss: nan, lr: 0.0010000, time: 17.24\n",
      "2022-08-13 21:08:18,374 Epoch[120/150] train loss: 0.36256, val loss: nan, lr: 0.0010000, time: 17.25\n",
      "2022-08-13 21:08:36,017 Epoch[121/150] train loss: 0.36110, val loss: nan, lr: 0.0010000, time: 17.64\n",
      "2022-08-13 21:08:52,766 Epoch[122/150] train loss: 0.36091, val loss: nan, lr: 0.0010000, time: 16.75\n",
      "2022-08-13 21:09:09,366 Epoch[123/150] train loss: 0.36083, val loss: nan, lr: 0.0010000, time: 16.60\n",
      "2022-08-13 21:09:28,134 Epoch[124/150] train loss: 0.36189, val loss: nan, lr: 0.0010000, time: 18.77\n",
      "2022-08-13 21:09:45,541 Epoch[125/150] train loss: 0.36090, val loss: nan, lr: 0.0010000, time: 17.41\n",
      "2022-08-13 21:10:03,019 Epoch[126/150] train loss: 0.36121, val loss: nan, lr: 0.0010000, time: 17.48\n",
      "2022-08-13 21:10:20,051 Epoch[127/150] train loss: 0.36223, val loss: nan, lr: 0.0010000, time: 17.03\n",
      "2022-08-13 21:10:37,694 Epoch[128/150] train loss: 0.36187, val loss: nan, lr: 0.0010000, time: 17.64\n",
      "2022-08-13 21:10:55,415 Epoch[129/150] train loss: 0.36095, val loss: nan, lr: 0.0010000, time: 17.72\n",
      "2022-08-13 21:11:11,896 Epoch[130/150] train loss: 0.36122, val loss: nan, lr: 0.0010000, time: 16.48\n",
      "2022-08-13 21:11:28,710 Epoch[131/150] train loss: 0.36144, val loss: nan, lr: 0.0010000, time: 16.81\n",
      "2022-08-13 21:11:45,513 Epoch[132/150] train loss: 0.36037, val loss: nan, lr: 0.0010000, time: 16.80\n",
      "2022-08-13 21:12:01,383 Epoch[133/150] train loss: 0.36152, val loss: nan, lr: 0.0010000, time: 15.87\n",
      "2022-08-13 21:12:19,343 Epoch[134/150] train loss: 0.36093, val loss: nan, lr: 0.0010000, time: 17.96\n",
      "2022-08-13 21:12:35,696 Epoch[135/150] train loss: 0.36134, val loss: nan, lr: 0.0010000, time: 16.35\n",
      "2022-08-13 21:12:51,653 Epoch[136/150] train loss: 0.36162, val loss: nan, lr: 0.0010000, time: 15.95\n",
      "2022-08-13 21:13:08,278 Epoch[137/150] train loss: 0.36110, val loss: nan, lr: 0.0010000, time: 16.62\n",
      "2022-08-13 21:13:25,594 Epoch[138/150] train loss: 0.36098, val loss: nan, lr: 0.0010000, time: 17.31\n",
      "2022-08-13 21:13:42,084 Epoch[139/150] train loss: 0.36073, val loss: nan, lr: 0.0010000, time: 16.49\n",
      "2022-08-13 21:13:58,856 Epoch[140/150] train loss: 0.36125, val loss: nan, lr: 0.0010000, time: 16.77\n",
      "2022-08-13 21:14:16,187 Epoch[141/150] train loss: 0.36185, val loss: nan, lr: 0.0010000, time: 17.33\n",
      "2022-08-13 21:14:32,627 Epoch[142/150] train loss: 0.36023, val loss: nan, lr: 0.0010000, time: 16.44\n",
      "2022-08-13 21:14:49,852 Epoch[143/150] train loss: 0.36156, val loss: nan, lr: 0.0010000, time: 17.22\n",
      "2022-08-13 21:15:08,327 Epoch[144/150] train loss: 0.36051, val loss: nan, lr: 0.0010000, time: 18.47\n",
      "2022-08-13 21:15:27,033 Epoch[145/150] train loss: 0.36227, val loss: nan, lr: 0.0010000, time: 18.71\n",
      "2022-08-13 21:15:44,781 Epoch[146/150] train loss: 0.36116, val loss: nan, lr: 0.0010000, time: 17.75\n",
      "2022-08-13 21:16:02,270 Epoch[147/150] train loss: 0.35997, val loss: nan, lr: 0.0010000, time: 17.49\n",
      "2022-08-13 21:16:18,745 Epoch[148/150] train loss: 0.36038, val loss: nan, lr: 0.0010000, time: 16.47\n",
      "2022-08-13 21:16:36,455 Epoch[149/150] train loss: 0.36166, val loss: nan, lr: 0.0010000, time: 17.71\n",
      "2022-08-13 21:16:53,084 Epoch[150/150] train loss: 0.36102, val loss: nan, lr: 0.0010000, time: 16.63\n",
      "2022-08-13 21:16:53,086 => end training\n",
      "2022-08-13 21:16:53,087 => calculating train scores\n",
      "2022-08-13 21:17:14,565 => train score\n",
      "accuracy: 0.9993397452439572\n",
      "presision: 0.8872363439697134\n",
      "recall: 0.9972644376899696\n",
      "f1: 0.9390383514596452\n",
      "2022-08-13 21:17:14,568 => calculating test scores\n",
      "2022-08-13 21:17:20,843 => test score\n",
      "accuracy: 0.9943454869199497\n",
      "presision: 0.25813449023861174\n",
      "recall: 0.3236627379873073\n",
      "f1: 0.28720836685438456\n",
      "2022-08-13 21:17:20,860 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 21:17:38,091 Epoch[1/150] train loss: 0.50015, val loss: nan, lr: 0.0010000, time: 17.23\n",
      "2022-08-13 21:17:54,331 Epoch[2/150] train loss: 0.42949, val loss: nan, lr: 0.0010000, time: 16.24\n",
      "2022-08-13 21:18:09,304 Epoch[3/150] train loss: 0.42559, val loss: nan, lr: 0.0010000, time: 14.97\n",
      "2022-08-13 21:18:26,046 Epoch[4/150] train loss: 0.42368, val loss: nan, lr: 0.0010000, time: 16.74\n",
      "2022-08-13 21:18:42,763 Epoch[5/150] train loss: 0.42201, val loss: nan, lr: 0.0010000, time: 16.72\n",
      "2022-08-13 21:19:01,382 Epoch[6/150] train loss: 0.42061, val loss: nan, lr: 0.0010000, time: 18.62\n",
      "2022-08-13 21:19:18,373 Epoch[7/150] train loss: 0.41930, val loss: nan, lr: 0.0010000, time: 16.99\n",
      "2022-08-13 21:19:35,313 Epoch[8/150] train loss: 0.41788, val loss: nan, lr: 0.0010000, time: 16.94\n",
      "2022-08-13 21:19:51,854 Epoch[9/150] train loss: 0.41592, val loss: nan, lr: 0.0010000, time: 16.54\n",
      "2022-08-13 21:20:08,847 Epoch[10/150] train loss: 0.41537, val loss: nan, lr: 0.0010000, time: 16.99\n",
      "2022-08-13 21:20:26,378 Epoch[11/150] train loss: 0.41313, val loss: nan, lr: 0.0010000, time: 17.53\n",
      "2022-08-13 21:20:43,582 Epoch[12/150] train loss: 0.41219, val loss: nan, lr: 0.0010000, time: 17.20\n",
      "2022-08-13 21:21:00,537 Epoch[13/150] train loss: 0.41092, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 21:21:17,420 Epoch[14/150] train loss: 0.41010, val loss: nan, lr: 0.0010000, time: 16.88\n",
      "2022-08-13 21:21:34,482 Epoch[15/150] train loss: 0.40911, val loss: nan, lr: 0.0010000, time: 17.06\n",
      "2022-08-13 21:21:52,792 Epoch[16/150] train loss: 0.40745, val loss: nan, lr: 0.0010000, time: 18.31\n",
      "2022-08-13 21:22:10,088 Epoch[17/150] train loss: 0.40644, val loss: nan, lr: 0.0010000, time: 17.29\n",
      "2022-08-13 21:22:26,835 Epoch[18/150] train loss: 0.40519, val loss: nan, lr: 0.0010000, time: 16.74\n",
      "2022-08-13 21:22:43,441 Epoch[19/150] train loss: 0.40444, val loss: nan, lr: 0.0010000, time: 16.60\n",
      "2022-08-13 21:23:00,638 Epoch[20/150] train loss: 0.40321, val loss: nan, lr: 0.0010000, time: 17.19\n",
      "2022-08-13 21:23:17,586 Epoch[21/150] train loss: 0.40206, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 21:23:34,698 Epoch[22/150] train loss: 0.40159, val loss: nan, lr: 0.0010000, time: 17.11\n",
      "2022-08-13 21:23:51,423 Epoch[23/150] train loss: 0.40048, val loss: nan, lr: 0.0010000, time: 16.72\n",
      "2022-08-13 21:24:07,441 Epoch[24/150] train loss: 0.39856, val loss: nan, lr: 0.0010000, time: 16.02\n",
      "2022-08-13 21:24:24,977 Epoch[25/150] train loss: 0.39856, val loss: nan, lr: 0.0010000, time: 17.53\n",
      "2022-08-13 21:24:42,133 Epoch[26/150] train loss: 0.39690, val loss: nan, lr: 0.0010000, time: 17.15\n",
      "2022-08-13 21:24:59,086 Epoch[27/150] train loss: 0.39630, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 21:25:15,178 Epoch[28/150] train loss: 0.39547, val loss: nan, lr: 0.0010000, time: 16.09\n",
      "2022-08-13 21:25:32,615 Epoch[29/150] train loss: 0.39549, val loss: nan, lr: 0.0010000, time: 17.43\n",
      "2022-08-13 21:25:50,089 Epoch[30/150] train loss: 0.39410, val loss: nan, lr: 0.0010000, time: 17.47\n",
      "2022-08-13 21:26:06,422 Epoch[31/150] train loss: 0.39683, val loss: nan, lr: 0.0010000, time: 16.33\n",
      "2022-08-13 21:26:24,017 Epoch[32/150] train loss: 0.39329, val loss: nan, lr: 0.0010000, time: 17.59\n",
      "2022-08-13 21:26:41,176 Epoch[33/150] train loss: 0.39312, val loss: nan, lr: 0.0010000, time: 17.16\n",
      "2022-08-13 21:26:57,664 Epoch[34/150] train loss: 0.39188, val loss: nan, lr: 0.0010000, time: 16.49\n",
      "2022-08-13 21:27:14,221 Epoch[35/150] train loss: 0.39229, val loss: nan, lr: 0.0010000, time: 16.56\n",
      "2022-08-13 21:27:31,539 Epoch[36/150] train loss: 0.39095, val loss: nan, lr: 0.0010000, time: 17.32\n",
      "2022-08-13 21:27:49,150 Epoch[37/150] train loss: 0.39007, val loss: nan, lr: 0.0010000, time: 17.61\n",
      "2022-08-13 21:28:06,345 Epoch[38/150] train loss: 0.39206, val loss: nan, lr: 0.0010000, time: 17.19\n",
      "2022-08-13 21:28:23,459 Epoch[39/150] train loss: 0.38880, val loss: nan, lr: 0.0010000, time: 17.11\n",
      "2022-08-13 21:28:41,761 Epoch[40/150] train loss: 0.38908, val loss: nan, lr: 0.0010000, time: 18.30\n",
      "2022-08-13 21:28:58,528 Epoch[41/150] train loss: 0.38811, val loss: nan, lr: 0.0010000, time: 16.77\n",
      "2022-08-13 21:29:14,954 Epoch[42/150] train loss: 0.38790, val loss: nan, lr: 0.0010000, time: 16.42\n",
      "2022-08-13 21:29:32,031 Epoch[43/150] train loss: 0.38790, val loss: nan, lr: 0.0010000, time: 17.08\n",
      "2022-08-13 21:29:48,727 Epoch[44/150] train loss: 0.38724, val loss: nan, lr: 0.0010000, time: 16.69\n",
      "2022-08-13 21:30:06,934 Epoch[45/150] train loss: 0.38810, val loss: nan, lr: 0.0010000, time: 18.21\n",
      "2022-08-13 21:30:22,435 Epoch[46/150] train loss: 0.38694, val loss: nan, lr: 0.0010000, time: 15.50\n",
      "2022-08-13 21:30:38,633 Epoch[47/150] train loss: 0.38636, val loss: nan, lr: 0.0010000, time: 16.20\n",
      "2022-08-13 21:30:56,084 Epoch[48/150] train loss: 0.38573, val loss: nan, lr: 0.0010000, time: 17.45\n",
      "2022-08-13 21:31:13,237 Epoch[49/150] train loss: 0.38572, val loss: nan, lr: 0.0010000, time: 17.15\n",
      "2022-08-13 21:31:31,091 Epoch[50/150] train loss: 0.38530, val loss: nan, lr: 0.0010000, time: 17.85\n",
      "2022-08-13 21:31:47,957 Epoch[51/150] train loss: 0.38488, val loss: nan, lr: 0.0010000, time: 16.86\n",
      "2022-08-13 21:32:04,372 Epoch[52/150] train loss: 0.38449, val loss: nan, lr: 0.0010000, time: 16.41\n",
      "2022-08-13 21:32:22,129 Epoch[53/150] train loss: 0.38521, val loss: nan, lr: 0.0010000, time: 17.75\n",
      "2022-08-13 21:32:38,811 Epoch[54/150] train loss: 0.38517, val loss: nan, lr: 0.0010000, time: 16.68\n",
      "2022-08-13 21:32:55,590 Epoch[55/150] train loss: 0.38397, val loss: nan, lr: 0.0010000, time: 16.78\n",
      "2022-08-13 21:33:10,324 Epoch[56/150] train loss: 0.38388, val loss: nan, lr: 0.0010000, time: 14.73\n",
      "2022-08-13 21:33:27,078 Epoch[57/150] train loss: 0.38430, val loss: nan, lr: 0.0010000, time: 16.75\n",
      "2022-08-13 21:33:43,372 Epoch[58/150] train loss: 0.38295, val loss: nan, lr: 0.0010000, time: 16.29\n",
      "2022-08-13 21:34:00,102 Epoch[59/150] train loss: 0.38435, val loss: nan, lr: 0.0010000, time: 16.73\n",
      "2022-08-13 21:34:16,284 Epoch[60/150] train loss: 0.38270, val loss: nan, lr: 0.0010000, time: 16.18\n",
      "2022-08-13 21:34:33,579 Epoch[61/150] train loss: 0.38177, val loss: nan, lr: 0.0010000, time: 17.29\n",
      "2022-08-13 21:34:50,405 Epoch[62/150] train loss: 0.38341, val loss: nan, lr: 0.0010000, time: 16.82\n",
      "2022-08-13 21:35:06,354 Epoch[63/150] train loss: 0.38262, val loss: nan, lr: 0.0010000, time: 15.95\n",
      "2022-08-13 21:35:21,747 Epoch[64/150] train loss: 0.38222, val loss: nan, lr: 0.0010000, time: 15.39\n",
      "2022-08-13 21:35:38,633 Epoch[65/150] train loss: 0.38283, val loss: nan, lr: 0.0010000, time: 16.88\n",
      "2022-08-13 21:35:55,722 Epoch[66/150] train loss: 0.38136, val loss: nan, lr: 0.0010000, time: 17.09\n",
      "2022-08-13 21:36:12,964 Epoch[67/150] train loss: 0.38193, val loss: nan, lr: 0.0010000, time: 17.24\n",
      "2022-08-13 21:36:30,688 Epoch[68/150] train loss: 0.38224, val loss: nan, lr: 0.0010000, time: 17.72\n",
      "2022-08-13 21:36:47,577 Epoch[69/150] train loss: 0.38223, val loss: nan, lr: 0.0010000, time: 16.89\n",
      "2022-08-13 21:37:05,219 Epoch[70/150] train loss: 0.38268, val loss: nan, lr: 0.0010000, time: 17.64\n",
      "2022-08-13 21:37:21,604 Epoch[71/150] train loss: 0.38008, val loss: nan, lr: 0.0010000, time: 16.38\n",
      "2022-08-13 21:37:38,552 Epoch[72/150] train loss: 0.38067, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 21:37:55,255 Epoch[73/150] train loss: 0.38182, val loss: nan, lr: 0.0010000, time: 16.70\n",
      "2022-08-13 21:38:12,629 Epoch[74/150] train loss: 0.38072, val loss: nan, lr: 0.0010000, time: 17.37\n",
      "2022-08-13 21:38:30,167 Epoch[75/150] train loss: 0.38142, val loss: nan, lr: 0.0010000, time: 17.54\n",
      "2022-08-13 21:38:47,196 Epoch[76/150] train loss: 0.38099, val loss: nan, lr: 0.0010000, time: 17.03\n",
      "2022-08-13 21:39:04,736 Epoch[77/150] train loss: 0.37937, val loss: nan, lr: 0.0010000, time: 17.54\n",
      "2022-08-13 21:39:21,910 Epoch[78/150] train loss: 0.37930, val loss: nan, lr: 0.0010000, time: 17.17\n",
      "2022-08-13 21:39:38,563 Epoch[79/150] train loss: 0.37975, val loss: nan, lr: 0.0010000, time: 16.65\n",
      "2022-08-13 21:39:54,877 Epoch[80/150] train loss: 0.38119, val loss: nan, lr: 0.0010000, time: 16.31\n",
      "2022-08-13 21:40:12,183 Epoch[81/150] train loss: 0.38062, val loss: nan, lr: 0.0010000, time: 17.30\n",
      "2022-08-13 21:40:29,991 Epoch[82/150] train loss: 0.38101, val loss: nan, lr: 0.0010000, time: 17.81\n",
      "2022-08-13 21:40:47,150 Epoch[83/150] train loss: 0.38066, val loss: nan, lr: 0.0010000, time: 17.16\n",
      "2022-08-13 21:41:04,409 Epoch[84/150] train loss: 0.37922, val loss: nan, lr: 0.0010000, time: 17.26\n",
      "2022-08-13 21:41:21,231 Epoch[85/150] train loss: 0.37849, val loss: nan, lr: 0.0010000, time: 16.82\n",
      "2022-08-13 21:41:38,928 Epoch[86/150] train loss: 0.37803, val loss: nan, lr: 0.0010000, time: 17.70\n",
      "2022-08-13 21:41:56,598 Epoch[87/150] train loss: 0.38020, val loss: nan, lr: 0.0010000, time: 17.67\n",
      "2022-08-13 21:42:14,216 Epoch[88/150] train loss: 0.37958, val loss: nan, lr: 0.0010000, time: 17.62\n",
      "2022-08-13 21:42:30,382 Epoch[89/150] train loss: 0.38024, val loss: nan, lr: 0.0010000, time: 16.16\n",
      "2022-08-13 21:42:47,391 Epoch[90/150] train loss: 0.37831, val loss: nan, lr: 0.0010000, time: 17.01\n",
      "2022-08-13 21:43:04,631 Epoch[91/150] train loss: 0.37833, val loss: nan, lr: 0.0010000, time: 17.24\n",
      "2022-08-13 21:43:22,217 Epoch[92/150] train loss: 0.37988, val loss: nan, lr: 0.0010000, time: 17.58\n",
      "2022-08-13 21:43:39,653 Epoch[93/150] train loss: 0.37803, val loss: nan, lr: 0.0010000, time: 17.43\n",
      "2022-08-13 21:43:56,005 Epoch[94/150] train loss: 0.37833, val loss: nan, lr: 0.0010000, time: 16.35\n",
      "2022-08-13 21:44:13,320 Epoch[95/150] train loss: 0.37922, val loss: nan, lr: 0.0010000, time: 17.31\n",
      "2022-08-13 21:44:30,908 Epoch[96/150] train loss: 0.37846, val loss: nan, lr: 0.0010000, time: 17.59\n",
      "2022-08-13 21:44:49,924 Epoch[97/150] train loss: 0.37737, val loss: nan, lr: 0.0010000, time: 19.01\n",
      "2022-08-13 21:45:06,548 Epoch[98/150] train loss: 0.38036, val loss: nan, lr: 0.0010000, time: 16.62\n",
      "2022-08-13 21:45:23,814 Epoch[99/150] train loss: 0.37803, val loss: nan, lr: 0.0010000, time: 17.26\n",
      "2022-08-13 21:45:40,553 Epoch[100/150] train loss: 0.37803, val loss: nan, lr: 0.0010000, time: 16.74\n",
      "2022-08-13 21:45:56,462 Epoch[101/150] train loss: 0.37912, val loss: nan, lr: 0.0010000, time: 15.91\n",
      "2022-08-13 21:46:13,165 Epoch[102/150] train loss: 0.37783, val loss: nan, lr: 0.0010000, time: 16.70\n",
      "2022-08-13 21:46:30,106 Epoch[103/150] train loss: 0.38031, val loss: nan, lr: 0.0010000, time: 16.94\n",
      "2022-08-13 21:46:47,026 Epoch[104/150] train loss: 0.37762, val loss: nan, lr: 0.0010000, time: 16.92\n",
      "2022-08-13 21:47:04,030 Epoch[105/150] train loss: 0.37704, val loss: nan, lr: 0.0010000, time: 17.00\n",
      "2022-08-13 21:47:21,823 Epoch[106/150] train loss: 0.37812, val loss: nan, lr: 0.0010000, time: 17.79\n",
      "2022-08-13 21:47:39,396 Epoch[107/150] train loss: 0.37860, val loss: nan, lr: 0.0010000, time: 17.57\n",
      "2022-08-13 21:47:56,128 Epoch[108/150] train loss: 0.37813, val loss: nan, lr: 0.0010000, time: 16.73\n",
      "2022-08-13 21:48:13,575 Epoch[109/150] train loss: 0.37743, val loss: nan, lr: 0.0010000, time: 17.45\n",
      "2022-08-13 21:48:29,294 Epoch[110/150] train loss: 0.37723, val loss: nan, lr: 0.0010000, time: 15.72\n",
      "2022-08-13 21:48:45,149 Epoch[111/150] train loss: 0.37863, val loss: nan, lr: 0.0010000, time: 15.85\n",
      "2022-08-13 21:49:01,532 Epoch[112/150] train loss: 0.37831, val loss: nan, lr: 0.0010000, time: 16.38\n",
      "2022-08-13 21:49:18,113 Epoch[113/150] train loss: 0.37803, val loss: nan, lr: 0.0010000, time: 16.58\n",
      "2022-08-13 21:49:35,088 Epoch[114/150] train loss: 0.37620, val loss: nan, lr: 0.0010000, time: 16.97\n",
      "2022-08-13 21:49:52,559 Epoch[115/150] train loss: 0.37796, val loss: nan, lr: 0.0010000, time: 17.47\n",
      "2022-08-13 21:50:10,168 Epoch[116/150] train loss: 0.37735, val loss: nan, lr: 0.0010000, time: 17.61\n",
      "2022-08-13 21:50:27,490 Epoch[117/150] train loss: 0.37913, val loss: nan, lr: 0.0010000, time: 17.32\n",
      "2022-08-13 21:50:44,621 Epoch[118/150] train loss: 0.37823, val loss: nan, lr: 0.0010000, time: 17.13\n",
      "2022-08-13 21:51:01,751 Epoch[119/150] train loss: 0.37607, val loss: nan, lr: 0.0010000, time: 17.13\n",
      "2022-08-13 21:51:18,874 Epoch[120/150] train loss: 0.37761, val loss: nan, lr: 0.0010000, time: 17.12\n",
      "2022-08-13 21:51:36,396 Epoch[121/150] train loss: 0.37793, val loss: nan, lr: 0.0010000, time: 17.52\n",
      "2022-08-13 21:51:52,324 Epoch[122/150] train loss: 0.37659, val loss: nan, lr: 0.0010000, time: 15.93\n",
      "2022-08-13 21:52:09,007 Epoch[123/150] train loss: 0.37902, val loss: nan, lr: 0.0010000, time: 16.68\n",
      "2022-08-13 21:52:24,489 Epoch[124/150] train loss: 0.37695, val loss: nan, lr: 0.0010000, time: 15.48\n",
      "2022-08-13 21:52:40,433 Epoch[125/150] train loss: 0.37581, val loss: nan, lr: 0.0010000, time: 15.94\n",
      "2022-08-13 21:52:57,563 Epoch[126/150] train loss: 0.37572, val loss: nan, lr: 0.0010000, time: 17.13\n",
      "2022-08-13 21:53:14,101 Epoch[127/150] train loss: 0.38052, val loss: nan, lr: 0.0010000, time: 16.54\n",
      "2022-08-13 21:53:32,165 Epoch[128/150] train loss: 0.37661, val loss: nan, lr: 0.0010000, time: 18.06\n",
      "2022-08-13 21:53:48,320 Epoch[129/150] train loss: 0.37562, val loss: nan, lr: 0.0010000, time: 16.15\n",
      "2022-08-13 21:54:04,984 Epoch[130/150] train loss: 0.37681, val loss: nan, lr: 0.0010000, time: 16.66\n",
      "2022-08-13 21:54:21,516 Epoch[131/150] train loss: 0.37700, val loss: nan, lr: 0.0010000, time: 16.53\n",
      "2022-08-13 21:54:38,432 Epoch[132/150] train loss: 0.37556, val loss: nan, lr: 0.0010000, time: 16.91\n",
      "2022-08-13 21:54:54,977 Epoch[133/150] train loss: 0.37576, val loss: nan, lr: 0.0010000, time: 16.54\n",
      "2022-08-13 21:55:12,802 Epoch[134/150] train loss: 0.38085, val loss: nan, lr: 0.0010000, time: 17.82\n",
      "2022-08-13 21:55:30,450 Epoch[135/150] train loss: 0.37577, val loss: nan, lr: 0.0010000, time: 17.65\n",
      "2022-08-13 21:55:47,824 Epoch[136/150] train loss: 0.37729, val loss: nan, lr: 0.0010000, time: 17.37\n",
      "2022-08-13 21:56:04,047 Epoch[137/150] train loss: 0.37703, val loss: nan, lr: 0.0010000, time: 16.22\n",
      "2022-08-13 21:56:20,537 Epoch[138/150] train loss: 0.37675, val loss: nan, lr: 0.0010000, time: 16.49\n",
      "2022-08-13 21:56:36,779 Epoch[139/150] train loss: 0.37624, val loss: nan, lr: 0.0010000, time: 16.24\n",
      "2022-08-13 21:56:53,432 Epoch[140/150] train loss: 0.37740, val loss: nan, lr: 0.0010000, time: 16.65\n",
      "2022-08-13 21:57:10,835 Epoch[141/150] train loss: 0.37671, val loss: nan, lr: 0.0010000, time: 17.40\n",
      "2022-08-13 21:57:27,940 Epoch[142/150] train loss: 0.37521, val loss: nan, lr: 0.0010000, time: 17.10\n",
      "2022-08-13 21:57:44,093 Epoch[143/150] train loss: 0.37524, val loss: nan, lr: 0.0010000, time: 16.15\n",
      "2022-08-13 21:58:00,257 Epoch[144/150] train loss: 0.37800, val loss: nan, lr: 0.0010000, time: 16.16\n",
      "2022-08-13 21:58:17,664 Epoch[145/150] train loss: 0.37625, val loss: nan, lr: 0.0010000, time: 17.40\n",
      "2022-08-13 21:58:33,502 Epoch[146/150] train loss: 0.37528, val loss: nan, lr: 0.0010000, time: 15.84\n",
      "2022-08-13 21:58:48,224 Epoch[147/150] train loss: 0.37687, val loss: nan, lr: 0.0010000, time: 14.72\n",
      "2022-08-13 21:59:05,808 Epoch[148/150] train loss: 0.37581, val loss: nan, lr: 0.0010000, time: 17.58\n",
      "2022-08-13 21:59:22,645 Epoch[149/150] train loss: 0.37530, val loss: nan, lr: 0.0010000, time: 16.83\n",
      "2022-08-13 21:59:39,710 Epoch[150/150] train loss: 0.37831, val loss: nan, lr: 0.0010000, time: 17.06\n",
      "2022-08-13 21:59:39,711 => end training\n",
      "2022-08-13 21:59:39,712 => calculating train scores\n",
      "2022-08-13 22:00:00,736 => train score\n",
      "accuracy: 0.9970931741902386\n",
      "presision: 0.6393184280508224\n",
      "recall: 0.9864741641337386\n",
      "f1: 0.7758321878921891\n",
      "2022-08-13 22:00:00,739 => calculating test scores\n",
      "2022-08-13 22:00:06,705 => test score\n",
      "accuracy: 0.9908768324515441\n",
      "presision: 0.1554160125588697\n",
      "recall: 0.3590208522212149\n",
      "f1: 0.21692686935086278\n",
      "2022-08-13 22:00:06,721 => start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rnns': 1, 'rnn_hidden_dim': 128, 'weight': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 22:00:23,345 Epoch[1/150] train loss: 0.52872, val loss: nan, lr: 0.0010000, time: 16.62\n",
      "2022-08-13 22:00:39,786 Epoch[2/150] train loss: 0.47989, val loss: nan, lr: 0.0010000, time: 16.44\n",
      "2022-08-13 22:00:55,945 Epoch[3/150] train loss: 0.47532, val loss: nan, lr: 0.0010000, time: 16.16\n",
      "2022-08-13 22:01:13,021 Epoch[4/150] train loss: 0.47224, val loss: nan, lr: 0.0010000, time: 17.07\n",
      "2022-08-13 22:01:30,104 Epoch[5/150] train loss: 0.46946, val loss: nan, lr: 0.0010000, time: 17.08\n",
      "2022-08-13 22:01:47,487 Epoch[6/150] train loss: 0.46707, val loss: nan, lr: 0.0010000, time: 17.38\n",
      "2022-08-13 22:02:04,434 Epoch[7/150] train loss: 0.46506, val loss: nan, lr: 0.0010000, time: 16.95\n",
      "2022-08-13 22:02:22,336 Epoch[8/150] train loss: 0.46381, val loss: nan, lr: 0.0010000, time: 17.90\n",
      "2022-08-13 22:02:39,848 Epoch[9/150] train loss: 0.46231, val loss: nan, lr: 0.0010000, time: 17.51\n",
      "2022-08-13 22:02:56,579 Epoch[10/150] train loss: 0.46021, val loss: nan, lr: 0.0010000, time: 16.73\n",
      "2022-08-13 22:03:13,160 Epoch[11/150] train loss: 0.45874, val loss: nan, lr: 0.0010000, time: 16.58\n",
      "2022-08-13 22:03:27,463 Epoch[12/150] train loss: 0.45691, val loss: nan, lr: 0.0010000, time: 14.30\n",
      "2022-08-13 22:03:45,669 Epoch[13/150] train loss: 0.45480, val loss: nan, lr: 0.0010000, time: 18.20\n",
      "2022-08-13 22:04:02,353 Epoch[14/150] train loss: 0.45325, val loss: nan, lr: 0.0010000, time: 16.68\n",
      "2022-08-13 22:04:18,967 Epoch[15/150] train loss: 0.45147, val loss: nan, lr: 0.0010000, time: 16.61\n",
      "2022-08-13 22:04:36,208 Epoch[16/150] train loss: 0.45039, val loss: nan, lr: 0.0010000, time: 17.24\n",
      "2022-08-13 22:04:52,417 Epoch[17/150] train loss: 0.44924, val loss: nan, lr: 0.0010000, time: 16.21\n",
      "2022-08-13 22:05:09,200 Epoch[18/150] train loss: 0.44649, val loss: nan, lr: 0.0010000, time: 16.78\n",
      "2022-08-13 22:05:26,787 Epoch[19/150] train loss: 0.44599, val loss: nan, lr: 0.0010000, time: 17.59\n",
      "2022-08-13 22:05:43,557 Epoch[20/150] train loss: 0.44411, val loss: nan, lr: 0.0010000, time: 16.77\n",
      "2022-08-13 22:06:00,298 Epoch[21/150] train loss: 0.44299, val loss: nan, lr: 0.0010000, time: 16.74\n",
      "2022-08-13 22:06:16,269 Epoch[22/150] train loss: 0.44204, val loss: nan, lr: 0.0010000, time: 15.97\n",
      "2022-08-13 22:06:32,340 Epoch[23/150] train loss: 0.44080, val loss: nan, lr: 0.0010000, time: 16.07\n",
      "2022-08-13 22:06:48,827 Epoch[24/150] train loss: 0.44019, val loss: nan, lr: 0.0010000, time: 16.48\n",
      "2022-08-13 22:07:03,553 Epoch[25/150] train loss: 0.43821, val loss: nan, lr: 0.0010000, time: 14.72\n",
      "2022-08-13 22:07:19,998 Epoch[26/150] train loss: 0.43674, val loss: nan, lr: 0.0010000, time: 16.44\n",
      "2022-08-13 22:07:36,129 Epoch[27/150] train loss: 0.43611, val loss: nan, lr: 0.0010000, time: 16.13\n",
      "2022-08-13 22:07:52,429 Epoch[28/150] train loss: 0.43743, val loss: nan, lr: 0.0010000, time: 16.30\n",
      "2022-08-13 22:08:09,026 Epoch[29/150] train loss: 0.43580, val loss: nan, lr: 0.0010000, time: 16.59\n",
      "2022-08-13 22:08:26,634 Epoch[30/150] train loss: 0.43488, val loss: nan, lr: 0.0010000, time: 17.61\n",
      "2022-08-13 22:08:43,537 Epoch[31/150] train loss: 0.43344, val loss: nan, lr: 0.0010000, time: 16.90\n",
      "2022-08-13 22:08:59,682 Epoch[32/150] train loss: 0.43293, val loss: nan, lr: 0.0010000, time: 16.14\n",
      "2022-08-13 22:09:15,725 Epoch[33/150] train loss: 0.43153, val loss: nan, lr: 0.0010000, time: 16.04\n",
      "2022-08-13 22:09:32,579 Epoch[34/150] train loss: 0.43048, val loss: nan, lr: 0.0010000, time: 16.85\n"
     ]
    }
   ],
   "source": [
    "max_acc = [[0, 0, 0, 0], None]\n",
    "max_pre = [[0, 0, 0, 0], None]\n",
    "max_rcl = [[0, 0, 0, 0], None]\n",
    "max_f1 = [[0, 0, 0, 0], None]\n",
    "max_models = [None for _ in range(4)]\n",
    "\n",
    "for n_rnns in params['n_rnns']:\n",
    "    for dim in params['rnn_hidden_dim']:\n",
    "        for weight in params['pos_weight']:\n",
    "            param = dict(n_rnns=n_rnns, rnn_hidden_dim=dim, weight=weight)\n",
    "            print(param)\n",
    "            \n",
    "            # update config\n",
    "            config = {}\n",
    "            for key, val in mdl_cfg.items():\n",
    "                config[key] = val\n",
    "            for key, val in param.items():\n",
    "                config[key] = val\n",
    "            pos_weight = param[\"weight\"]\n",
    "                \n",
    "            # init model, loss, optim\n",
    "            model = init_model(config, device)\n",
    "            criterion = init_loss([1, pos_weight], device)\n",
    "            optimizer, scheduler = init_optim(\n",
    "                model, train_cfg[\"optim\"][\"lr\"], train_cfg[\"optim\"][\"lr_rate\"]\n",
    "            )\n",
    "            \n",
    "            # training\n",
    "            model, epoch, history = train(\n",
    "                model, train_loader, val_loader,\n",
    "                criterion, optimizer, scheduler,\n",
    "                epoch_len, logger, device\n",
    "            )\n",
    "            \n",
    "            # test\n",
    "            score = test(model, test_loader, logger, device)\n",
    "            acc, pre, rcl, f1 = score\n",
    "            \n",
    "            # update max scores\n",
    "            if acc > max_acc[0][0]:\n",
    "                max_acc[0] = score\n",
    "                max_acc[1] = param\n",
    "                max_models[0] = model\n",
    "            if pre > max_pre[0][1]:\n",
    "                max_pre[0] = score\n",
    "                max_pre[1] = param\n",
    "                max_models[1] = model\n",
    "            if rcl > max_rcl[0][2]:\n",
    "                max_rcl[0] = score\n",
    "                max_rcl[1] = param\n",
    "                max_models[2] = model\n",
    "            if f1 > max_f1[0][3]:\n",
    "                max_f1[0] = score\n",
    "                max_f1[1] = param\n",
    "                max_models[3] = model\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd10c2-70ac-46bb-a799-ea597a2204ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"epoch={epoch}\")\n",
    "print('max accuracy: ', max_acc[1])\n",
    "acc, pre, rcl, f1 = max_acc[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max precision: ', max_pre[1])\n",
    "acc, pre, rcl, f1 = max_pre[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max recall: ', max_rcl[1])\n",
    "acc, pre, rcl, f1 = max_rcl[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))\n",
    "\n",
    "print('max f1: ', max_f1[1])\n",
    "acc, pre, rcl, f1 = max_f1[0]\n",
    "print('accuracy: {:.3f}'.format(acc), 'precision: {:.3f}'.format(pre), 'recall: {:.3f}'.format(rcl), 'f1_score: {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "784c9760-81f6-4649-8df0-1d465e53a98f",
   "metadata": {},
   "source": [
    "epoch=120\n",
    "max accuracy:  {'n_rnns': 3, 'rnn_hidden_dim': 256, 'weight': 8}\n",
    "accuracy: 0.996 precision: 0.560 recall: 0.416 f1_score: 0.477\n",
    "max precision:  {'n_rnns': 3, 'rnn_hidden_dim': 128, 'weight': 8}\n",
    "accuracy: 0.996 precision: 0.561 recall: 0.385 f1_score: 0.457\n",
    "max recall:  {'n_rnns': 1, 'rnn_hidden_dim': 256, 'weight': 32}\n",
    "accuracy: 0.992 precision: 0.321 recall: 0.574 f1_score: 0.411\n",
    "max f1:  {'n_rnns': 3, 'rnn_hidden_dim': 128, 'weight': 16}\n",
    "accuracy: 0.995 precision: 0.537 recall: 0.439 f1_score: 0.483"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0835b-4abf-46d4-ba43-1d4c26646dbb",
   "metadata": {},
   "source": [
    "## モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54bf6e-f404-4857-b6cb-5b82b32d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max recall\n",
    "model = max_models[2]\n",
    "param = max_rcl[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9824f0c-aaf5-4d21-9350-2b5137621cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_recall_ep{epoch}.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0d883-965c-43a5-9c60-bac39e468dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_recall_ep{epoch}.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e18e5-12d3-41cb-a631-48554ecc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max f1\n",
    "model = max_models[3]\n",
    "param = max_f1[1]\n",
    "config = {}\n",
    "for key, val in mdl_cfg.items():\n",
    "    config[key] = val\n",
    "for key, val in param.items():\n",
    "    config[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc4a91-ce20-4aa0-a790-effed2c75aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'models/passing/pass_model_lstm_f1_ep{epoch}.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb143daa-abbe-419a-9fb9-8d58fc1ad234",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"pretrained_path\"] = model_path\n",
    "with open(f'config/passing/pass_model_lstm_f1_ep{epoch}.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a60fd-b6fe-4f37-9dea-75091bdeaf20",
   "metadata": {},
   "source": [
    "# 検証\n",
    "## モデルロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3656ce-2abd-4508-9594-2a72718a379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "epoch = 150\n",
    "rcl_f1 = \"f1\"\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "mdl_cfg_path = f'config/passing/pass_model_lstm_{rcl_f1}_ep{epoch}.yaml'\n",
    "with open(mdl_cfg_path, \"r\") as f:\n",
    "    mdl_cfg = yaml.safe_load(f)\n",
    "model = init_model(mdl_cfg, device)\n",
    "\n",
    "param = torch.load(mdl_cfg[\"pretrained_path\"])\n",
    "model.load_state_dict(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade6b7-93f0-4a5a-ac67-4901ddddaafe",
   "metadata": {},
   "source": [
    "## データロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a5b8a-8a2f-4fe7-831a-76d0b8635821",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict, y_dict = make_all_data(inds, train_cfg[\"dataset\"][\"setting\"], grp_cfg[\"passing\"][\"default\"], logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddeeb2-dbc8-4860-b7c1-0c856f0df519",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(train_cfg[\"dataset\"][\"random_seed\"])\n",
    "\n",
    "seq_len = grp_cfg[\"passing\"][\"default\"][\"seq_len\"]\n",
    "size = mdl_cfg[\"size\"]\n",
    "\n",
    "keys_1 = [key for key in x_dict if 1 in y_dict[key]]\n",
    "keys_0 = [key for key in x_dict if 1 not in y_dict[key]]\n",
    "random_keys_1 = np.random.choice(keys_1, size=len(keys_1), replace=False)\n",
    "random_keys_0 = np.random.choice(keys_0, size=len(keys_0), replace=False)\n",
    "\n",
    "train_ratio = train_cfg[\"dataset\"][\"train_ratio\"]\n",
    "val_ratio = train_cfg[\"dataset\"][\"val_ratio\"]\n",
    "train_len_1 = int(len(keys_1) * train_ratio)\n",
    "train_len_0 = int(len(keys_0) * train_ratio)\n",
    "val_len_1 = int(len(keys_1) * val_ratio)\n",
    "val_len_0 = int(len(keys_0) * val_ratio)\n",
    "\n",
    "train_keys_1 = random_keys_1[:train_len_1].tolist()\n",
    "val_keys_1 = random_keys_1[train_len_1 : train_len_1 + val_len_1].tolist()\n",
    "test_keys_1 = random_keys_1[train_len_1 + val_len_1 :].tolist()\n",
    "train_keys_0 = random_keys_0[:train_len_0].tolist()\n",
    "test_keys_0 = random_keys_0[train_len_0:].tolist()\n",
    "val_keys_0 = random_keys_1[train_len_0 : train_len_0 + val_len_0].tolist()\n",
    "\n",
    "train_keys = sorted(train_keys_1 + train_keys_0)\n",
    "val_keys = sorted(val_keys_1 + val_keys_0)\n",
    "test_keys = sorted(test_keys_1 + test_keys_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc30e4-b08b-4287-8b07-f8effaaebe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(x_lst, y_lst, seq_len=30, size=4):\n",
    "    x_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(x_lst) - seq_len + 1):\n",
    "        x = x_lst[i:i + seq_len]\n",
    "        x_seq.append(x)\n",
    "        y_seq.append(y_lst[i + seq_len - 1])\n",
    "    \n",
    "    return x_seq, y_seq\n",
    "\n",
    "\n",
    "columns = [\"distance\", \"body_direction\", \"arm_ave\", \"wrist_distance\"]\n",
    "def plot(x_lst, y_lst, pred, seq_len=30, path=None):\n",
    "    x_lst = [[0 for _ in range(x_lst.shape[1])]] + [[np.nan for _ in range(x_lst.shape[1])] for i in range(seq_len - 1)] + x_lst.tolist()\n",
    "    y_lst = [0] + [np.nan for i in range(seq_len - 1)] + y_lst\n",
    "    pred = [0] + [np.nan for i in range(seq_len - 1)] + pred.tolist()\n",
    "    \n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    ax = fig.add_axes((0.04, 0.17, 0.80, 0.81))\n",
    "    \n",
    "    ax.plot(pred, label='pred')\n",
    "    ax.plot(y_lst, linestyle=':', label='ground truth')\n",
    "    for i, feature in enumerate(np.array(x_lst).T):\n",
    "        ax.plot(feature, alpha=0.4, label=columns[i])\n",
    "\n",
    "    ax.set_ylim((-0.05, 1.05))\n",
    "    ax.set_xlabel('frame')\n",
    "    ax.legend(\n",
    "        bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0,\n",
    "        fontsize=20, handlelength=0.8, handletextpad=0.2\n",
    "    )\n",
    "    \n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    if path is not None:\n",
    "        fig.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae855e-89c6-4545-8cc2-6850204c1cf9",
   "metadata": {},
   "source": [
    "## トレインデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a72bc4-e0f7-42e4-946d-0727cdf210a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '02_06_1_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f9e0e-6b00-4dac-9641-1088126b63c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_all_train = []\n",
    "pred_all_train = []\n",
    "y_eve_train = []\n",
    "pred_eve_train = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in train_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "        \n",
    "        x, _ = create_sequence(x_lst, y_lst, seq_len, size)\n",
    "        x = torch.Tensor(np.array(x)).float().to(device)\n",
    "        \n",
    "        if len(x) == 0:\n",
    "            continue\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[seq_len - 1:]\n",
    "        y_lst = y_lst[seq_len - 1:]\n",
    "            \n",
    "        y_all_train += y_lst\n",
    "        pred_all_train += pred.tolist()\n",
    "        y_eve_train.append(1 in y_lst)\n",
    "        pred_eve_train.append(1 in pred.tolist())\n",
    "        \n",
    "        if 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(\"data\", \"passing\", \"image\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, seq_len, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24e519-53ca-405c-a4ed-5bc31fc7d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_train, pred_all_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_train, pred_all_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_train, pred_all_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_train, pred_all_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388e620-e79f-4a62-9530-ba821dcb9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_train, pred_eve_train)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_train, pred_eve_train)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_train, pred_eve_train)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_train, pred_eve_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055dc76-a0e1-42b0-85a8-358c036dba57",
   "metadata": {},
   "source": [
    "## テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445c2a5-4a0f-49c4-9ef1-97983f804401",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = [\n",
    "    '08_03_2_5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4075ac9-b3b6-460b-9cc2-dac368fed21f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_all_test = []\n",
    "pred_all_test = []\n",
    "y_eve_test = []\n",
    "pred_eve_test = []\n",
    "tn, fn = 0, 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for key in test_keys:\n",
    "        x_lst = np.array(x_dict[key])\n",
    "        y_lst = y_dict[key]\n",
    "\n",
    "        x, _ = create_sequence(x_lst, y_lst, seq_len, size)\n",
    "        x = torch.Tensor(x).float().to(device)\n",
    "\n",
    "        if len(x) == 0:\n",
    "            tn += 1\n",
    "            continue\n",
    "            \n",
    "        pred = model(x)\n",
    "        pred = pred.max(1)[1]\n",
    "        pred = pred.cpu().numpy()\n",
    "\n",
    "        x_lst = x_lst[seq_len - 1:]\n",
    "        y_lst = y_lst[seq_len - 1:]\n",
    "        \n",
    "        y_all_test += y_lst\n",
    "        pred_all_test += pred.tolist()\n",
    "        y_eve_test.append(1 in y_lst)\n",
    "        pred_eve_test.append(1 in pred.tolist())\n",
    "        if 1 not in y_lst:\n",
    "            if 1 not in pred:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        \n",
    "        if 1 not in pred and 1 not in y_lst:\n",
    "            continue\n",
    "            \n",
    "        print(key)\n",
    "        path = None\n",
    "        if key in save_keys:\n",
    "            path = os.path.join(\"data\", \"passing\", \"image\", f\"rnn_test_{key}.pdf\")\n",
    "        plot(x_lst, y_lst, pred, seq_len, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125394d0-a266-4220-9cab-a86b943a9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:.3f}'.format(accuracy_score(y_all_test, pred_all_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_all_test, pred_all_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_all_test, pred_all_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_all_test, pred_all_test)))\n",
    "\n",
    "cm = confusion_matrix(y_all_test, pred_all_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa66db1-dae9-433e-a64b-803a7d9f02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per event\n",
    "print('accuracy: {:.3f}'.format(accuracy_score(y_eve_test, pred_eve_test)))\n",
    "print('precision: {:.3f}'.format(precision_score(y_eve_test, pred_eve_test)))\n",
    "print('recall: {:.3f}'.format(recall_score(y_eve_test, pred_eve_test)))\n",
    "print('f1_score: {:.3f}'.format(f1_score(y_eve_test, pred_eve_test)))\n",
    "\n",
    "print('true negative:', tn)\n",
    "print('false negative:', fn)\n",
    "\n",
    "cm = confusion_matrix(y_eve_test, pred_eve_test)\n",
    "sns.heatmap(cm, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30255301-5894-4c67-961f-6123eff18fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
